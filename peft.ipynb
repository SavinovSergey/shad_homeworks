{"cells": [{"cell_type": "markdown", "metadata": {"id": "aSWEcS2XKgzi"}, "source": ["### Practice: Parameter Efficient Fine-Tuning\n", "In this notebook, you're gonna fine-tune large language models within limited GPU memory."]}, {"cell_type": "code", "execution_count": 1, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T09:58:52.235198Z", "iopub.status.busy": "2025-10-05T09:58:52.234582Z", "iopub.status.idle": "2025-10-05T10:00:10.191932Z", "shell.execute_reply": "2025-10-05T10:00:10.191330Z", "shell.execute_reply.started": "2025-10-05T09:58:52.235163Z"}, "id": "7xeRF_hSKgzs"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m425.8/425.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n", "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"]}], "source": ["%pip install --quiet transformers accelerate sentencepiece optimum peft bitsandbytes\n", "\n", "import os\n", "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n", "\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "\n", "import transformers\n", "from tqdm.auto import tqdm, trange\n", "import warnings\n", "warnings.filterwarnings(\"ignore\")\n", "import gc\n", "assert torch.cuda.is_available(), \"you need cuda for this part\"\n", "\n", "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T07:21:12.791134Z", "iopub.status.busy": "2025-10-05T07:21:12.790791Z", "iopub.status.idle": "2025-10-05T07:21:12.795488Z", "shell.execute_reply": "2025-10-05T07:21:12.794823Z", "shell.execute_reply.started": "2025-10-05T07:21:12.791110Z"}}, "outputs": [], "source": ["# !pip install numba\n", "\n", "# from numba import cuda\n", "# device = cuda.get_current_device()\n", "# device.reset()"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 86, "referenced_widgets": ["a7adb06901b34e03893f30ecc23b97ee", "d94f975c69b7421f9851edeff1acbc1d", "7cc587f710c94a72976f67013c0d18f1", "7db40f2dcb2e46309b29e137eac7bba2", "b6013ba4a99743b3a00f7a51a366a507", "7220ba464c234cbba33068f18f68e7c8", "94aa4f70041942639c8759a9e371b80e", "a83adb34773a459a89ae687f328b1aa2", "3ee1280bc2b6439e8298f0ea8c74d30e", "93ed39f5849c493eaa8035bd3d11047b", "845a855f36124f03a30829e21df98702"]}, "execution": {"iopub.execute_input": "2025-10-05T07:21:12.796542Z", "iopub.status.busy": "2025-10-05T07:21:12.796240Z", "iopub.status.idle": "2025-10-05T07:23:03.602257Z", "shell.execute_reply": "2025-10-05T07:23:03.601733Z", "shell.execute_reply.started": "2025-10-05T07:21:12.796517Z"}, "id": "VMzFwx29Kgzu", "outputId": "7077979d-a419-4b4b-af7f-ff34d54697ca"}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "e2f301bc5dd14719b2ba9b31986eaf46", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer_config.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "394ae93b37fa4b4d86055607ce43778e", "version_major": 2, "version_minor": 0}, "text/plain": ["vocab.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "c2b1285b6bce43abbe8cbaf602127b30", "version_major": 2, "version_minor": 0}, "text/plain": ["merges.txt: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "48a4996ba5cc46f7a5273d0c5c508531", "version_major": 2, "version_minor": 0}, "text/plain": ["tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "1029ece5845f4fb7a8c0a4ba5fb4e6cc", "version_major": 2, "version_minor": 0}, "text/plain": ["config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stderr", "output_type": "stream", "text": ["2025-10-05 07:21:27.107182: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n", "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n", "E0000 00:00:1759648887.423982      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n", "E0000 00:00:1759648887.511892      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "7c68128c473d47c18f462c22d348d999", "version_major": 2, "version_minor": 0}, "text/plain": ["model.safetensors.index.json: 0.00B [00:00, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "b1794f7b2bc04f6987198f113d4cc52b", "version_major": 2, "version_minor": 0}, "text/plain": ["Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "3ec7f3ee44ca4031bd51c8e3d7e78a59", "version_major": 2, "version_minor": 0}, "text/plain": ["model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "854fe4aa7aa7467db666cf59db64a11b", "version_major": 2, "version_minor": 0}, "text/plain": ["model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "e6d665f10f9946688c086405561c72c5", "version_major": 2, "version_minor": 0}, "text/plain": ["model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "14e24a62ada74b9eb34a2011790ba1da", "version_major": 2, "version_minor": 0}, "text/plain": ["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "e61592bc10c846c7bdfae544893c15f7", "version_major": 2, "version_minor": 0}, "text/plain": ["generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"]}, "metadata": {}, "output_type": "display_data"}], "source": ["model_name = \"Qwen/Qwen3-4B\"\n", "quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True)\n", "\n", "# loading Qwen tokenizer ...\n", "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, device_map=device)\n", "tokenizer.pad_token_id = tokenizer.eos_token_id\n", "\n", "# ... and the model itself\n", "model = transformers.AutoModelForCausalLM.from_pretrained(\n", "    model_name, device_map=device, low_cpu_mem_usage=True, offload_state_dict=True,\n", "    quantization_config=quantization_config, torch_dtype=torch.float32,\n", ")\n", "for param in model.parameters():\n", "    param.requires_grad=False\n", "\n", "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n", "model.enable_input_require_grads()     # override an implementation quirk in gradient checkpoints that disables backprop unless inputs require grad\n", "# more on gradient checkpointing: https://pytorch.org/docs/stable/checkpoint.html https://arxiv.org/abs/1604.06174"]}, {"cell_type": "markdown", "metadata": {"id": "rgspB2JwSIS2"}, "source": ["### Prompt tuning: the story of a fox (2 pts)\n", "\n", "![img](https://i.imgur.com/Ux3qQAu.png) (source: theodd1souts.fandom.com)"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "execution": {"iopub.execute_input": "2025-10-05T07:23:03.604128Z", "iopub.status.busy": "2025-10-05T07:23:03.603652Z", "iopub.status.idle": "2025-10-05T07:23:06.347906Z", "shell.execute_reply": "2025-10-05T07:23:06.347109Z", "shell.execute_reply.started": "2025-10-05T07:23:03.604108Z"}, "id": "H13pYFRxQi4U", "outputId": "597e1af9-399a-41ab-8d8d-9c1c216d906c"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "Output: A quick brown fox jumped over the lazy dog. That's the famous\n"]}], "source": ["prompt = 'A quick brown fox'\n", "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n", "\n", "for i in range(10):\n", "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n", "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n", "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n", "\n", "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0].cpu().numpy().tolist()))"]}, {"cell_type": "markdown", "metadata": {"id": "VVhZACT6SgLq"}, "source": ["What a blatant lie! This particular fox assures you that it didn't in fact jump over the lazy dog. No, sir! The fox was just minding its own business. __Your task is to train the model to say truth: no dog was jumped over today.__"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "execution": {"iopub.execute_input": "2025-10-05T07:23:06.349082Z", "iopub.status.busy": "2025-10-05T07:23:06.348798Z", "iopub.status.idle": "2025-10-05T07:23:07.991902Z", "shell.execute_reply": "2025-10-05T07:23:07.990978Z", "shell.execute_reply.started": "2025-10-05T07:23:06.349054Z"}, "id": "_r6UVDl4NEua", "outputId": "67ab27e0-af96-41c7-f0a9-db92e842cd80"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Loss: tensor(4.3350, device='cuda:0', grad_fn=<NllLossBackward0>)\n"]}], "source": ["the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n", "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n", "outputs = model(**batch)\n", "\n", "next_word_logits = outputs.logits[:, :-1]\n", "true_next_tokens = batch['input_ids'][:, 1:]\n", "loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n", "\n", "print(\"Loss:\", loss)"]}, {"cell_type": "markdown", "metadata": {"id": "amvNufS8WXa0"}, "source": ["Except, we can't train the entire model - that would be 28GB gradients in float32. Instead, let's run [prompt tuning](https://arxiv.org/abs/2104.08691).\n", "\n", "![img](https://i.imgur.com/VwNNKnb.png)\n"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T07:23:07.993116Z", "iopub.status.busy": "2025-10-05T07:23:07.992821Z", "iopub.status.idle": "2025-10-05T07:23:08.000712Z", "shell.execute_reply": "2025-10-05T07:23:07.999862Z", "shell.execute_reply.started": "2025-10-05T07:23:07.993086Z"}, "id": "73ZOCFRZWR98"}, "outputs": [], "source": ["class WordEmbeddingsWithLearnedPrompts(nn.Module):\n", "    \"\"\"\n", "    To perform prompt tuning, you will need to replace model's original word embeddings with a layer - THIS layer\n", "     - that inserts trainable prompts instead of the first N token embeddings. \"\"\"\n", "\n", "    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n", "        super().__init__()\n", "        self.original_word_embeddings = word_embeddings\n", "        self.num_prompts = num_prompts\n", "        self.learnable_prompts = nn.Parameter(\n", "            torch.randn(1, num_prompts, word_embeddings.embedding_dim), requires_grad=True)\n", "\n", "    def forward(self, input_ids: torch.LongTensor):\n", "        # input_ids shape: [batch_size, seq length]\n", "        assert input_ids.dtype == torch.int64\n", "        assert input_ids.shape[1] > self.num_prompts\n", "        assert torch.all(input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).item(), \"don't forget to prepend several BOS tokens to input_ids\"\n", "\n", "        # Your task: embed input_ids, but replace the first :num_prompts: tokens with self.learnable_prompts\n", "        # This is because we will prepend :num_prompts: padding tokens at the beginning\n", "\n", "        # After you are done, you must produce a word embedding vector for each token in input_ids,\n", "        # except that the first :num_prompts: vectors should equal learnable_prompts;\n", "        # any additional vectors after first :num_prompts: ones should be embedded as usual\n", "        # Note: since you're dealing with trainable params, please torch.cat instead of item assignment\n", "\n", "        orig_outp = self.original_word_embeddings(input_ids)\n", "        return torch.cat((self.learnable_prompts, orig_outp[:, self.num_prompts:]), dim=1)"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "execution": {"iopub.execute_input": "2025-10-05T07:23:08.001916Z", "iopub.status.busy": "2025-10-05T07:23:08.001614Z", "iopub.status.idle": "2025-10-05T07:23:08.035508Z", "shell.execute_reply": "2025-10-05T07:23:08.034870Z", "shell.execute_reply.started": "2025-10-05T07:23:08.001888Z"}, "id": "kxUyUU2uT2f1", "outputId": "9d0de5c1-162a-4a6f-92c1-1a7f41069464"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Looks legit!\n"]}], "source": ["num_prompts = 16\n", "test_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n", "test_input_ids = tokenizer(\"a cat say on a may\", return_tensors='pt')['input_ids'].to(device)\n", "\n", "space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n", "                               dtype=torch.int64, device=device)\n", "test_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids], dim=1)\n", "\n", "with torch.amp.autocast('cuda'):\n", "  test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n", "\n", "assert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\n", "assert test_prompt_embeddings.shape[-1] == model.config.hidden_size\n", "assert torch.allclose(test_prompt_embeddings[:, :num_prompts], test_emb_layer.learnable_prompts.float())\n", "assert torch.allclose(test_prompt_embeddings[:, num_prompts:], model.model.embed_tokens(test_input_ids).float())\n", "print(\"Looks legit!\")"]}, {"cell_type": "markdown", "metadata": {"id": "FbKPgfT-crqW"}, "source": ["__Now that it works,__ let's inject learnable prompts into the main model and teach it about foxes."]}, {"cell_type": "code", "execution_count": 8, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T07:23:08.037075Z", "iopub.status.busy": "2025-10-05T07:23:08.036332Z", "iopub.status.idle": "2025-10-05T07:23:08.044003Z", "shell.execute_reply": "2025-10-05T07:23:08.043421Z", "shell.execute_reply.started": "2025-10-05T07:23:08.037045Z"}, "id": "QRe0lpREV49G"}, "outputs": [], "source": ["assert isinstance(model.model.embed_tokens, nn.Embedding), \"you have already replaced the embedding layer. If the replacement is broken, please reload the model\"\n", "\n", "model.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n", "\n", "opt = torch.optim.Adam([model.model.embed_tokens.learnable_prompts], lr=0.01)"]}, {"cell_type": "code", "execution_count": 9, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T07:23:08.045095Z", "iopub.status.busy": "2025-10-05T07:23:08.044793Z", "iopub.status.idle": "2025-10-05T07:24:41.469479Z", "shell.execute_reply": "2025-10-05T07:24:41.468650Z", "shell.execute_reply.started": "2025-10-05T07:23:08.045071Z"}, "id": "3gVQzgdka-Bm"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Step: 20, loss: 3.340280055999756.\n", "Step: 40, loss: 2.1509807109832764.\n", "Step: 60, loss: 0.8044292330741882.\n", "Step: 80, loss: 0.0049959332682192326.\n", "Step: 100, loss: 0.0010111827868968248.\n", "Step: 120, loss: 0.0005855258787050843.\n", "Step: 140, loss: 0.000455891655292362.\n", "Step: 160, loss: 0.0003850536304526031.\n", "Step: 180, loss: 0.0003361439739819616.\n", "Step: 200, loss: 0.00029881467344239354.\n", "Good job!\n"]}], "source": ["the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n", "num_steps = 200\n", "for i in range(num_steps):\n", "    batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n", "    space_for_prompts = torch.full([len(test_input_ids), num_prompts], fill_value=tokenizer.pad_token_id,\n", "                                   dtype=torch.int64, device=device)\n", "    batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n", "    batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n", "    \n", "    outputs = model(**batch)\n", "    next_word_logits = outputs.logits[:, num_prompts : -1, :]\n", "    true_next_tokens = batch['input_ids'][:, num_prompts + 1:]\n", "    loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n", "    loss.backward()\n", "\n", "    opt.step()\n", "    opt.zero_grad()\n", "    if (i + 1) % 20 == 0:\n", "        print(f\"Step: {i + 1}, loss: {loss}.\")\n", "\n", "# raise NotImplemented(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")\n", "assert loss.item() <= 0.1\n", "print(\"Good job!\")"]}, {"cell_type": "code", "execution_count": 10, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T07:24:41.472371Z", "iopub.status.busy": "2025-10-05T07:24:41.472141Z", "iopub.status.idle": "2025-10-05T07:24:44.433195Z", "shell.execute_reply": "2025-10-05T07:24:44.432400Z", "shell.execute_reply.started": "2025-10-05T07:24:41.472352Z"}, "id": "F7DkWHD-r1Xo"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "Output: A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway\n"]}], "source": ["prompt = 'A quick brown fox'\n", "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n", "batch['input_ids'] = torch.cat([space_for_prompts, batch['input_ids']], dim=1)\n", "batch['attention_mask'] = torch.cat([torch.ones_like(space_for_prompts), batch['attention_mask']], dim=1)\n", "\n", "\n", "for i in range(15):\n", "    next_token = model(**batch).logits[0, -1].argmax(-1).reshape(1, 1)\n", "    batch['input_ids'] = torch.cat([batch['input_ids'], next_token], dim=-1)\n", "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(next_token)], dim=-1)\n", "\n", "print(\"\\nOutput:\", tokenizer.decode(batch['input_ids'][0, num_prompts:].cpu().numpy().tolist()))\n", "\n", "# if you did everything right, the model will deny that the fox jumped over the lazy dog"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T07:24:44.434412Z", "iopub.status.busy": "2025-10-05T07:24:44.434119Z", "iopub.status.idle": "2025-10-05T07:24:44.852487Z", "shell.execute_reply": "2025-10-05T07:24:44.851740Z", "shell.execute_reply.started": "2025-10-05T07:24:44.434388Z"}}, "outputs": [{"data": {"text/plain": ["125"]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["del model\n", "del opt\n", "torch.cuda.empty_cache()\n", "gc.collect()"]}, {"cell_type": "markdown", "metadata": {"id": "sEkoFNdlshv_"}, "source": ["### Using HuggingFace PEFT (2 points)\n", "\n", "[`peft`](https://huggingface.co/docs/peft/index) is a transformer's sister library that allows you to apply various __p__arameter __e__fficient __f__ine-__t__uning methods to pre-trained transformers. The library imlements both prompt tuning, prefix tuning, as well as several adapter-based techniques under a common interface:\n", "\n"]}, {"cell_type": "code", "execution_count": 12, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "execution": {"iopub.execute_input": "2025-10-05T07:24:44.853635Z", "iopub.status.busy": "2025-10-05T07:24:44.853376Z", "iopub.status.idle": "2025-10-05T07:24:59.756585Z", "shell.execute_reply": "2025-10-05T07:24:59.756030Z", "shell.execute_reply.started": "2025-10-05T07:24:44.853612Z"}, "id": "mqEEpZm2Q4UC", "outputId": "2b760d0e-ac1e-4580-9472-997d1275385d"}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "97c65578df544af1807d1b7fe2a459a3", "version_major": 2, "version_minor": 0}, "text/plain": ["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": ["Trainable parameters: 40960\n", "Total parameters (excluding quantization): 2205851136\n"]}], "source": ["import peft\n", "\n", "# reloading model\n", "model = transformers.AutoModelForCausalLM.from_pretrained(\n", "    model_name, device_map=device, low_cpu_mem_usage=True, offload_state_dict=True,\n", "    quantization_config=quantization_config, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n", ")\n", "for param in model.parameters():\n", "    param.requires_grad=False\n", "    \n", "assert isinstance(model.model.embed_tokens, nn.Embedding), \"please reload the model\"\n", "\n", "peft_config = peft.PromptTuningConfig(task_type=peft.TaskType.CAUSAL_LM, num_virtual_tokens=16)\n", "model = peft.get_peft_model(model, peft_config)  # note: for most peft methods, this line also modifies model in-place\n", "print(\"Trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n", "print(\"Total parameters (excluding quantization):\", sum(p.numel() for p in model.parameters()))"]}, {"cell_type": "code", "execution_count": 13, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T07:24:59.757549Z", "iopub.status.busy": "2025-10-05T07:24:59.757317Z", "iopub.status.idle": "2025-10-05T07:24:59.762938Z", "shell.execute_reply": "2025-10-05T07:24:59.762248Z", "shell.execute_reply.started": "2025-10-05T07:24:59.757522Z"}}, "outputs": [], "source": ["opt = torch.optim.Adam(model.parameters(), lr=0.01)"]}, {"cell_type": "code", "execution_count": 14, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T07:24:59.763943Z", "iopub.status.busy": "2025-10-05T07:24:59.763704Z", "iopub.status.idle": "2025-10-05T07:26:38.864942Z", "shell.execute_reply": "2025-10-05T07:26:38.864302Z", "shell.execute_reply.started": "2025-10-05T07:24:59.763919Z"}, "id": "UW54GnzCwVpp"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Step: 20, loss: 3.5473380088806152.\n", "Step: 40, loss: 2.696080207824707.\n", "Step: 60, loss: 1.0062546730041504.\n", "Step: 80, loss: 0.6050422787666321.\n", "Step: 100, loss: 0.35903388261795044.\n", "Step: 120, loss: 0.3504926860332489.\n", "Step: 140, loss: 0.007468266878277063.\n", "Step: 160, loss: 0.0019193228799849749.\n", "Step: 180, loss: 0.0011071553453803062.\n", "Step: 200, loss: 0.0008447786094620824.\n", "Good job!\n"]}], "source": ["# Your task: optimize the PEFT-wrapped model to achieve next token prediction loss < 0.1, but this time using PEFT\n", "# Please note: you no longer need to prepend PAD tokens, but you still need to skip :num_virtual_tokens: first logits.\n", "# Finally, generate the sentence to make sure that the model learned the truth.\n", "\n", "the_truth = \"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"\n", "num_steps = 200\n", "for i in range(num_steps):\n", "    batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)    \n", "    outputs = model(**batch)\n", "    next_word_logits = outputs.logits[:, num_prompts : -1, :]\n", "    true_next_tokens = batch['input_ids'][:, 1:]\n", "    loss = F.cross_entropy(next_word_logits.flatten(0, 1), true_next_tokens.flatten(0, 1))\n", "    loss.backward()\n", "\n", "    opt.step()\n", "    opt.zero_grad()\n", "    if (i + 1) % 20 == 0:\n", "        print(f\"Step: {i + 1}, loss: {loss}.\")\n", "\n", "# raise NotImplemented(\"Your task: iteratively train the model to reduce loss using prompt optimizer (opt)\")\n", "assert loss.item() <= 0.1\n", "print(\"Good job!\")"]}, {"cell_type": "code", "execution_count": 15, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T07:26:38.865895Z", "iopub.status.busy": "2025-10-05T07:26:38.865673Z", "iopub.status.idle": "2025-10-05T07:26:39.279825Z", "shell.execute_reply": "2025-10-05T07:26:39.279195Z", "shell.execute_reply.started": "2025-10-05T07:26:38.865870Z"}}, "outputs": [{"data": {"text/plain": ["5386"]}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": ["del model\n", "del opt\n", "torch.cuda.empty_cache()\n", "gc.collect()"]}, {"cell_type": "markdown", "metadata": {"id": "uCkpKYjWxfhk"}, "source": ["### Parameter-efficient finetuning with LoRA (2 points)\n", "\n", "When training on more serious tasks, you can use low-rank adapters based on the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf).\n", "\n", "The core idea is to add low-rank adapters __in parallel with existing linear layers,__ like this:\n", "<center><img src=\"https://i.imgur.com/6bQLNiG.png\" width=240px></center>\n", "\n", "In the original LoRA paper, the adapters were only added to attention projection matrices. However, [subsequent works](https://arxiv.org/abs/2305.14314) show that it is useful to adapt FFNs as well. But before we do any training, we need to implement the basic LoRA layer."]}, {"cell_type": "code", "execution_count": 16, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 49, "referenced_widgets": ["b87c54c3bed847ab933eae8175359169", "8aaa22971b3e416eae8394ef3b3b3f0f", "e9c4ba0d262c4b76baa00532e209b92f", "e6f9064e6ec545debe2e90163f4c712c", "6aad5b046def4a7db1048434e874b5d5", "dba48e929a2e43ec8e4bb3d4b32475ca", "530d66e4732b4d5486165654415bd2dc", "2bd4b6acd8004c1e98c064708108938e", "09b07105e4f54d2bb5e6cc1c1f1a9c8e", "923869a5864c4d3d80fb76c99fff24e2", "25e1f3d72230485c9b84cae4f685a69a"]}, "execution": {"iopub.execute_input": "2025-10-05T07:26:39.280813Z", "iopub.status.busy": "2025-10-05T07:26:39.280542Z", "iopub.status.idle": "2025-10-05T07:26:51.684662Z", "shell.execute_reply": "2025-10-05T07:26:51.684131Z", "shell.execute_reply.started": "2025-10-05T07:26:39.280786Z"}, "id": "8zundaSzx90r", "outputId": "3faf7150-7685-4089-cf58-e03e4fce8bc6"}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "860284233f6347ada494033d8ca540e9", "version_major": 2, "version_minor": 0}, "text/plain": ["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}], "source": ["# re-load the model to remove any previous PEFT tuners\n", "model_name = \"Qwen/Qwen3-4B\"\n", "model = transformers.AutoModelForCausalLM.from_pretrained(\n", "    model_name, device_map=device, low_cpu_mem_usage=True, offload_state_dict=True,\n", "    quantization_config=quantization_config, torch_dtype=torch.float32,  # weights are 4-bit; layernorms and activations are fp32\n", ")\n", "for param in model.parameters():\n", "    param.requires_grad=False\n", "model.gradient_checkpointing_enable()\n", "model.enable_input_require_grads()"]}, {"cell_type": "code", "execution_count": 2, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T10:00:10.193703Z", "iopub.status.busy": "2025-10-05T10:00:10.193258Z", "iopub.status.idle": "2025-10-05T10:00:10.198909Z", "shell.execute_reply": "2025-10-05T10:00:10.198270Z", "shell.execute_reply.started": "2025-10-05T10:00:10.193679Z"}, "id": "MJ_hq4fwyPVR"}, "outputs": [], "source": ["class LoRALayer(nn.Module):\n", "    \"\"\"Wraps a linear layer with LoRA-like adapter. Wraps an existing OPT linear layer\"\"\"\n", "    def __init__(self, module: nn.Linear, rank: int):\n", "        super().__init__()\n", "        self.module = module  # pre-trained (frozen) linear layer\n", "        self.adapter_A = nn.Parameter(torch.empty(module.in_features, rank, device=module.weight.device))\n", "        nn.init.kaiming_uniform_(self.adapter_A, a=5 ** 0.5)\n", "        self.adapter_B = nn.Parameter(torch.zeros(rank, module.out_features, device=module.weight.device))\n", "\n", "    def forward(self, inp):\n", "        # Apply self.module and LoRA adapter, return the sum (self.module outputs + adapter outputs)\n", "        return self.module(inp) + inp @ self.adapter_A @ self.adapter_B"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "execution": {"iopub.execute_input": "2025-10-05T10:00:10.200005Z", "iopub.status.busy": "2025-10-05T10:00:10.199774Z", "iopub.status.idle": "2025-10-05T10:00:13.176422Z", "shell.execute_reply": "2025-10-05T10:00:13.175728Z", "shell.execute_reply.started": "2025-10-05T10:00:10.199988Z"}, "id": "tTzOs65JydcS", "outputId": "e07177c9-2f2b-432a-8a97-9e507df166bf"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["All tests passed!\n"]}], "source": ["# test your implementation\n", "test_linear = nn.Linear(128, 128)\n", "test_linear.weight.data[...] = torch.eye(128)\n", "test_adapter = LoRALayer(test_linear, rank=8)\n", "\n", "assert torch.allclose(test_adapter(torch.ones(1, 1, 128)), test_linear.bias + 1), \"please check your forward pass\"\n", "\n", "test_adapter.adapter_A.data[...] = torch.linspace(0.1, -0.5, 128 * 8).view(128, 8)\n", "test_adapter.adapter_B.data[...] = torch.linspace(0.5, -0.1, 128 * 8).view(8, 128)\n", "test_linear.bias.data[...] = torch.linspace(1., -1., 128)\n", "\n", "dummy_loss = F.mse_loss(test_adapter(torch.ones(1, 128) / 128).squeeze(), torch.linspace(-1, 1, 128))\n", "assert torch.allclose(dummy_loss, torch.tensor(1.3711389), rtol=0, atol=1e-4)\n", "dummy_loss.backward()\n", "assert all(w.grad is not None for w in [test_adapter.adapter_A, test_adapter.adapter_B]), \"some adapter weights have no grad\"\n", "assert torch.allclose(test_adapter.adapter_A.grad.sum(), torch.tensor(-0.60158), rtol=0, atol=1e-4), \"bad grad w.r.t. A\"\n", "assert torch.allclose(test_adapter.adapter_B.grad.sum(), torch.tensor(0.9931), rtol=0, atol=1e-4), \"bad grad w.r.t. B\"\n", "# note: bad grad means that your code is different from LoRA paper OR that your code is not autograd-friendly (e.g. no_grad)\n", "del dummy_loss, test_linear, test_adapter\n", "print(\"All tests passed!\")"]}, {"cell_type": "markdown", "metadata": {"id": "tajVTsvLulB6"}, "source": ["### Apply LoRA to the model\n", "\n", "The code below applies LoRA adapters on top of Q/K/V linear layers in Qwen attention. You may also choose to modify other layers:\n", "* self_attn.o_proj - attention output projection\n", "* mlp.up_proj, mlp.gate_proj, mlp.down_proj - transformer feedforward layers\n", "* lm_head - output LM head\n", "\n", "__Note:__ please scroll down for the homework task"]}, {"cell_type": "code", "execution_count": 19, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T07:26:51.746343Z", "iopub.status.busy": "2025-10-05T07:26:51.746067Z", "iopub.status.idle": "2025-10-05T07:26:51.752717Z", "shell.execute_reply": "2025-10-05T07:26:51.751901Z", "shell.execute_reply.started": "2025-10-05T07:26:51.746324Z"}}, "outputs": [{"data": {"text/plain": ["Qwen3ForCausalLM(\n", "  (model): Qwen3Model(\n", "    (embed_tokens): Embedding(151936, 2560)\n", "    (layers): ModuleList(\n", "      (0-35): 36 x Qwen3DecoderLayer(\n", "        (self_attn): Qwen3Attention(\n", "          (q_proj): Linear4bit(in_features=2560, out_features=4096, bias=False)\n", "          (k_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n", "          (v_proj): Linear4bit(in_features=2560, out_features=1024, bias=False)\n", "          (o_proj): Linear4bit(in_features=4096, out_features=2560, bias=False)\n", "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n", "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n", "        )\n", "        (mlp): Qwen3MLP(\n", "          (gate_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n", "          (up_proj): Linear4bit(in_features=2560, out_features=9728, bias=False)\n", "          (down_proj): Linear4bit(in_features=9728, out_features=2560, bias=False)\n", "          (act_fn): SiLU()\n", "        )\n", "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n", "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n", "      )\n", "    )\n", "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n", "    (rotary_emb): Qwen3RotaryEmbedding()\n", "  )\n", "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n", ")"]}, "execution_count": 19, "metadata": {}, "output_type": "execute_result"}], "source": ["model"]}, {"cell_type": "code", "execution_count": 20, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T07:26:51.753787Z", "iopub.status.busy": "2025-10-05T07:26:51.753512Z", "iopub.status.idle": "2025-10-05T07:26:51.811837Z", "shell.execute_reply": "2025-10-05T07:26:51.811158Z", "shell.execute_reply.started": "2025-10-05T07:26:51.753758Z"}, "id": "davyUVEwulB6"}, "outputs": [], "source": ["lora_rank = 8\n", "for name, module in model.model.layers.named_modules():\n", "    if 'Qwen3DecoderLayer' in repr(type(module)):\n", "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n", "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n", "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n", "\n", "assert sum(isinstance(module, LoRALayer) for module in model.modules()) == 108  # for Qwen3-8b"]}, {"cell_type": "code", "execution_count": 21, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "execution": {"iopub.execute_input": "2025-10-05T07:26:51.812963Z", "iopub.status.busy": "2025-10-05T07:26:51.812544Z", "iopub.status.idle": "2025-10-05T07:26:52.221389Z", "shell.execute_reply": "2025-10-05T07:26:52.220619Z", "shell.execute_reply.started": "2025-10-05T07:26:51.812946Z"}, "id": "AWzfvc0EulB6", "outputId": "b432afe7-08b9-4cb2-c6ac-01f85352a689", "tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Grad check successful, well done!\n"]}], "source": ["batch = tokenizer(\"This model wants to share its greatest secret:\", return_tensors='pt', return_token_type_ids=False).to(device)\n", "# test a single training step, make sure we get meaningful gradients\n", "with torch.amp.autocast('cuda', dtype=torch.float32):\n", "    out = model.forward(**batch)\n", "    (out.logits.norm() / 100).backward()\n", "\n", "for i, module in enumerate(model.modules()):\n", "    if isinstance(module, LoRALayer):\n", "        assert module.adapter_B.grad is not None\n", "        assert module.adapter_B.grad.norm().item() > 0\n", "\n", "model.zero_grad(set_to_none=True)\n", "print(\"Grad check successful, well done!\")"]}, {"cell_type": "code", "execution_count": 22, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T07:26:52.222748Z", "iopub.status.busy": "2025-10-05T07:26:52.222199Z", "iopub.status.idle": "2025-10-05T07:26:52.231034Z", "shell.execute_reply": "2025-10-05T07:26:52.230406Z", "shell.execute_reply.started": "2025-10-05T07:26:52.222716Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Trainable parameters: 3981312, in percent: 0.180.\n"]}], "source": ["def count_trainable_parameters(model):  \n", "    return sum(p.numel() for p in model.parameters() if p.requires_grad)  \n", "\n", "trainable_params = count_trainable_parameters(model)\n", "all_params = sum(p.numel() for p in model.parameters())\n", "print(f\"Trainable parameters: {trainable_params}, in percent: {trainable_params / all_params * 100:.3f}.\") "]}, {"cell_type": "markdown", "metadata": {"id": "rjIJ1vkUulB7"}, "source": ["### (example) How to train your model\n", "\n", "The example below shows how to train the LoRA adapters on a dummy dataset. You will need to run a _similar_ training task later.\n", "\n", "__Note:__ please scroll down for the homework task"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000, "referenced_widgets": ["a868f8a190f74df2a99a50e2ca9a7cb3", "895c44b30fd24b8b9bbedc631a7934ec", "92f3ee2ed68c4defbed2fe9bef0f20b2", "3321598939fd4d76957155f58097fadd", "f0fe9da3411840ef8d7eff80883cb8e9", "8ad2b69a25304e5f903f2fd43b538340", "7aea6cd9a18b4dd9b40bbe65fb0b9069", "f72b2d490b04440b800ac3c8ab05e625", "c6ddf10ea9ef499f917c81fde3e63cd2", "31c4ef0dab98410d88891a2c27fdb5c1", "54b23e64bbc444b4abc3f25832e3677d"]}, "id": "r9mIpntHulB8", "outputId": "21b0c176-b6b8-4b4e-c193-c4c8c61a3bf7"}, "outputs": [], "source": ["# checking if the model can learn. Change max_steps for proper training\n", "import datasets\n", "data = datasets.load_dataset(\"Abirate/english_quotes\", split=\"train[:64]\") # 64 lines\n", "data = data.map(lambda samples: tokenizer(samples['quote'], max_length=256, padding=True), batched=True)\n", "\n", "model._hf_peft_config_loaded = True  # silence a warning from HF trainer\n", "model.config.use_cache = False\n", "model.hf_device_map[''] = torch.device(\"cuda:0\")   # fix accelerate error\n", "\n", "trainer = transformers.Trainer(\n", "    model=model, train_dataset=data,\n", "    args=transformers.TrainingArguments(\n", "        per_device_train_batch_size=4,\n", "        warmup_steps=50, max_steps=20, learning_rate=5e-5, fp16=True,\n", "        logging_steps=5, output_dir='outputs', report_to=\"none\"),\n", "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n", ")\n", "# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n", "\n", "trainer.train()\n", "\n", "# NOTE: this is just an example! you do not have to wait for this progressbar to finish :)"]}, {"cell_type": "markdown", "metadata": {"id": "DQUlqoEAulB8"}, "source": ["### Final task: *actually* train the model (4 points)\n", "\n", "Your task is to fine-tune the model to _generate python code_. Please use the above examples for inspiration. More specifically,\n", "\n", "* __dataset:__ use [codeparrot-clean](https://huggingface.co/datasets/codeparrot/codeparrot-clean) or any other data containing python code. Since you do not need much data for this excercise, it is enough to use just shorter validation subset of `codeparrots`\n", "* __preprocessing:__ select python code based on file extentions (.py)  (may skip in case of codeparrot - it is 100% python)\n", "* __short lines:__ please take the first 512 characters of each line\n", "* __adapter type:__ please use LoRA as defined above __plus at least one of:__\n", "   - extra adapter on lm_head\n", "   - extra adapter on MLP components (mlp.*)\n", "   - trainable input embeddings (requires tweaking memory usage)\n", "\n", "* __training:__ you do not have to train to convergence. If all goes well, your model should `.generate` code after 500 steps. Please use batch size of at least 4 (4 x 1 x 512 tokens) using `gradient_accumulation_steps=4`.\n", "\n", "\n", "Note: the peft library also has LoRA implementation. However, we ask that for this assignment you show at least one complete training run with your own LoRA code.\n", "\n", "__Alternative assignment:__ Instead of doing python code, feel free to substitute the task with any other dataset, e.g. your favorite artist or podcast, as long as it's ethical. If you choose your own task, please show examples of what your model learned - or did not learn, akin to the code examples below."]}, {"cell_type": "code", "execution_count": 4, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T10:00:13.178374Z", "iopub.status.busy": "2025-10-05T10:00:13.178151Z", "iopub.status.idle": "2025-10-05T10:00:21.726891Z", "shell.execute_reply": "2025-10-05T10:00:21.725909Z", "shell.execute_reply.started": "2025-10-05T10:00:13.178356Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["--2025-10-05 10:00:13--  https://huggingface.co/datasets/codeparrot/codeparrot-clean/resolve/main/file-000000000001.json.gz\n", "Resolving huggingface.co (huggingface.co)... 13.226.251.112, 13.226.251.20, 13.226.251.81, ...\n", "Connecting to huggingface.co (huggingface.co)|13.226.251.112|:443... connected.\n", "HTTP request sent, awaiting response... 302 Found\n", "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdd236468d709f183925/50e79501f1fc8cd1dd00644073f04a6b7f07e1ca7d78021c84945a0f03629d4e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251005%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251005T100013Z&X-Amz-Expires=3600&X-Amz-Signature=2f4da7f37c98508e34cde63c7502a085533eff31dd8b6ab30670f740ca9ce591&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27file-000000000001.json.gz%3B+filename%3D%22file-000000000001.json.gz%22%3B&response-content-type=application%2Fgzip&x-id=GetObject&Expires=1759662013&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1OTY2MjAxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRkMjM2NDY4ZDcwOWYxODM5MjUvNTBlNzk1MDFmMWZjOGNkMWRkMDA2NDQwNzNmMDRhNmI3ZjA3ZTFjYTdkNzgwMjFjODQ5NDVhMGYwMzYyOWQ0ZSoifV19&Signature=D7paSbw0RuYr8dz%7EIbtFDmwKV1SJkh8i4OG6VE1tbOMH%7EvtXmoseBXYjQI4%7EZ0FZm4tdpLBrA6nMIdFHqp9HjgLUjqpwPgvN0CbR16fuhnT26bD6Bpu4DF70cBvA5HBqMtJCtw6sT%7ERImF8dpzy50roQ6KKfcI8nKI3aM%7Ev1Q8cPf1OuikipWFbiDDWwHk%7E-rNfD0oU9icO5dhEwhNOEN-MBPufzFw-QNUVtmZ95EpwyrTyf9gn7EXbrAvJBClrM9aahalPpTwjzYssVY7G9lmKT1k6ZNVH4wKTPoOO-LgiTc2V-i6thVwQHkvSzsQfY1i6fin%7EUO1gHmvpippF87A__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n", "--2025-10-05 10:00:13--  https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdd236468d709f183925/50e79501f1fc8cd1dd00644073f04a6b7f07e1ca7d78021c84945a0f03629d4e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251005%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251005T100013Z&X-Amz-Expires=3600&X-Amz-Signature=2f4da7f37c98508e34cde63c7502a085533eff31dd8b6ab30670f740ca9ce591&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27file-000000000001.json.gz%3B+filename%3D%22file-000000000001.json.gz%22%3B&response-content-type=application%2Fgzip&x-id=GetObject&Expires=1759662013&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1OTY2MjAxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRkMjM2NDY4ZDcwOWYxODM5MjUvNTBlNzk1MDFmMWZjOGNkMWRkMDA2NDQwNzNmMDRhNmI3ZjA3ZTFjYTdkNzgwMjFjODQ5NDVhMGYwMzYyOWQ0ZSoifV19&Signature=D7paSbw0RuYr8dz%7EIbtFDmwKV1SJkh8i4OG6VE1tbOMH%7EvtXmoseBXYjQI4%7EZ0FZm4tdpLBrA6nMIdFHqp9HjgLUjqpwPgvN0CbR16fuhnT26bD6Bpu4DF70cBvA5HBqMtJCtw6sT%7ERImF8dpzy50roQ6KKfcI8nKI3aM%7Ev1Q8cPf1OuikipWFbiDDWwHk%7E-rNfD0oU9icO5dhEwhNOEN-MBPufzFw-QNUVtmZ95EpwyrTyf9gn7EXbrAvJBClrM9aahalPpTwjzYssVY7G9lmKT1k6ZNVH4wKTPoOO-LgiTc2V-i6thVwQHkvSzsQfY1i6fin%7EUO1gHmvpippF87A__&Key-Pair-Id=K2L8F4GPSG1IFC\n", "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 13.226.251.62, 13.226.251.57, 13.226.251.118, ...\n", "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|13.226.251.62|:443... connected.\n", "HTTP request sent, awaiting response... 200 OK\n", "Length: 246127841 (235M) [application/gzip]\n", "Saving to: \u2018file-000000000001.json.gz\u2019\n", "\n", "file-000000000001.j 100%[===================>] 234.73M   309MB/s    in 0.8s    \n", "\n", "2025-10-05 10:00:14 (309 MB/s) - \u2018file-000000000001.json.gz\u2019 saved [246127841/246127841]\n", "\n", "rm: cannot remove 'file-000000000001.json.gz': No such file or directory\n"]}], "source": ["!wget https://huggingface.co/datasets/codeparrot/codeparrot-clean/resolve/main/file-000000000001.json.gz\n", "!gzip -d file-000000000001.json.gz\n", "!rm file-000000000001.json.gz"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T10:00:21.728378Z", "iopub.status.busy": "2025-10-05T10:00:21.728095Z", "iopub.status.idle": "2025-10-05T10:00:28.543115Z", "shell.execute_reply": "2025-10-05T10:00:28.542366Z", "shell.execute_reply.started": "2025-10-05T10:00:21.728351Z"}}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "131ae87ca75445898667628d53ec2547", "version_major": 2, "version_minor": 0}, "text/plain": ["Generating train split: 0 examples [00:00, ? examples/s]"]}, "metadata": {}, "output_type": "display_data"}, {"data": {"text/plain": ["DatasetDict({\n", "    train: Dataset({\n", "        features: ['content'],\n", "        num_rows: 100000\n", "    })\n", "})"]}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": ["import datasets\n", "data_files = ['file-000000000001.json']#, 'file-000000000002.json', 'file-000000000003.json']\n", "codeparrot_data = datasets.load_dataset('json', data_files=data_files)\n", "codeparrot_data = codeparrot_data.remove_columns(\n", "    ['repo_name', 'path', 'copies', 'size', 'license', 'hash', 'line_mean', 'line_max', 'alpha_frac', 'autogenerated']\n", ")\n", "codeparrot_data"]}, {"cell_type": "code", "execution_count": 16, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T11:14:47.760858Z", "iopub.status.busy": "2025-10-05T11:14:47.760579Z", "iopub.status.idle": "2025-10-05T11:14:57.849679Z", "shell.execute_reply": "2025-10-05T11:14:57.849126Z", "shell.execute_reply.started": "2025-10-05T11:14:47.760836Z"}}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "6d25a587707e483c879efbc2756b2660", "version_major": 2, "version_minor": 0}, "text/plain": ["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]}, "metadata": {}, "output_type": "display_data"}], "source": ["model_name = 'unsloth/Llama-3.2-3B'\n", "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, device_map=device)\n", "tokenizer.pad_token_id = tokenizer.eos_token_id\n", "quantization_config = transformers.BitsAndBytesConfig(load_in_4bit=True)\n", "\n", "model = transformers.AutoModelForCausalLM.from_pretrained(\n", "    model_name, device_map=device, low_cpu_mem_usage=True, offload_state_dict=True,\n", "    quantization_config=quantization_config, torch_dtype=torch.float32,\n", ")\n", "for param in model.parameters():\n", "    param.requires_grad=False\n", "\n", "model.gradient_checkpointing_enable()  # only store a small subset of activations, re-compute the rest.\n", "model.enable_input_require_grads()"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T10:02:10.792683Z", "iopub.status.busy": "2025-10-05T10:02:10.792176Z", "iopub.status.idle": "2025-10-05T10:06:52.541863Z", "shell.execute_reply": "2025-10-05T10:06:52.541278Z", "shell.execute_reply.started": "2025-10-05T10:02:10.792661Z"}}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "3624499791d4431bbe386b667dbbee81", "version_major": 2, "version_minor": 0}, "text/plain": ["Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"]}, "metadata": {}, "output_type": "display_data"}], "source": ["MAX_LENGTH = 512\n", "\n", "def prepare_code(example):\n", "    return tokenizer(example['content'], padding='max_length', max_length=MAX_LENGTH, truncation=True)\n", "\n", "codeparrot_data = codeparrot_data.map(prepare_code, batched=True)"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T10:06:52.542966Z", "iopub.status.busy": "2025-10-05T10:06:52.542670Z", "iopub.status.idle": "2025-10-05T10:07:01.453905Z", "shell.execute_reply": "2025-10-05T10:07:01.453124Z", "shell.execute_reply.started": "2025-10-05T10:06:52.542938Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["PROMPT: \n", "Question:\n", "Let h = -2.9 - 0.1. Let z = 0.5 + 1.5. Let q = 2.3 - z. Which is the closest to q?  (a) \n", "PROMPT: import\n", "import React, { useEffect } from'react';\n", "import { connect } from'react-redux';\n", "import { Button, Col, Form, Row } from'react-bootstrap';\n", "import { addPost, editPost, getPosts, getPostsByCategory, get\n", "PROMPT: from\n", "from django.shortcuts import render, redirect, get_object_or_404\n", "from.models import Article\n", "from.forms import ArticleForm\n", "from django.contrib.auth.decorators import login_required\n", "from django.contrib.auth import get_user_model\n", "from django.contrib import messages\n", "\n", "PROMPT: while\n", "while True:\n", "    try:\n", "        a, b = map(int, input().split())\n", "        if a > b:\n", "            a, b = b, a\n", "        print(a * b)\n", "    except:\n", "        break\n", "\n", "PROMPT: try\n", "try:\n", "    from setuptools import setup\n", "except ImportError:\n", "    from distutils.core import setup\n", "\n", "config = {\n", "    'description': 'POC',\n", "    'author': 'Yash Khandelwal',\n", "   'version': '0.1',\n", "\n", "PROMPT: if\n", "if you are looking for the best and most comfortable bed, you should consider the benefits of the adjustable bed. Adjustable beds are an excellent way to provide the best possible rest for your family members, but they are also an excellent way to provide the best possible\n", "PROMPT: for\n", "for _ in range(int(input())):\n", "    n = int(input())\n", "    a = list(map(int, input().split()))\n", "    a.sort()\n", "    if n % 2 == 0:\n", "        print(a[(n // 2) - 1\n", "PROMPT: torch\n", "torch.nn.functional.conv2d (x, weight, bias=None, stride=1, padding=0, dilation=1, groups=1, bias_attr=None, out=None, data_format=None, *args, **kwargs) \u00b6\n", "Performs\n"]}], "source": ["prompts = ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch']\n", "inputs = tokenizer(prompts, return_tensors='pt', padding=True).to(device)\n", "outputs = model.generate(**inputs, max_new_tokens=50)\n", "texts_before_finetuning = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n", "for p, text in zip(prompts, texts_before_finetuning):\n", "    print(\"PROMPT:\", p)\n", "    print(text)"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T08:15:59.642001Z", "iopub.status.busy": "2025-10-05T08:15:59.641244Z", "iopub.status.idle": "2025-10-05T08:15:59.648486Z", "shell.execute_reply": "2025-10-05T08:15:59.647852Z", "shell.execute_reply.started": "2025-10-05T08:15:59.641977Z"}}, "outputs": [{"data": {"text/plain": ["LlamaForCausalLM(\n", "  (model): LlamaModel(\n", "    (embed_tokens): Embedding(128256, 4096, padding_idx=128255)\n", "    (layers): ModuleList(\n", "      (0-31): 32 x LlamaDecoderLayer(\n", "        (self_attn): LlamaAttention(\n", "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n", "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n", "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n", "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n", "        )\n", "        (mlp): LlamaMLP(\n", "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n", "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n", "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n", "          (act_fn): SiLU()\n", "        )\n", "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n", "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n", "      )\n", "    )\n", "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n", "    (rotary_emb): LlamaRotaryEmbedding()\n", "  )\n", "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n", ")"]}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": ["model"]}, {"cell_type": "code", "execution_count": 17, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T11:15:47.973016Z", "iopub.status.busy": "2025-10-05T11:15:47.972490Z", "iopub.status.idle": "2025-10-05T11:15:48.019713Z", "shell.execute_reply": "2025-10-05T11:15:48.019240Z", "shell.execute_reply.started": "2025-10-05T11:15:47.972992Z"}}, "outputs": [], "source": ["lora_rank = 4\n", "\n", "for name, module in model.model.layers.named_modules():\n", "    if 'LlamaDecoderLayer' in repr(type(module)):\n", "        module.self_attn.q_proj = LoRALayer(module.self_attn.q_proj, rank=lora_rank).to(device)\n", "        module.self_attn.k_proj = LoRALayer(module.self_attn.k_proj, rank=lora_rank).to(device)\n", "        module.self_attn.v_proj = LoRALayer(module.self_attn.v_proj, rank=lora_rank).to(device)\n", "\n", "        module.mlp.gate_proj = LoRALayer(module.mlp.gate_proj, rank=lora_rank).to(device)\n", "        module.mlp.up_proj = LoRALayer(module.mlp.up_proj, rank=lora_rank).to(device)\n", "        module.mlp.down_proj = LoRALayer(module.mlp.down_proj, rank=lora_rank).to(device)"]}, {"cell_type": "code", "execution_count": 18, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T11:15:48.485302Z", "iopub.status.busy": "2025-10-05T11:15:48.484765Z", "iopub.status.idle": "2025-10-05T11:15:48.498975Z", "shell.execute_reply": "2025-10-05T11:15:48.498194Z", "shell.execute_reply.started": "2025-10-05T11:15:48.485270Z"}}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Trainable parameters: 5390336, in percent: 0.298.\n"]}], "source": ["def count_trainable_parameters(model):  \n", "    return sum(p.numel() for p in model.parameters() if p.requires_grad)  \n", "\n", "trainable_params = count_trainable_parameters(model)\n", "all_params = sum(p.numel() for p in model.parameters())\n", "print(f\"Trainable parameters: {trainable_params}, in percent: {trainable_params / all_params * 100:.3f}.\") "]}, {"cell_type": "code", "execution_count": 19, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T11:16:28.728663Z", "iopub.status.busy": "2025-10-05T11:16:28.728016Z", "iopub.status.idle": "2025-10-05T13:33:25.034019Z", "shell.execute_reply": "2025-10-05T13:33:25.032949Z", "shell.execute_reply.started": "2025-10-05T11:16:28.728637Z"}}, "outputs": [{"data": {"text/html": ["\n", "    <div>\n", "      \n", "      <progress value='151' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [150/150 2:15:59, Epoch 0.05/1]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Step</th>\n", "      <th>Training Loss</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>10</td>\n", "      <td>0.656900</td>\n", "    </tr>\n", "    <tr>\n", "      <td>20</td>\n", "      <td>0.664200</td>\n", "    </tr>\n", "    <tr>\n", "      <td>30</td>\n", "      <td>0.621000</td>\n", "    </tr>\n", "    <tr>\n", "      <td>40</td>\n", "      <td>0.595600</td>\n", "    </tr>\n", "    <tr>\n", "      <td>50</td>\n", "      <td>0.617600</td>\n", "    </tr>\n", "    <tr>\n", "      <td>60</td>\n", "      <td>0.607800</td>\n", "    </tr>\n", "    <tr>\n", "      <td>70</td>\n", "      <td>0.619700</td>\n", "    </tr>\n", "    <tr>\n", "      <td>80</td>\n", "      <td>0.647500</td>\n", "    </tr>\n", "    <tr>\n", "      <td>90</td>\n", "      <td>0.659000</td>\n", "    </tr>\n", "    <tr>\n", "      <td>100</td>\n", "      <td>0.617200</td>\n", "    </tr>\n", "    <tr>\n", "      <td>110</td>\n", "      <td>0.627000</td>\n", "    </tr>\n", "    <tr>\n", "      <td>120</td>\n", "      <td>0.599200</td>\n", "    </tr>\n", "    <tr>\n", "      <td>130</td>\n", "      <td>0.588100</td>\n", "    </tr>\n", "    <tr>\n", "      <td>140</td>\n", "      <td>0.578700</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["model._hf_peft_config_loaded = True  # silence a warning from HF trainer\n", "model.config.use_cache = False\n", "model.hf_device_map[''] = torch.device(\"cuda:0\")   # fix accelerate error\n", "\n", "trainer = transformers.Trainer(\n", "    model=model, train_dataset=codeparrot_data['train'],\n", "    args=transformers.TrainingArguments(\n", "        per_device_train_batch_size=4, gradient_accumulation_steps=4,\n", "        # note: if you want larger batch size, increase gradient_accumulation_steps\n", "        warmup_steps=50, max_steps=150, learning_rate=5e-4, fp16=True,\n", "        logging_steps=10, output_dir='outputs', report_to=\"none\",\n", "        weight_decay=0.01),\n", "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n", ")\n", "# if you see cache warnings, set `model.config.use_cache = False` to silence them. Please re-enable for inference!\n", "\n", "trainer.train()"]}, {"cell_type": "code", "execution_count": 22, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T13:36:18.776786Z", "iopub.status.busy": "2025-10-05T13:36:18.775900Z", "iopub.status.idle": "2025-10-05T13:36:27.753606Z", "shell.execute_reply": "2025-10-05T13:36:27.752828Z", "shell.execute_reply.started": "2025-10-05T13:36:18.776751Z"}}, "outputs": [], "source": ["model.config.use_cache = True\n", "prompts = ['', 'import', 'from', 'while', 'try', 'if', 'for', 'torch']\n", "inputs = tokenizer(prompts, return_tensors='pt', padding=True).to(device)\n", "outputs = model.generate(**inputs, max_new_tokens=50)\n", "texts_after_finetuning = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n", "# for p, text in zip(prompts, texts_before_finetuning):\n", "#     print(\"PROMPT:\", p)\n", "#     print(text)"]}, {"cell_type": "code", "execution_count": 23, "metadata": {"execution": {"iopub.execute_input": "2025-10-05T13:37:14.001865Z", "iopub.status.busy": "2025-10-05T13:37:14.001595Z", "iopub.status.idle": "2025-10-05T13:37:14.008435Z", "shell.execute_reply": "2025-10-05T13:37:14.007704Z", "shell.execute_reply.started": "2025-10-05T13:37:14.001844Z"}, "id": "SSucUeB4ulB9", "outputId": "88f008b5-e68b-4949-d695-4d0de17cdd5c"}, "outputs": [{"data": {"text/html": ["<table style=\"border:1px solid black\" >\n", "  <tr>\n", "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n", "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n", "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n", "  </tr>\n", "  <tr>\n", "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">``</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">Question:\n", "Let h = -2.9 - 0.1. Let z = 0.5 + 1.5. Let q = 2.3 - z. Which is the closest to q?  (a) </pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">from __future__ import unicode_literals\n", "\n", "import json\n", "import logging\n", "import re\n", "import time\n", "\n", "import requests\n", "import simplejson\n", "from django.conf import settings\n", "from django.core.exceptions import ImproperlyConfigured\n", "from django.core.urlresolvers</pre></td>\n", "  </tr>\n", "  <tr>\n", "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`import`</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">import React, { useEffect } from'react';\n", "import { connect } from'react-redux';\n", "import { Button, Col, Form, Row } from'react-bootstrap';\n", "import { addPost, editPost, getPosts, getPostsByCategory, get</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "def read_data(file_name):\n", "    data = np.loadtxt(file_name)\n", "    return data\n", "\n", "def plot_data(data, file_name):\n", "    plt.figure()\n", "    plt.plot(data[:, 0], data[:,</pre></td>\n", "  </tr>\n", "  <tr>\n", "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`from`</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">from django.shortcuts import render, redirect, get_object_or_404\n", "from.models import Article\n", "from.forms import ArticleForm\n", "from django.contrib.auth.decorators import login_required\n", "from django.contrib.auth import get_user_model\n", "from django.contrib import messages\n", "</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">from __future__ import absolute_import, division, print_function\n", "\n", "import os\n", "import sys\n", "import unittest\n", "import numpy as np\n", "from scipy import sparse\n", "from scipy import stats\n", "from scipy import linalg\n", "from scipy import optimize\n", "from scipy import</pre></td>\n", "  </tr>\n", "  <tr>\n", "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`while`</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">while True:\n", "    try:\n", "        a, b = map(int, input().split())\n", "        if a > b:\n", "            a, b = b, a\n", "        print(a * b)\n", "    except:\n", "        break\n", "</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">while True:\n", "    n = int(input())\n", "    if n == 0:\n", "        break\n", "    else:\n", "        count = 0\n", "        for i in range(n):\n", "            for j in range(n):\n", "                if i == j:\n", "                    count +=</pre></td>\n", "  </tr>\n", "  <tr>\n", "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`try`</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">try:\n", "    from setuptools import setup\n", "except ImportError:\n", "    from distutils.core import setup\n", "\n", "config = {\n", "    'description': 'POC',\n", "    'author': 'Yash Khandelwal',\n", "   'version': '0.1',\n", "</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">try:\n", "    import Tkinter\n", "except:\n", "    import tkinter as Tkinter\n", "from tkinter import ttk\n", "import os\n", "import os.path\n", "import sys\n", "import time\n", "import random\n", "import threading\n", "import re\n", "import shutil\n", "import threading\n", "import socket</pre></td>\n", "  </tr>\n", "  <tr>\n", "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`if`</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">if you are looking for the best and most comfortable bed, you should consider the benefits of the adjustable bed. Adjustable beds are an excellent way to provide the best possible rest for your family members, but they are also an excellent way to provide the best possible</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">if __name__ == '__main__':\n", "    # This file is executed only when the program is run as a script\n", "    # (e.g. by typing `python -m mymodule` at the command line)\n", "    from mymodule import *\n", "   </pre></td>\n", "  </tr>\n", "  <tr>\n", "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`for`</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">for _ in range(int(input())):\n", "    n = int(input())\n", "    a = list(map(int, input().split()))\n", "    a.sort()\n", "    if n % 2 == 0:\n", "        print(a[(n // 2) - 1</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">for _ in range(int(input())):\n", "    n, k = list(map(int, input().split()))\n", "    if k == 0:\n", "        print(n)\n", "        continue\n", "    if n % k == 0:\n", "        print(n)\n", "        continue\n", "</pre></td>\n", "  </tr>\n", "  <tr>\n", "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`torch`</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">torch.nn.functional.conv2d (x, weight, bias=None, stride=1, padding=0, dilation=1, groups=1, bias_attr=None, out=None, data_format=None, *args, **kwargs) \u00b6\n", "Performs</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">torch.nn.functional.conv2d \u00b6\n", "torch.nn.functional. conv2d ( input, weight, bias=None, stride=1, padding=0, dilation=1, transposed=False, output_padding=0, groups=1, bias_shape=None</pre></td>\n", "  </tr>\n", "</table>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["# This template helps to compare generated code samples in pretty table form\n", "# feel free to present your work in other forms\n", "\n", "from IPython.display import HTML, display\n", "table_template = \"\"\"<table style=\"border:1px solid black\" >\n", "  <tr>\n", "    <th style=\"text-align: center; border:1px solid black\">PROMPT</th>\n", "    <th style=\"text-align: center; border:1px solid black\">BEFORE</th>\n", "    <th style=\"text-align: center; border:1px solid black\">AFTER</th>\n", "  </tr>\n", "{}\n", "</table>\"\"\"\n", "\n", "row_template = '''  <tr>\n", "    <td style=\"width:20%; border:1px solid black\"><pre align=\"left\">`{}`</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n", "    <td style=\"width:40%; border:1px solid black\"><pre align=\"left\">{}</pre></td>\n", "  </tr>'''\n", "\n", "rows = []\n", "\n", "for p, b, a in zip(prompts, texts_before_finetuning, texts_after_finetuning):\n", "    # replace placeholders in the format() arguments\n", "    rows.append(row_template.format(p, b, a))\n", "\n", "display(HTML(table_template.format('\\n'.join(rows))))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**All generations in python**"]}], "metadata": {"accelerator": "GPU", "colab": {"gpuType": "T4", "provenance": []}, "kaggle": {"accelerator": "nvidiaTeslaT4", "dataSources": [], "dockerImageVersionId": 31090, "isGpuEnabled": true, "isInternetEnabled": true, "language": "python", "sourceType": "notebook"}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.3"}}, "nbformat": 4, "nbformat_minor": 4}