{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "version": "3.11.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "accelerator": "GPU", "colab": {"gpuType": "T4", "provenance": []}, "kaggle": {"accelerator": "nvidiaTeslaT4", "dataSources": [], "dockerImageVersionId": 31154, "isInternetEnabled": true, "language": "python", "sourceType": "notebook", "isGpuEnabled": true}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Model Quantization\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/yandexdataschool/nlp_course/blob/2024/week10_efficiency/hw_quantization.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\nIn this session, you're going to implement post-training quantization approaches for _Large Language Models_, ranging from naive ones to State of The Art techniques. The main goal is to implement [GPTQ](https://arxiv.org/abs/2210.17323), with some of it's newer extension left as bonus exercises.\n\n<font color='red'>Important note:</font>\nThis homework is designed to run on colab with T4 gpu. It requires at least 15Gb of *VRAM*, 12Gb of *RAM*. If your machine meets those criteria, you should be good to go too.\n\n", "metadata": {"id": "npIrOYmGWU7b"}}, {"cell_type": "markdown", "source": "# Installing the Dependencies", "metadata": {"id": "Ld5GgOmT2WE1"}}, {"cell_type": "code", "source": "%%capture\n%pip install transformers==4.35.0\n%pip install sentencepiece==0.1.99\n%pip install datasets==2.14.6\n%pip install accelerate==0.24.1\n%pip install Ninja==1.11.1.1", "metadata": {"id": "2v6kSVkV2cSS", "executionInfo": {"status": "ok", "timestamp": 1761053118470, "user_tz": -180, "elapsed": 149995, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:03:54.157551Z", "iopub.execute_input": "2025-10-24T14:03:54.158259Z", "iopub.status.idle": "2025-10-24T14:05:39.495014Z", "shell.execute_reply.started": "2025-10-24T14:03:54.158233Z", "shell.execute_reply": "2025-10-24T14:05:39.494016Z"}}, "outputs": [], "execution_count": 1}, {"cell_type": "markdown", "source": "### Imports", "metadata": {"id": "Xtpmhiksi62J"}}, {"cell_type": "code", "source": "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n%env CUDA_VISIBLE_DEVICES=0 # Change it if you're on a multy-GPU machine\n\nimport os\nimport math\nimport random\nfrom tqdm.notebook import tqdm, trange\nfrom typing import Mapping\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport transformers\nfrom transformers.models.llama.modeling_llama import LlamaDecoderLayer, LlamaForCausalLM\nfrom transformers.models.llama.configuration_llama import LlamaConfig\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "cBpuR3vpi62J", "outputId": "4cc28a23-baba-47fe-989e-61ede582e728", "executionInfo": {"status": "ok", "timestamp": 1761053137410, "user_tz": -180, "elapsed": 18933, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:05:39.497081Z", "iopub.execute_input": "2025-10-24T14:05:39.497370Z", "iopub.status.idle": "2025-10-24T14:05:47.888499Z", "shell.execute_reply.started": "2025-10-24T14:05:39.497344Z", "shell.execute_reply": "2025-10-24T14:05:47.887888Z"}}, "outputs": [{"name": "stdout", "text": "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\nenv: CUDA_VISIBLE_DEVICES=0 # Change it if you're on a multy-GPU machine\n", "output_type": "stream"}, {"name": "stderr", "text": "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n  _torch_pytree._register_pytree_node(\n", "output_type": "stream"}], "execution_count": 2}, {"cell_type": "markdown", "source": "# Quantizing Matrices Row-Wise", "metadata": {"id": "uQkjY8Jp1OpH"}}, {"cell_type": "markdown", "source": "### Basic Quantization", "metadata": {"id": "HzEoWTNMi62K"}}, {"cell_type": "markdown", "source": "**Mapping the values to the allowed range**\n\nQuantization is the process of mapping input values from a large set to output values in a smaller set. For instance, if we consider 4-bit\nquantization, our values are represented by $4$ bits, meaning we can represent values between 0 and $2^4-1=15$.\n\n * To produce the quantized representation, we need to be able to map the matrix values to and from this range.\n * For reasons that become important later, we will perform this mapping independently for each matrix row.\n * We will parametrize the mapping like this: $out = \\frac{in}{scale} + zero$, where $scale$ and $zero$ are row-wise constants.\n * For a matrix of size `(m, k)` ($m$ rows, $k$ columns) we will aggregate those parameters into two vectors `scale` and `zero` of size `(m, 1)`.\n\n**Task (0.5pt):** Complete the function below to perform this mapping:", "metadata": {"id": "OFG1FDsVi62L"}}, {"cell_type": "code", "source": "def get_scale_and_zero(x: Tensor, max_abs: float) -> tuple[Tensor, Tensor]:\n    \"\"\" Given a tensor x of shape (m, k) and max_abs > 0 produce tensors scale and zero of shape (m, 1)\n        such that 0 < x / scale + zero < max_abs\"\"\"\n    # YOUR CODE HERE>>>>>>>>>\n    min_val = torch.min(x, dim=-1).values\n    max_val = torch.max(x, dim=-1).values\n\n    scale = (max_val - min_val) / max_abs\n    scale[scale == 0] = 1\n    zero = -min_val / scale\n\n    # <<<<<<<<<<<<<<<<<<<<<<<\n    return scale.unsqueeze(-1), zero.unsqueeze(-1)\n", "metadata": {"id": "z1xDv2d12hw9", "executionInfo": {"status": "ok", "timestamp": 1761053137414, "user_tz": -180, "elapsed": 2, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:05:47.889167Z", "iopub.execute_input": "2025-10-24T14:05:47.889495Z", "iopub.status.idle": "2025-10-24T14:05:47.894401Z", "shell.execute_reply.started": "2025-10-24T14:05:47.889476Z", "shell.execute_reply": "2025-10-24T14:05:47.893643Z"}}, "outputs": [], "execution_count": 3}, {"cell_type": "code", "source": "# Testing your code\n\nx = torch.arange(512 * 1024).reshape(512, 1024).float()\nscale, zero = get_scale_and_zero(x, 15)\nassert scale.shape == (512, 1), \"scale is wrong shape\"\nassert zero.shape == (512, 1), \"zero is wrong shape\"\nassert torch.all(scale * 15 <= 1023.1), \"Scale can't be that large. The resulting interval is too wide\"\nassert torch.all(scale * 15 >= 1022.9), \"Scale shouldn't be that small. The resulting interval is too narrow\"\nassert torch.all(-0.001 <  x / scale + zero) and torch.all(x / scale + zero < 15 + 0.001)\n\nx = torch.zeros(128, 128)\nscale, zero = get_scale_and_zero(x, 15)\nassert torch.all(scale == 1) and torch.all(scale * 15 >= 0.99), \"If all the values in a row are identical, let us set scale to 1\"\nprint(\"All tests passed!\")", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "aZ8TZiGyi62M", "outputId": "a978be16-28a9-4e67-c475-0b6f27756476", "executionInfo": {"status": "ok", "timestamp": 1761053137475, "user_tz": -180, "elapsed": 60, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:05:47.895465Z", "iopub.execute_input": "2025-10-24T14:05:47.895726Z", "iopub.status.idle": "2025-10-24T14:05:47.951988Z", "shell.execute_reply.started": "2025-10-24T14:05:47.895705Z", "shell.execute_reply": "2025-10-24T14:05:47.951385Z"}}, "outputs": [{"name": "stdout", "text": "All tests passed!\n", "output_type": "stream"}], "execution_count": 4}, {"cell_type": "markdown", "source": "**Quantization**\n\nHaving mapped the values into the allowed range, we can simply round them to obtain the quantized matrix. Complete the functions below to perform row-wise quantization. Note that:\n * You should `torch.clamp(...)` the quantized values to ensure that they are in the allowed range.\n * Some functions return the quantized matrix, as well as the quantization constants, because we'll need them to dequantize the matrix. Use `get_scale_and_zero` to obtain the them.\n * Note that we cast the quantized tensor to `uint8`, but the values themselves must be in the possibly narrower range, as determined by the number of bits. Obviously, we require the latter to be less or equal than 8.\n\n**Task (0.5pt):** Complete the function below to perform quantization:", "metadata": {"id": "8E1DfQaSi62N"}}, {"cell_type": "code", "source": "def quantize(x: Tensor, scale: Tensor, zero: Tensor, bits: int) -> Tensor:\n    \"\"\"Quantizes a tensor\n    Args:\n        x (Tensor): tensor to quantize\n        scale (Tensor): values interval mapping scale\n        zero (Tensor): values interval mapping zero\n        bits (int): number of bits to quantize to\n\n    Returns:\n        Tensor: quantized tensor in uint8\n    \"\"\"\n    # YOUR CODE HERE>>>>>>>>>\n    max_abs = 2**bits - 1\n    quantized_x = torch.round(x / scale + zero)\n    quantized_x = torch.clamp(quantized_x, 0, max_abs)\n    # <<<<<<<<<<<<<<<<<<<<<<<\n    return quantized_x.to(torch.uint8)\n\n\ndef dequantize(quantized_x: Tensor, scale: Tensor, zero: Tensor) -> Tensor:\n    \"\"\"Dequantize a tensor\n    Args:\n        quantized_x (Tensor): quantized tensor in uint8\n        scale (Tensor): values interval mapping scale\n        zero (Tensor): values interval mapping zero\n\n    Returns:\n        Tensor: dequantized tensor\n    \"\"\"\n    # YOUR CODE HERE>>>>>>>>>\n    return quantized_x * scale - zero * scale\n    # <<<<<<<<<<<<<<<<<<<<<<<\n\n\ndef measure_and_quantize(x: Tensor, bits: float) -> tuple[Tensor, Tensor, Tensor]:\n    \"\"\"Determine the values interval mapping parameters and quantize a tensor\n    Args:\n        x (Tensor): tensor to quantize\n        bits (float): number of bits to quantize to\n\n    Returns:\n        tuple[Tensor, Tensor, Tensor]: quantized tensor, scale, zero\n    \"\"\"\n    # YOUR CODE HERE>>>>>>>>>\n    max_abs = 2**bits - 1\n    scale, zero = get_scale_and_zero(x, max_abs)\n    quantized_x = quantize(x, scale, zero, bits)\n    # <<<<<<<<<<<<<<<<<<<<<<<\n    return quantized_x, scale, zero\n", "metadata": {"id": "9ZR0OZ0ai62N", "executionInfo": {"status": "ok", "timestamp": 1761053137498, "user_tz": -180, "elapsed": 21, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:05:47.953699Z", "iopub.execute_input": "2025-10-24T14:05:47.953901Z", "iopub.status.idle": "2025-10-24T14:05:47.959959Z", "shell.execute_reply.started": "2025-10-24T14:05:47.953885Z", "shell.execute_reply": "2025-10-24T14:05:47.959210Z"}}, "outputs": [], "execution_count": 5}, {"cell_type": "code", "source": "# Testing your code\n\nx = torch.arange(512 * 1024).reshape(512, 1024).float()\nscale, zero = get_scale_and_zero(x, 15)\nquantized_x, scale, zero = measure_and_quantize(x, 4)\n\nassert quantized_x.shape == x.shape, \"Shape of quantized_x is incorrect\"\nassert scale.shape == (512, 1), \"Shape of scale is incorrect\"\nassert zero.shape == (512, 1), \"Shape of zero is incorrect\"\nassert torch.all(quantized_x >= 0) and torch.all(quantized_x <= 15) and torch.any(quantized_x == 15), \"wrong quantized_x values range\"\nassert torch.allclose(x, dequantize(quantized_x, scale, zero), atol=50), \"Dequantized values are too far from the original values\"\nprint(\"All tests passed!\")", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "WM8eIsR8i62O", "outputId": "c5f1562b-0c9f-4e71-badd-8586b7dee52d", "executionInfo": {"status": "ok", "timestamp": 1761053137556, "user_tz": -180, "elapsed": 56, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:05:47.960727Z", "iopub.execute_input": "2025-10-24T14:05:47.961012Z", "iopub.status.idle": "2025-10-24T14:05:47.996366Z", "shell.execute_reply.started": "2025-10-24T14:05:47.960984Z", "shell.execute_reply": "2025-10-24T14:05:47.995604Z"}}, "outputs": [{"name": "stdout", "text": "All tests passed!\n", "output_type": "stream"}], "execution_count": 6}, {"cell_type": "markdown", "source": "**Using the quantized matrix**\n\nTo actually use the matrix, we'll have to map it's values back into their original form.", "metadata": {"id": "sBDvbhJSi62O"}}, {"cell_type": "code", "source": "nn.Linear", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 187}, "id": "s9cFwhEDYQ2R", "executionInfo": {"status": "ok", "timestamp": 1761053137648, "user_tz": -180, "elapsed": 77, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "outputId": "9c230ed5-b5c2-4890-903b-401f191008eb", "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:05:47.997211Z", "iopub.execute_input": "2025-10-24T14:05:47.997486Z", "iopub.status.idle": "2025-10-24T14:05:48.002748Z", "shell.execute_reply.started": "2025-10-24T14:05:47.997461Z", "shell.execute_reply": "2025-10-24T14:05:48.002184Z"}}, "outputs": [{"execution_count": 7, "output_type": "execute_result", "data": {"text/plain": "torch.nn.modules.linear.Linear"}, "metadata": {}}], "execution_count": 7}, {"cell_type": "code", "source": "class QuantizedLinear(nn.Module):\n    def __init__(self, quantized_weight, scale, zero, bias):\n        super().__init__()\n        self.quantized_weight = nn.Parameter(quantized_weight, requires_grad=False)\n        self.scale = nn.Parameter(scale, requires_grad=False)\n        self.zero = nn.Parameter(zero, requires_grad=False)\n        self.bias = nn.Parameter(bias.data.clone().detach()) if bias is not None else None\n\n    def forward(self, input):\n        return F.linear(input, dequantize(self.quantized_weight, self.scale, self.zero), self.bias)\n", "metadata": {"id": "NMzGxzt-i62P", "executionInfo": {"status": "ok", "timestamp": 1761053137648, "user_tz": -180, "elapsed": 23, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:05:48.003320Z", "iopub.execute_input": "2025-10-24T14:05:48.003545Z", "iopub.status.idle": "2025-10-24T14:05:48.015763Z", "shell.execute_reply.started": "2025-10-24T14:05:48.003527Z", "shell.execute_reply": "2025-10-24T14:05:48.014946Z"}}, "outputs": [], "execution_count": 8}, {"cell_type": "markdown", "source": "This class will be used as a replacement for `nn.Linear`. It holds the quantized weight and only dequantizes it during it's forward passes.", "metadata": {"id": "ncM_JzOii62P"}}, {"cell_type": "markdown", "source": "# LLM Quantization", "metadata": {"id": "UPaMiUC91VYH"}}, {"cell_type": "markdown", "source": "### Preparations\n\nRun all the cells in this subsection to download and prepare the model and the data", "metadata": {"id": "2HRqro7X1eoT"}}, {"cell_type": "markdown", "source": "**Downloading the model**\n\nRun the code below to download the model checkpoint.", "metadata": {"id": "ipghRjSA1jVN"}}, {"cell_type": "code", "source": "!mkdir model", "metadata": {"id": "JIUbMToO3C9B", "executionInfo": {"status": "ok", "timestamp": 1761053137710, "user_tz": -180, "elapsed": 83, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:05:48.016405Z", "iopub.execute_input": "2025-10-24T14:05:48.016587Z", "iopub.status.idle": "2025-10-24T14:05:48.160862Z", "shell.execute_reply.started": "2025-10-24T14:05:48.016571Z", "shell.execute_reply": "2025-10-24T14:05:48.159864Z"}}, "outputs": [], "execution_count": 9}, {"cell_type": "code", "source": "from huggingface_hub import snapshot_download\n\nLLAMA_REPO = \"Enoch/llama-7b-hf\"\nsnapshot_download(repo_id=LLAMA_REPO, local_dir=\"./model\")\n", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000, "referenced_widgets": ["3b799dcd0dc941feaec7f07378dbc6ca", "76f4dde5647e4adba4524b0dd2ca060c", "7d91e338660f4d08a5e24c3f829ee8d4", "c045fd6085a44f45833533545f641a6e", "ec68cca7108c43b3a957a1acef5fa690", "3372eb07f5d94418b99511e2e9073b7b", "a93177a8c5c7470582db137b986f739f", "c83cd8ee2890482f9e91f2e59ee91051", "cd96fe150dd44646bad62921a2607147", "ec6b39470b824a7697653e94a999fdc5", "72bcf73b261743f385fa77ce8b409b01", "11c2ce3c7b8e4a33bb1c83daaa5dcac0", "15622303f5e545b1bb7a4f8320a5fb61", "aa3b5bb640d542bb9e48b65ec48cc18d", "af763dfb792b47e3b22803c3bd351c8f", "bf3a61237ac44f7db3878b5e2d55ec01", "5e2e7b28ac5d46f6b042d6bb08e88702", "1d1177922aea4f57a623e99c55d6114e", "5173d75e90614646bccad831135816a9", "448575090ab2444f9960f037aef4c5af", "caad9f1702454ccb92ec4a622b1edfee", "f17fb02e39af47ccb22a9b0f46da06dd", "0ee6914c077b4fb89b41a328826009be", "15a12d9dd3254bbaac87695335b69873", "2789facebcf7445b987217a274b92661", "0183a11d35a14993852e2a60eb8aa9a5", "3dfbb1e0d5234362a3eb1e6fa8b17423", "7a4f65a97c114829a5c9d59c380a3a53", "ae65e2447b6649dd96fae6157d622ef2", "c88633731b8f42c28dc2dbc1048e94dc", "27dc712d52864c3a90f207a25dd539d1", "ca449d2522c849b4b3dd2a35db94a497", "fc83004c19f34fc59b52a3fec1a6d34c", "5d4bd373e9d143e48fb856ef09057126", "76d429bb42eb4fe195ff9aaf3587b8a3", "ed1c9e3080524b838a8b4030033c9c08", "e83d8d1d950f4a318806ceef6e9052e1", "26ea138e583a48ffa8191893e0e425f8", "4dc1e13c9e8f4f7e8ba1459e2e5fff11", "74481644039645a5ada76cf40d9a2699", "392985454b874d7ab3cc1266464cb9ce", "4915fa60c1704373b7a29f97e9f9a545", "6f4f6da6d5c442a39b23cd9753e7cb84", "c3281e7bec9c4be19aaf2dcbee2e3899", "4a63c9fcfe9246ff84c152ac21ff8792", "a8e64f2e770a450aab5b1d42d5b1225b", "0d3d5cbd84084636a253575c688297ae", "296cb9f1169a4362b7740aad09921ba4", "dbc1bf9c7044409b9e93e21b7f793675", "417f19307bc64706ae97bcc5071c1533", "ccde0fcb39b44e7aa80487fe49856246", "b9669fad484648b884ed5f1df3bc5e2b", "a8455c0a3bda44239556d3187f22fcf6", "9d800c0825254eb39239a243e4dc184e", "a13c5f6c7aad45159193771b35bae2fb", "129442c8df2448c0bbc9a738e40f47d6", "258063e594744ad6b9fba1d3def014e5", "241960a4e24c42b189908bbe7e577cd3", "514459f0541048dead058320989e7b6f", "7f826e5750934231910dd3671770c80c", "25eaa0be296e40919132deca6d5588ad", "94ae9e7e5a894f9bbf510f8010dca8c2", "6c6ecc958dc144d3ab9f84931d17a544", "ac3f63c376404fff8c0f2aebb41b9b65", "40e4287b0064400085772786e6bfe1fe", "95e024590a854204a74ce992a75493de", "b1ff7ae0311342f9aa7071c13e77fb02", "fb9c3688f979496396fafac85ac5e77a", "8e7d05e3e1f64b588ebbc1e6662fe367", "4fd940b796e64d1c8b39b2634f50e6d9", "f5eea3bafe1b4f10846710c689ba8421", "0c2635f835a64c8792ea4c0902fee7a0", "ea2bbcf569a446279fafca2743a15ca7", "e589222a16e14900baef36e952c985b0", "2a216594fd2e48ecaca24e6cd4d3c2b7", "d6555c526b3944a1ad3136a3014dc6ab", "8ffc3ae7161c4f1bbce7a10f1f195f5f", "4c967033214d4729a8d9affe8efd7768", "8c0bb68970894ff4b2d90627d48f3170", "149b8bce79ec435e9eac93fc1b6e5bb8", "b158da5b580e4ccdb708d38ad23f88af", "58e5dfba1b104cbd9bc9b7caf099849c", "a4ed40aeb77a45a4aba8eb989cf760f0", "de6600b8234f4b248e2745e76f261913", "bd40d1851a0e4d1ab4b6a344506e2474", "87b5787e9dbe4e03b7b469f5830d5fe5", "a54db37f8e674b188c3580fe52943663", "fc399641fd5c4177a2d8ce12c96b58f9", "7b4ab7fb8c874642b255d16e4a887ee1", "727eae7128fb4528be11cd91e776bed2", "58e89e3111a34144aa873b4a8d277966", "9e8dfd40473544f6885610157ad6be64", "e4db45148a404a9bb870fe2c349b639a", "c1aa1be0fcaa4d9e85e78825e8a904f2", "3f543d99d2314ecb858fdebc9048bde1", "c439b797748049b3886f3ef946e30802", "559ec8b8de324d65bb892a2e20f32816", "f983030bf2b1413c912357f56f3ae713", "6c222733363e49f7aecf302c903478ae", "865ef081dea245ccb236f05ace388fec", "260d0fc55af847ab8dd93b10a87e1bb1", "4c98756f86b949d2a261239ae4f84ca7", "0e8327e7c2a84c26aec12b5aa2332c08", "1d00c3cc65424205aeb24358575281c4", "bde29edc285a415a89c9686359c7c263", "b1f8130cab41418bb646b2fea2871cf9", "761f3e115676460787d5e303cc998d74", "faf3dfabf278470787a83e6da8a335a9", "a20a185f1c8e4411b122caab2f560e5d", "47b0f30ff60149aa9cbb49e4f14790f6", "d025510842094cc0b25d1e72a4f4094c", "807939a4e96b4bdf95da6bf4ca791e90", "dcabc8e6b4d54918bafd04b8e3e5ec33", "4adaca7365b0401ab1b10dd5a9202a72", "abd032047e544af7ab662435cb6447e9", "955912dc5fbd48e2860a7b36590e3418", "4019eea6abd449f280439034e16c8ee4", "1351121529bf43b5b3063eed58ea1333", "33afd306c8ed40d8adc2d87e9eab8c92", "36b3978eb14f498a86bf063afbf136b0", "a39f6be685864b9ca66a5437c176d3be", "9aba79e141e5443a9ba24e74897c67ac", "8aba2cbc03ec4cb790a944ef524aaea9", "d8d9afc15e5345f39dd75f4620bcaf53", "0aa91ebcedd34a3a8f0c03518e69fc23", "544273166fc94ba6818b51a2c3c06211", "9873fa8adf974990b198d1e8a130f5e7", "4b6e50aebda0494d97622ff66df99698", "14be7f59c4be40c78010f6d70906c01d", "92e8184bf2ae48d5a3aeab535d4c3f9f", "17e87b87e0e147d2957d5a34a7a701fe", "c670655466384f0382b7a227076d271e", "1cac3b2ef3974c629382c3932f7b7855", "4c95f8d9aa91439fad1b56c2967e2b71", "480a6294b7cd4ac4ada1f63dc29f0876", "e7768711048844989fe2354cc88bed3e", "d558d118f0c343c5ba57016f3af7acd2", "74dab50239d940c59fd41e13ca40244b", "3dd781a6226343baad62ece23e0c5b14", "8a4d678cc2f34c1ca6fe6a8c9531eec4", "2cc557400e8a40ada5ae4da1e0f068ba", "e5a7d5be43434b1c96fb196a397d166d", "d46f209f9eeb445c816e8b1226bed1cb", "94566d2960f749ccb797dee0fce924f6", "343f7722fed14b0b8e90cf003ff8783e", "aa9438664a6d40f08149aa2d011cc91f", "679a5b35f63f4341891fc075e001c8bd", "d8e62176b0894693bccc15f0cbee6f6d", "ab8ae9b679d2499081267c7acd4a1acd", "531809ad5dc0450086684f2143e0b9bd", "fed49aef1ceb49aaa91cc875101ba029", "50c1fa795c1d4cb4a31f725968e6bba6", "b8442b04de5c44b48bd0634b40469464", "69115081fbb048e1a2aa4cdafb40f1fc", "cf6f5900fdaa408abdde87034281e3c6", "f2aa600ff9d742eab8a493c45bb2cf4d", "ca9d0f214315487c87dcf1ec396cd2c3", "e96d7671a7e748fe966cc5c427bdba4b", "bf0d57ac80494494b9d830f7b273010f", "4733550444884ede89a0ea2a448784ec", "9beb7065cb0d4452b3b400ce9a04775d", "79df1e7125d14046b5209e74efa55416", "d2fe5e6e05034ae5a47bf2287284b9b2", "c84dd84c02754cfbb37db45d5159fb98", "4c4efd0caf894a1fae9f04c9668ec7c5", "eb48eb7aa7614d64a720b7e0961281b4", "c5f4d16ade334cd8a122c6f7ee06b02c", "69e424d70ee64e75ae86093236bd89fd", "bc428e1d8da74aa8980c2051147d9b98", "65743cfc57c944a68a40a9647476c080", "0c89f15d99b1461ca1c62f9569e98d8c", "af60482cc25649d5a93139ffa5ecae93", "4d91299059ec4894b332c4bcfcda7e49", "46a1e26bd9314156963265acc713a160", "c0e087aee0304db3874d7806a2419245", "0bfd25064c1441d4811227122b47ba98", "626c761587714b91b27ecace9554296e", "f1bd790b87264896aa116f1519d3cbbd", "e45d0c2dd5c648a3a7c6f7707ad2cdab", "2c92fb50ae3540cdaad090f3b64a790a", "01e6fa0219db4c53919adc29fa043e06", "31300225e1ad40b8955c72641d66c039", "5f2ae26573d74f66b1fa679f54d7ff6f", "89dcbdc5fa5944b6a3f7ba0a06310e08", "e4051fbc5fba424bbe178f25d912c865", "9dcbc87134e0472b9b43f805a4ca12a7", "3cd90ba8c47b413c8ea2af867f0474b3", "e3308a92cb5a45d085abaa230cdb0ed3", "8d92782b68824f34b7d67728ce1c3196", "82a8f43662874c82b8855384a876a841", "3b8c78739ebd498fa5d7d2839b426863", "fc7de318c7c44c6fb8571641e3abffa7", "da2c5e1793714135b419eeaf6be5771a", "217daf48a9d34d51b327453d9717cb77", "b8661bb2eb964fa8bb907315fc01fe47", "0ad6e045cb7a4f94a2818079424863ce", "ac7fcf2bba6d44a09f58f6d230f6322c", "30dac0235e6e4fa89cc976268a8a9a65", "414e7d24c8284dbfa55987125e43f1ca", "a3db1d4c2e17447a945e57a9423faf2b", "d561166eacb747339c93909a8200f43c", "b2b0a93716b54619a3a69138d47aed5a", "9fd23656516649c5946f530b189baa74", "0f0483b531304d218f9e492b9c51c742", "1c5d1b3f2cf142e385c33e17f2965db3", "a832402394c642319741c68d832bfeda", "207b5c5a6047494192df1e3a9939ace0", "36e3c2358e7449ceb4b9c7c26d09e487", "7fdc136d2b024236ac38e89cc1300199", "591b0609fa7b4c37880037e8c7f7bc71", "0c7afaf5e2f94ace8ecd77e055b8818b", "0097dc34279341c9816748decafa778e", "b25a29c4cfca4558b562185c3c785489", "4b2c4c9743e848baa5bbe13882d5fd81", "b41b92111c914afa921eed69e42c8e0a", "ba20018872af4c948260ebc5ca8053f4", "b8564cbce60a4927be0e8484760b9349", "0e29b018e85a495aab3f0b26511fd07f", "7a0621b830dc4ede8129e5996ab65d3e", "5787608d014d4c25b8a16b018b887651", "6b349ffa526e47669a61e84482351bca", "31297c9202a34a15bd517060516c4986", "6f2d1fa963c0471392df2633a3631460", "2ba2142a32dd40449503d1f349d92925", "a2e59e62408a4998bb53e285e42d2d17", "8a2b70181b2e4eaeb0f4050819433ecd", "ef733e86ad6b4708a93e6075c427ab53", "93c76e4b938643ddb52e695cde319b2d", "e85053f58fc24e9eb30ab82c1d18bf1b", "2e4e9a2933a7419c83da47a13e8af4cb", "ec675e832afb46dfa6cc1c2bd07694df", "4167fd19602942d88f2af2332abd5985", "5a97a5d5b2d344fabaf4f6d2fee8dfef", "f91e051cf2b343bfb2f3360a0e3d886e", "dca058c0c967446b816e007aefe47b09", "8daf835741864cbc9dbc5efee085efc8", "2327e45550f74ccb906ed3bd1d0081b1", "f3e971f88cba47d6b133ba254e02afd1", "f62240c8676d4d0d91f24700e193e19d", "17f1625108cd4d30bb98fb5d23ebf738", "ac14ac308cf84756a96cad3031c0df76", "0bd0e95ada8a4b62a986496f51da7a19", "e456ff5da1c84575873bcdba1f8da080", "9941c2fb3aa44747b632af6ce0c102a5", "bffe72db3b544a9d9cb9883badfbfb61", "1d0973eef44b4a0da320dae96c5150fe", "ee72d18c98674b0f8aa309536ab5191c", "985d78ae315a465abd06c611b40fbd86", "eaed833fa2c6427c949cbbfdf0b734d4", "e82bf7b769fa4b1bb23c730172b18e07", "266a4ae50a4e475ca0dd3d609bed8993", "f97bad8e67c8433d98a5a23a8f99e208", "32c1b354fc0a4db2bbf3a8ea17eb66ec", "ebaa926b6e8e4b81b5e70ac28fc1bbbc", "99f5994a75dc4f5a960d6951b3201b34", "d30706fb65774456a87c810c93ae0243", "2a455492c0e6465a838229e2983aa383", "daf0f5b1c8c84cb594060b0d099b1c2e", "e56b9c59f10345efae784cfacec3e22e", "cd5542b0f012487f80b5d39666bbf5a7", "862224161fab438897bf2ec29e36d852", "088327fa60e84c06a94f127efe280acf", "b62554370c1e416980da24bcd173fc46", "80a8f620d2dc4c1e9cc4b87ace52202a", "9bbff9c0e0a4469b9b9478d1680c0163", "3356556fe3c34c52b311ef5c55112676", "2d6d7c5d736043afb99043b5030de5c7", "b79b6ed08f5b4804a0334e3ccceb88d7", "7ca7455b927949229050b01e182d48c8", "f0d14fc3864d41d1a3f052d4f6d610d0", "5551f91cafcb4c8aaa87ff13e2fb8908", "e70ec8b686234d42bbcd2e27f71f7ce1", "a9783e2001ae4877b90eaa1599856837", "a92cb038fddb44539a079892c478aaed", "115179abf1d14cacb2bdb02cdc4c4d72", "b4873ad7790541a7a956cda081d4b57c", "30c7876a73bd43469618bf46832fd673", "2cc1a5026177432e945bc00550ff16b7", "2bd50dd2ee5a46d9a674ec048f165b81", "37dae90146d04525bbea62804a767a18", "c571765e835e4cc786c4db8d2d54bbc6", "0c0ea3d235474123961a830f7e372b0a", "b60dd52688c54f2a999af958de2ab4d1", "72f9ee17aada42159417b9894d039fad", "74f4b3bb601b4d6aaecbf6b4a8575904", "b2b73f962f8c4db79b962095bec1ce49", "8a773f2e389d456f8c3edb829bbcae8d", "1948f14dc7a9487280935511b8edb292", "16c0c870a3f64630a7fe8ef82a6db7e1", "ee9a2ca0db8941c2b6bd9b097e28a516", "381ea7e98f9f475e92b3c0992a45cb3a", "ea3d026bb84c4bf1a3861a71ae832f23", "9b8323ca96404074bc33174470a98513", "01ad84a579294b078fc45b6f6dc0bc07", "b402517c6b5f48b293c41fb483012f4f", "007569b6d5d740ae91255a5c62c5bb8a", "1576e4ce078d4902ae5dac997367a022", "3f1abd507f5f4ba3a108ebdcdc62f033", "04bf198d88e54ef1b37d3e915820e9d6", "d1720b1367c84962907402cd0173b8e5", "7edc9050c4f84b5d9d0ccf7417a103c7", "b62a1f70a4c54b08a45996f2a8d1b440", "e2e6d1de16174060b803f2e5c4ae0ba2", "92ee397e6c6e4392a57b88983c6195c2", "66dc5dcaab2648a48a00fc2caed84ca5", "8e0d1135190545de8f1de22c8cde9600", "90e4b7ffd3cd4c4a9b5ce90f2a82c0ae", "ab70cae139394d9b9d2850e18915b0df", "c749a47d931a4018a73b375453ceb9d6", "b7d3e53692874dc4930cf0e65e66149b", "d1fb783e2b904020be212ff1422ca875", "3fe63dbd4a5146f3917ee9522a5e0509", "ea784aad268642d8b5e74c6301db3d2d", "0aa622fe468c41daa270656096785fb7", "39236be1beba4ea69ff40f53e0817dc2", "4c7736df189244359cc60de1bbe49a54", "0311dd65498549f7b6c325e62732abc8", "d9bbcd92bf7f4a51869d9c2e3e621041", "69b35b1461ad4bcb9a9fe7b2ce581fbd", "52f50f3cfd4f46f5a7363a34bdada54a", "beecca727d6a4077a1f9e9b324b228c3", "4d86b57b6f824f89bee1fe7a3c84a2b2", "cee786ad75e14c96adabb81013ee2b85", "ac779395b04e4227a82a10355b9af78b", "9406f083607b4f0b8d9fb6f92045bd2b", "f5b608e77f9b4fbf8500e0ce35abf41d", "c2c5eb3a2ca64513bf74293441cbe683", "3b569d7c613a46f48199da6f3a129424", "df55fc906b474ed184ef5bd68135e4da", "306364d4ac9449c2966c4c799fdf8f45", "07caf12f013d47d881c70998421ea387", "c62ee2165994466680f9b87a0c7ee4ea", "6c96948d72fa4fd5a49669414790e1ab", "5a1c9fcad6214af18465fcc757cd80fe", "e9d0a697106f423eb650479302b5cee4", "3cab9e95166747db9080c9ecbc3c8a89", "d6d48e7e0afd4f9f810ccc3e5167d996", "0767119efea24e4ba47740d00ada43e6", "a503cb4c817f4dad89bf699a9547131b", "47f718bea2cd4e5c8a04dacc1263fc88", "64e55678ab4c4836bc2fc5b996245374", "e5c58fa0918d45aa8f578538fcfb35c4", "5773b4324cad40dd8c9a472825705a5b", "156024eafd9f41bc8ef408356ec030ad", "fa0d432d2dca4edbb9523850ef5c1a60", "0bfb3aa8d0a2458abe9550a97067a428", "e3f96b25aa784153b7b99b5a37a650b1", "4e1a9f8ce4364d6187cae4f874221317", "c10bc874877a4f0f85bed9a0811d3cf1", "d8ca78a0953d42adb6025c0f0af291b4", "26fb7d7e5d384792894ea7f72af5b6e8", "e70b73efe4324b6eb0cb02e35daa7139", "ec304656237c4f158d60582ac14dffbf", "27aa35350b254961baff8b82b89c91de", "8234ed6cbaf9450d9629cffd2116ce33", "5926907e396440af860ec070e3936c30", "927d53244eda4f7d9021b2b5a3634883", "40af3391927646de90602a2b0eda46fd", "c0bdc7c226f44b6a9bcaac65838c10fb", "48927c0ffc684cb5bd3e9c4881aae6bc", "8474c3724d6345fa830ed8089b89e667", "51d2497827a04c90b5ae948d72576c69", "ae4505711397433599ddaf3604cfed3c", "1cf1fbb81fcb419e96e0dc0e22b66fc5", "629cb83cd35e451290c8b3f9c2e565c3", "9a6c7aea1cfc44a28c6f31549fe17bcf", "5a583dbec42b47c3a8733c538c831e7c", "9a8d22699a2948688c9f1c8efe9a53b3", "c33ecaeac7754cb2938621dc6aed8659", "a2e0af2fdab0415790a1f3e2a86fd395", "8ad85065ea074c23905735ce3cf2ceb5", "d3c523c7f3c544c5b3093af3abe62239", "a4a2dd6ca3444bcb999e79b866cec218", "02f803dc80c249e5a287168a053a290e", "2fdbb9fe78454257b8b8aaa42bfaf0c1", "3eba58d18f7142b4a8af487edbf3e526", "04a2f55480724f4c8d0aba4ede5992e0", "9c12d1c7e9b74a63b750b8185913eb26", "6ec3eec584ec420e854d5b4e3c29bb28", "b48b9c20a232445d8f829de8375e5080", "7301832014fd4e768452cdbdde650041", "da51b78b6625444395cd46a2e6b2f962", "9fb130528af44deab325b2e4d93a439b", "4a3a01992596415aa9ff683f35cdd171", "bb771a567925407f9708ed4309b67c67", "8c493f202bd240dcaafe7f485b926cee", "38478fd7c6494addabdba8225030aa70", "2f7ed2a491344d0f89fdacba65f32680", "5eff9b02d2a54cfaae8860aeb8f082fd", "e0ff20c090ba46b2bd659f694bfd5b72", "7e6c35573bd84bcdac58c5e3cee1cb93", "21d1d1effc3c44baaec31f569c772be4", "b769042504c240b882917250bf01b0ce", "1ba7bc2e42ae40ee81a2c11d5ce3b93d", "954dccfe84104e0f93d02c528b577aef", "fa77d03f716c421ca33d6eefa837fbdb", "0ead574411c948edb104176683243960", "281c85312bc94cd783b87fa5b2fcdd5e", "d50acc72011b410580502952d29df8d9", "842a43dfe6d04467b1251f42628e5bf2", "1b8f2f2fbf89496dafcab360888a9873", "0fe8afaaa25849229f0ac9e1856a3f28", "ef9aee92a94a4d0a95a4472b147932f4", "347afc08c0d943cba2f1545f804840a1", "1cd04938af7c488aaa0225482c50a8be", "a1836be95d05421d918559af6048fcfb", "0aa233c9d8054b62980150f998863cee", "cc5fe6b62ae04607b1330889aa6150f6", "c35191a8856d4191b04fd91abd4c16b6", "fc686112f14c4a24930e72a42f0ec696", "dabfafe3dd0e49709e504832b0e464fb", "58e37ede9e024bdf981f12bf851bf8ad", "94349121596c4451a069c02ccc3f1b31", "225df92dc86b430d9da6fab329754e31", "cf62accb72e14f249324751d1e877970", "b459f717261d4fd693f2db390cab59f1", "0102b6c7d0744cfd98105c451d74c9a9", "ba017208daa542e580a74b65116825fd", "7937d5cd986e4f87b59f6d8b3e6e889d", "22d8fb89675c4b91a001ec153c774051", "5b3955e40cea4d9e806973224a02fdff", "739d1664fc11412c9c8e8f3db9605173", "af47bc4843294cc68947826bd5f7aecd", "33c5afec0e0d4b9e874f6b839334bb85", "52f01a5b6793442bb03692a673625443", "8007cc7d653741a58f7b4d2efa105fec", "b540f1dca09e408698d1737626819bbe", "fc7706cc8a4345f297e384f35b21f424", "f566dbe724cd46419966a25aaabefc9b", "1e7c82a7587344ea825a1978ee63c7bc", "ab05074093b54f20ba486d75ddf4ba46", "60509804ad07476abd0e3b5805932c42", "dc60a6d33c084cb99465fa96d81120b0", "155332b4fb0543439d16648684b41229", "3064d13c32c341a8b2addc7e6d26e53b", "8edf5a449fdf469c937b6ec74a5fe235", "3878fcbca09a4151a35be3324c7a219c", "6638797a2d42402897987bdc97ac4e7a", "2404142c58fd4fd39d185d5cf5ae7db9", "30d8a388aa3444aaaa85b471e3983c99", "a4b9a2a7766b450a961b74c193946bfc", "14211fabe45d4e7d882c786e0d92c727", "d69fe92fa5ed4994962015bd1bded938", "362a160c9e07487b90f96217d4ab027b", "5ccc6ec9959f4b4889f300deac798da5", "480108084398428097d7331b1deebe35", "bbbf683eecaa4457bebb6eae73a2e3b3", "be2f5ce976fc40b296d4cd0aad4a8a2f", "d913627b95a6400eaf0f4e4125a4ecbe", "88e4ab2d403b42fcb2258b9d9afc1876", "cfec3d141c604ded8a8ea81413568418", "8fa2b0f8daa040d6bd90439299ee23ab", "b8f50da371634b89b9b59a42a71db67c", "3effb82a716c4292a287bd60e76fcb3d", "37adc9bb88ec44ef989572c04bf4531e", "5f443c8736bd4440957d01c761ddab1c", "20ec0ff7ded84752899d6341c6869abb", "c09c7fd2b3a74f0885b74a718fff27e6", "b8b8f9de14494185899f5099923646b0", "8486bcc3af114d1e917f961c8f6ecde9", "e58d2c93bf3d41399fbce0ed8a3a9a34", "440605605ceb4773bb1e53b5d4be9e93", "a63cce50a07b486196fc624958df4993", "019693c4618446adb9a664d94c5d8ae2", "55b479582ec847e59c03151be3f3af2d", "890b1136eb654e3cb969aeab605f586e", "b5e3c00fb51740bfb3c6b71527c0b005", "5982bbbbc5314240864b2320b57b78b3", "597553efef8f4d1d98919ecd0dc28a26", "fe5e1963e4d74314ace16a223afe4dfd", "5f74d94e26214d4b889cea1364b1e2d0", "dd57b363fc5f44adac49296a8bd7a9eb", "a8ff98c1f91542848b22c8464164e338"]}, "id": "j9SnAHdc19ap", "outputId": "e27cd5d9-9538-472e-c20f-85188fa24838", "executionInfo": {"status": "ok", "timestamp": 1761053206495, "user_tz": -180, "elapsed": 68787, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:05:48.162156Z", "iopub.execute_input": "2025-10-24T14:05:48.162407Z", "iopub.status.idle": "2025-10-24T14:06:32.437617Z", "shell.execute_reply.started": "2025-10-24T14:05:48.162385Z", "shell.execute_reply": "2025-10-24T14:06:32.436227Z"}}, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Fetching 42 files:   0%|          | 0/42 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "af13e3c9bf354c16932dae945ad01a1c"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading config.json:   0%|          | 0.00/511 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f3f80173769f41efb47ab8d2d272e33a"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading generation_config.json:   0%|          | 0.00/151 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b5daf33d29b247ad9c4597eacfd81115"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00001-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5e60c90283024673a6ad02219389a31f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading .gitattributes: 0.00B [00:00, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ffb5c29bcf174d2bb151986e7b61a952"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading README.md: 0.00B [00:00, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "cba8ebd565914e919647b7930305acdf"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading LICENSE: 0.00B [00:00, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7662ad5018a744bf8fbb70fe8c78631e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00003-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "83abe1c84bcb40ee98978c6285d33994"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00004-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8dacea1060d04d2faf9dffa166d38dfb"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00002-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2e196141a1b641e8bad160dc804efc01"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00005-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "fee329ca7982449a8be97ffc9b02ee9e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00008-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0dc627d1bb4d40bcbebab35434a4db08"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00006-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "deefccc16880434f80b4e210d2bdb308"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00007-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0e70a8091bde4efd8e7cd5fae469c24c"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00009-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "735ce53e755e4749ab65a833f95e6c78"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00010-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d5cfd9a4170c4536a097eaa8755866be"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00011-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0970c5ea504542c793cdf783541ac47f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00012-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e89fcac255274afbac4b89ab7563cc31"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00014-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2226d464ccff41079781e6d84f8fb27e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00015-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d745d8f3f29b486dbb6b3f0c01c78d30"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00016-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4c96f7b34e6542738b2dc5737cac835f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00013-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "be209d36c3254a5ba26f112f71477b3b"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00017-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0eafac901a464bdf9cfb02a5783d2450"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00018-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f4e6e8b5d5664ef585f51c6bf3d4276f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00019-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f4e3a432af2949d291f03718943477ec"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00020-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "008a7daa818e4a8989db750b6927428c"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00021-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2f85b47b9aab47288654218758309d88"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00022-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "893870c708484e21a4634a9d5c1ce6ba"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00023-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6ad28bac4e5f4c6aa319ba29765b9adb"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00024-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6c284d047a9949f5b153b7c252d40cb9"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00025-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3dbb115afbc74c6888a5ded2f2c6ea17"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00026-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "60796beb2f28447c8724888204611533"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00027-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e6649c26781b489f8c375c25f3378f9e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00028-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "311faba291aa4a5f94e70d964612e9b9"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00029-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6bd2b1f799ba42b1a928404c9ddf820f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00030-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "24b84a2aa11c4b58b46b11c65aca7cfd"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00031-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c9819192894043bea0056e000c392b3d"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00032-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1b67dbc2bc4c4146b31f835eaeb810b0"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)l-00033-of-00033.bin:   0%|          | 0.00/524M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e4cb7bb8e8e04229b571a14b34b12b99"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)model.bin.index.json: 0.00B [00:00, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b2f471fbc5f94965aafde9c03c65caf5"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading (\u2026)cial_tokens_map.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "58b5b5815b3041e99d7ebe30505e21fe"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a6a8f0aba6394cc9b1abd3cc7eb67dda"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading tokenizer_config.json:   0%|          | 0.00/218 [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "dd605a7934724f24bc43afac3ac85fb2"}}, "metadata": {}}, {"execution_count": 10, "output_type": "execute_result", "data": {"text/plain": "'/kaggle/working/model'"}, "metadata": {}}], "execution_count": 10}, {"cell_type": "markdown", "source": "**Dispatching the model**", "metadata": {"id": "MKQSag7f1nJC"}}, {"cell_type": "markdown", "source": "To properly quantize the model we'll need two functions.\n 1. `initialize_layerless_llama` creates a llama model without any layers, but correct weights otherwise\n 2. `load_and_dispatch_a_layer` loads a layer inserts it into the model after the last layer", "metadata": {"id": "HrNGVHZhi62T"}}, {"cell_type": "code", "source": "# Disable fancy model initialization since we'll override those values anyway\ndef skip(*args, **kwargs):\n    pass\ntorch.nn.init.kaiming_uniform_ = skip\ntorch.nn.init.uniform_ = skip\ntorch.nn.init.normal_ = skip\n\n\ndef initialize_layerless_llama(checkpoint_path):\n    config = LlamaConfig.from_pretrained(LLAMA_REPO)\n    config.num_hidden_layers=0\n\n    model = LlamaForCausalLM(config)\n    model.load_state_dict(torch.load(os.path.join(checkpoint_path, \"pytorch_model-00033-of-00033.bin\")))\n    model.seqlen = 2048\n    model.config.use_cache = False\n\n    return model.to(torch.float16)\n\n\ndef load_and_dispatch_a_layer(layer_idx, checkpoint_path, model: LlamaForCausalLM):\n    if checkpoint_path == \"TEST\":\n        linear = nn.Linear(16, 16)\n        linear.weight.data = torch.arange(16 * 16).reshape(16, 16).float()\n        model.model.layers.append(nn.ModuleDict({\"submodule\": linear}))\n        return\n\n    config = transformers.AutoConfig.from_pretrained(LLAMA_REPO)\n\n    layer = LlamaDecoderLayer(config)\n    layer_state_dict = torch.load(os.path.join(checkpoint_path, f\"pytorch_model-{layer_idx + 1:05}-of-00033.bin\"))\n    layer_state_dict = {name[len(f\"model.layers.{layer_idx}.\"):]: tensor for name, tensor in layer_state_dict.items()}\n    layer.load_state_dict(layer_state_dict, strict=False)\n    del layer_state_dict\n\n    model.model.layers.append(layer.to(torch.float16))\n", "metadata": {"id": "ls5meN1c4No-", "executionInfo": {"status": "ok", "timestamp": 1761053206552, "user_tz": -180, "elapsed": 46, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:06:32.438654Z", "iopub.execute_input": "2025-10-24T14:06:32.439312Z", "iopub.status.idle": "2025-10-24T14:06:32.448916Z", "shell.execute_reply.started": "2025-10-24T14:06:32.439281Z", "shell.execute_reply": "2025-10-24T14:06:32.448198Z"}}, "outputs": [], "execution_count": 11}, {"cell_type": "markdown", "source": "Calling `initialize_layerless_llama` and then calling `load_and_dispatch_a_layer` for each layer in order would fully load the model, but we'll also quantize the layes as we go.", "metadata": {"id": "xsOwpAvbi62U"}}, {"cell_type": "markdown", "source": "### RTN Quantization for LLaMA", "metadata": {"id": "agCFfP7x2Au6"}}, {"cell_type": "markdown", "source": "**Auxiliary functions:**\n * `find_layers` takes a module and returns a dictionary containing all of it's *Linear* submodules with their path-names as the keys.\n * `replace_submodule` takes a module, a path-name and a submodule and replaces the module's submodule at path-name with the new submodule.", "metadata": {"id": "zwS6bTIwi62V"}}, {"cell_type": "code", "source": "def find_layers(module: nn.Module, name='') -> dict[str, nn.Module]:\n    if type(module) == nn.Linear:\n        return {name: module}\n    res = {}\n    for name1, child in module.named_children():\n        res.update(find_layers(\n            child, name=name + '.' + name1 if name != '' else name1\n        ))\n    return res\n\n\ndef replace_submodule(module, submodule_path, new_submodule):\n    submodule_names = submodule_path.split(\".\")\n    for submodule in submodule_names[:-1]:\n        module = getattr(module, submodule)\n    setattr(module, submodule_names[-1], new_submodule)", "metadata": {"id": "8vo0UYKvi62V", "executionInfo": {"status": "ok", "timestamp": 1761053206553, "user_tz": -180, "elapsed": 8, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:06:32.449870Z", "iopub.execute_input": "2025-10-24T14:06:32.450390Z", "iopub.status.idle": "2025-10-24T14:06:35.346411Z", "shell.execute_reply.started": "2025-10-24T14:06:32.450355Z", "shell.execute_reply": "2025-10-24T14:06:35.345470Z"}}, "outputs": [], "execution_count": 12}, {"cell_type": "markdown", "source": "**Load-Quantize cycle**\n\nFirst, take a look at the function below. It uses the functions above to load the layers one by one and iterate over their `Linear` submodules replacing them with `QuantizedLinear`. A few things to keep in mind:\n * Note that the quantization happens on `.cuda()`, because we'll load *LLaMA* in `float16` which is not supported on `cpu`.\n * We call `torch.cuda.empty_cache()` after processing each layer because we'll have just enough *VRAM* for this to work.\n * The loaded model is placed in RAM.\n\n**Task (0.5pt):** implement RTN quantization for *LLaMA*", "metadata": {"id": "oJALPHCSi62W"}}, {"cell_type": "code", "source": "@torch.no_grad()\ndef llama_rtn(checkpoint_path: os.PathLike, model: LlamaForCausalLM, bits: int):\n    \"\"\"Loads LLaMA layers one by one and quantizes them with RTN\n    Args:\n        checkpoint_path (os.PathLike): folder containing LLaMA weights\n        model (LlamaForCausalLM): model to dispatch layers into\n        bits (int): number of bits to quantize to\n    \"\"\"\n    # Load and quantize all the layers\n    layers = model.model.layers\n    assert len(layers) == 0\n    for i in trange(32):\n        load_and_dispatch_a_layer(i, checkpoint_path, model)\n        layer = layers[i].cuda()\n\n        linear_submodules = find_layers(layer)\n        # Quantize the linear layers and replace the original ones with them\n        for name, linear in linear_submodules.items():\n            # YOUR CODE HERE>>>>>>>>>\n            q, scale, zero = measure_and_quantize(linear.weight.data, bits)\n            q_linear = QuantizedLinear(q, scale, zero, linear.bias)\n            replace_submodule(layer, name, q_linear)\n            # <<<<<<<<<<<<<<<<<<<<<<<\n\n        layers[i] = layer.cpu()\n        torch.cuda.empty_cache()\n", "metadata": {"id": "2TRwggs3Lb2F", "executionInfo": {"status": "ok", "timestamp": 1761053206554, "user_tz": -180, "elapsed": 7, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:06:35.347317Z", "iopub.execute_input": "2025-10-24T14:06:35.347610Z", "iopub.status.idle": "2025-10-24T14:06:36.239168Z", "shell.execute_reply.started": "2025-10-24T14:06:35.347586Z", "shell.execute_reply": "2025-10-24T14:06:36.238172Z"}}, "outputs": [], "execution_count": 13}, {"cell_type": "code", "source": "# Testing your code\n\nmodel = nn.ModuleDict({\"model\": nn.ModuleDict({\"layers\": nn.ModuleList([])})})\nllama_rtn(\"TEST\", model, 4)\n\nassert len(model.model.layers) == 32, \"You didn't load all the layers\"\nassert all(isinstance(layer.submodule, QuantizedLinear) for layer in model.model.layers), \"Some Linears weren't properly replaced\"\nassert torch.all(model.model.layers[0].submodule.quantized_weight == torch.arange(16).unsqueeze(0).repeat(16, 1)), \"Quantized weights are weird\"\nassert torch.all(model.model.layers[0].submodule.scale == 1), \"Quantized scales are weird\"\nprint(\"All tests passed!\")", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 66, "referenced_widgets": ["cd9e4f562a3f45268b5a01f64b032bda", "904c41fddccb41b9bb5b4e4e3a083a16", "cc8fd0474ca54e298f2b454c3c785b67", "5ae897095dbc4fa7addd4a4e11b212b2", "a2b4c1524c1f4786a0c9792fcf8f50cf", "4ab114be9d754e7abc4ee5eb42297d54", "13ce4348a1644dfa8df40469f869d7c8", "b2b291fe214144339d78fd5f48fdda48", "93d80dcbb3df4cf48743abc851b5a065", "4b9772a37d2b4d9697b19fe2d2c9621f", "e29f61d495094bf480de6d0a27d85bf7"]}, "id": "wj8FBs1ji62W", "outputId": "d066e1cb-c6c0-4d2f-9046-5fbed65a12ec", "executionInfo": {"status": "ok", "timestamp": 1761053207231, "user_tz": -180, "elapsed": 683, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:06:36.243287Z", "iopub.execute_input": "2025-10-24T14:06:36.243652Z", "iopub.status.idle": "2025-10-24T14:06:38.030023Z", "shell.execute_reply.started": "2025-10-24T14:06:36.243634Z", "shell.execute_reply": "2025-10-24T14:06:38.029015Z"}}, "outputs": [{"output_type": "display_data", "data": {"text/plain": "  0%|          | 0/32 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6b98cb80061f4479b665e24eef1a34ea"}}, "metadata": {}}, {"name": "stdout", "text": "All tests passed!\n", "output_type": "stream"}], "execution_count": 14}, {"cell_type": "markdown", "source": "### Testing the Quantized Model\n\nNow we have everything we need to quantize the _LLaMA-7B_ model to 4 bits. Let us do that.", "metadata": {"id": "E4OlcGF2aEts"}}, {"cell_type": "code", "source": "MODEL = \"./model/\"\nSEED = 0\nBITS = 4", "metadata": {"id": "luUgnOhxaFcw", "executionInfo": {"status": "ok", "timestamp": 1761053207296, "user_tz": -180, "elapsed": 55, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:06:38.030744Z", "iopub.execute_input": "2025-10-24T14:06:38.030952Z", "iopub.status.idle": "2025-10-24T14:06:38.291486Z", "shell.execute_reply.started": "2025-10-24T14:06:38.030936Z", "shell.execute_reply": "2025-10-24T14:06:38.290578Z"}}, "outputs": [], "execution_count": 15}, {"cell_type": "code", "source": "tokenizer = AutoTokenizer.from_pretrained(LLAMA_REPO, use_fast=False)\n\nmodel = initialize_layerless_llama(MODEL)\nllama_rtn(MODEL, model, BITS)\nmodel = model.cuda()", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 86, "referenced_widgets": ["a0aa9466aba9430b8d0cc060a2aa76fc", "8e50f06a16b34de4ba9c52196f1ffe31", "6673957c96e242a5ab2b5b35e5733154", "64121b042f284d51bcf2f086b0afabd1", "df4f4de663d64db5928f2e6038538add", "257a224e3c7b4b299038139163bd0469", "093c0e7ef9ff4b678bd0b9d0fd065203", "3b03de67393844b9bbc9cf00ac07ac29", "5d9b7d8f1d28498380867a700e9cdaab", "3631a06de68e4eb6998718fe6d7319d5", "1396625ce2ba405b84f72fb0f411c0be"]}, "id": "tUD4z4t7a7z8", "outputId": "5222eea1-ed93-41d2-d2b2-32b4962d9299", "executionInfo": {"status": "ok", "timestamp": 1761053297154, "user_tz": -180, "elapsed": 89856, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:06:38.292368Z", "iopub.execute_input": "2025-10-24T14:06:38.292637Z", "iopub.status.idle": "2025-10-24T14:07:47.167683Z", "shell.execute_reply.started": "2025-10-24T14:06:38.292610Z", "shell.execute_reply": "2025-10-24T14:07:47.167009Z"}}, "outputs": [{"name": "stderr", "text": "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "  0%|          | 0/32 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4fcf63785247416b919db245da89633e"}}, "metadata": {}}], "execution_count": 16}, {"cell_type": "code", "source": "questions = [\n    \"What is the capital of France?\",\n    \"Can you explain the Pythagorean theorem?\",\n    \"What is photosynthesis?\",\n    \"Give me a summary of 'Romeo and Juliet'\",\n    \"How far is the moon from the Earth?\",\n    \"What is a haiku?\",\n]\nanswers = []\n\nfor question in tqdm(questions):\n    tokenized_input = tokenizer(\n        f\"QUESTION: {question}\\n ANSWER:\",\n        return_tensors=\"pt\"\n    )\n    with torch.no_grad():\n        output = model.generate(\n            tokenized_input[\"input_ids\"].cuda(),\n            max_length=50, num_beams=3, early_stopping=True,\n        )[0]\n    answer = tokenizer.decode(output, skip_special_tokens=True)\n    answers.append(answer[:answer.find(\".\")] + \".\")\n", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 104, "referenced_widgets": ["3cfbc93fcd8e4d619d4ee867e043cce5", "9a925eb6407445cc94c23a5d06d37827", "d46c63739b824d93b4351fec7f0ba1bd", "126fae068dda4a8ab7f45c0f45421fe3", "4774275b1c2d420397f17fd0e6f3be51", "0ace064f0bfa4253bcd0d554c6ffa289", "f06c443fefc042bda5cad40110bd1590", "061939f0a8aa4c8dbb9e56b35dd4cdba", "6ad88e9ff56b4512aa6a9de276a27a76", "a26b052a11bd403b922a3b0c38fbf941", "1ef624d68f8e4000a92b1a5898a3c88c"]}, "id": "8EN9tVawbUfG", "outputId": "6503e7c6-651e-4dbf-ba7a-85dd257c3a2d", "executionInfo": {"status": "ok", "timestamp": 1761053376756, "user_tz": -180, "elapsed": 79586, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:07:47.168517Z", "iopub.execute_input": "2025-10-24T14:07:47.168734Z", "iopub.status.idle": "2025-10-24T14:09:11.510701Z", "shell.execute_reply.started": "2025-10-24T14:07:47.168717Z", "shell.execute_reply": "2025-10-24T14:09:11.509832Z"}}, "outputs": [{"output_type": "display_data", "data": {"text/plain": "  0%|          | 0/6 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "22ba166348fd4b8f92fbe9546fe587fb"}}, "metadata": {}}, {"name": "stderr", "text": "/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n", "output_type": "stream"}], "execution_count": 17}, {"cell_type": "code", "source": "print(*answers, sep=\"\\n\\n\")", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "m-QFd87GjNvd", "outputId": "36213bb6-e9b3-4ad6-e55c-8aa97905ced1", "executionInfo": {"status": "ok", "timestamp": 1761053376794, "user_tz": -180, "elapsed": 30, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:09:11.511667Z", "iopub.execute_input": "2025-10-24T14:09:11.511947Z", "iopub.status.idle": "2025-10-24T14:09:11.516799Z", "shell.execute_reply.started": "2025-10-24T14:09:11.511924Z", "shell.execute_reply": "2025-10-24T14:09:11.516040Z"}}, "outputs": [{"name": "stdout", "text": "QUESTION: What is the capital of France?\n ANSWER: Paris.\n\nQUESTION: Can you explain the Pythagorean theorem?\n ANSWER: The Pythagorean theorem states that the sum of the squares of the sides of a right triangle is equal to the square o.\n\nQUESTION: What is photosynthesis?\n ANSWER: Photosynthesis is the process by which plants use the energy of sunlight to convert carbon dioxide and water into carbohydrates an.\n\nQUESTION: Give me a summary of 'Romeo and Juliet'\n ANSWER: Romeo and Juliet is a tragedy written by William Shakespeare about two young star-crossed lovers, Rom.\n\nQUESTION: How far is the moon from the Earth?\n ANSWER: About 240,000 miles.\n\nQUESTION: What is a haiku?\n ANSWER: A haiku is a form of Japanese poetry that consists of three unrhymed lines of 5, 7, and 5 syllables.\n", "output_type": "stream"}], "execution_count": 18}, {"cell_type": "code", "source": "model = model.cpu()\ntorch.cuda.empty_cache()", "metadata": {"id": "Fn-Vko3urrM0", "executionInfo": {"status": "ok", "timestamp": 1761053381694, "user_tz": -180, "elapsed": 4895, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:09:11.517608Z", "iopub.execute_input": "2025-10-24T14:09:11.517905Z", "iopub.status.idle": "2025-10-24T14:09:18.404108Z", "shell.execute_reply.started": "2025-10-24T14:09:11.517885Z", "shell.execute_reply": "2025-10-24T14:09:18.403479Z"}}, "outputs": [], "execution_count": 19}, {"cell_type": "markdown", "source": "### Evaluating the model\n\nBefore we start quantizing the model itself, let us create a way to evaluate it's performance.\n", "metadata": {"id": "EOFpjGxX2H-J"}}, {"cell_type": "markdown", "source": "**Downloading the data**\n\nAs a metric of the models' performance, we'll use it's PPL on the [wikitext2](https://paperswithcode.com/dataset/wikitext-2) dataset. Let us download and tokenize it. We'll need two subsets of it:\n * Test set of size ... to evaluate the models.\n * A train subset of size ... that we'll use later for GPTQ.", "metadata": {"id": "_G6Muc5ZyupZ"}}, {"cell_type": "code", "source": "SEED = 0\n\ndef get_wikitext2(seed, seqlen, nsamples=128):\n    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n\n    tokenizer = AutoTokenizer.from_pretrained(LLAMA_REPO, use_fast=False)\n\n    train_input_ids = tokenizer(\"\\n\\n\".join(traindata['text']), return_tensors='pt').input_ids\n    random.seed(seed)\n    train_batch = []\n    for _ in range(nsamples):\n        i = random.randint(0, train_input_ids.shape[1] - seqlen - 1)\n        j = i + seqlen\n        inp = train_input_ids[:, i:j]\n        tar = inp.clone()\n        tar[:, :-1] = -100\n        train_batch.append(inp[0])\n\n    test_input_ids = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt').input_ids\n    test_input_ids = test_input_ids[:, :(test_input_ids.shape[1] // seqlen) *  seqlen]\n    test_input_ids = test_input_ids.reshape(test_input_ids.shape[1] // seqlen, seqlen)\n\n    return torch.stack(train_batch), test_input_ids\n\ntrain_batch, test_input_ids = get_wikitext2(SEED, 2048)", "metadata": {"id": "CUSNnguPysaD", "executionInfo": {"status": "ok", "timestamp": 1761053418519, "user_tz": -180, "elapsed": 36823, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "colab": {"base_uri": "https://localhost:8080/", "height": 305, "referenced_widgets": ["aa65d0fac4f54ec194b7f2347bba6783", "244a7cb0161f43c3a0d8d718e66e67fa", "b1297cfe4ab643718e7945d562c46b02", "d89a575f30df4c1198319ced75eae8e6", "9514abf089b542a5b4faa155b7408d19", "f7413411b7f24ffb9ee621b923aaf15f", "a3b4d68bacf64a0caec1a2264c674204", "a076cd74a86d4a4dbafc9098fc5d2014", "c5ad1b9f294741d2b37745c8934ca558", "bd80cc8b8a384ac0bf1cbeb74897c484", "e3eca9a06b7e4eeba403143f62fd6ab7", "cffa55120a7844e9a29ab6afb18827f3", "f5c4e292f5de4592a4390f6bc4f3a7c5", "29d970eb481644158291dcb84e2daae6", "72fd34d382a14d7c81f4f90732d91a53", "fa1817b098264d7695c768431e980a9c", "5ad26e9455cc4aebb4df751c9fd649c4", "e5c2d8e0050840f8871458c5deb88990", "ab7495737d394fb08e847d5dd71de8d4", "14a1c4f1691f4df999c7835704d529c5", "989fdcb965394c9abd25623468e3d8d5", "e192c90d306d427e8ebe06fd7fc4e104", "1eede1ef4e534765ac59afc154fe881b", "d242f1a5e262424cbf025103e2a44d09", "72b0acf263d04e75aa471b150dcdc99d", "26bb230c06b1458abf4f360cb6f82713", "12b9a7ff655a4d3f82e5a80502f8fc0e", "525c850a302e49b68636a596f02968c4", "d404465cf9d14a959ddc456898a38076", "94bd17456ba14058a8682dc5aa5449e0", "bb7d4c75405a427b93e25583d9611b04", "1d140508f4374d2090741fa4e14841ff", "08cde74c0da648e7a2e9bc7f08a2737e", "aeea2dd0d18a4ac29948e2b17420c4a7", "be8e76257f9040989e6202b872cb7ab9", "efa0a283d6ac4b74815b53e64b765c28", "f00abc16938743f49ec16704b7eb1de0", "e7ecd7ceae7b490f917b8fab2470e373", "0443a3e39ea144a5b5efb7464f453b92", "7b46a505b0064994ae0e7171cf198960", "c4da8ff225534af687e91656a924c644", "0e998c5a31da4ba2a675781b1d6a8a15", "d7643ce1b3dc46859089c6e2d2f42a07", "235fd569fbcb4e1387565df0deb50b01", "2766ed120115406ca5a667affc6f0644", "048f79d1200f4a1fa0fcd2881f2e6183", "1e836f1897af4d058eb396eebf1b704c", "4352ba83b03f4836a7608e0ef664f34b", "f87f92c6ceea409a8dc262c362ef9458", "7741a20a39324729aaa315e8a18096ce", "0a0a0e4706a04b21a681719d8f99724d", "b6442d2acfe64a8da621b4e7185b52b1", "d0412baad79b4b6d9927a9ca27905b99", "ceb84078f3eb4efaafae8b19163b9b23", "445f885319374cc8bea4d8fb03fa3dda", "4db62a1c416a4aa99e0bfb3ed1d1e72e", "b99769be690b4fdcb253a85fb33b5ceb", "d8a659e14c90483581e5dce1cf5d0404", "b3955b0f3a2c4098ba1b773e911d64c5", "7573c6c6562349069c52be22c298627d", "a630666abc6c4dc6ad4a55c23ad0c7a0", "6a18d6074d454da981f86b6cac359ee6", "6aaa3d62e7e94f1293a9888678d4f78d", "dbbc7f13f223420a9661677653d89c09", "7c1920e033464628b8e9d2f6b5d756a2", "421346b5a0c047c3845df119ba7e8d0c", "c9741c91c85e42c3b8973ceeb28869d1", "e6c119b3377941129c35cb2f28ec17bd", "bc3cbd95c37d483c978e4e03e6613a04", "46f837965760405ba0e4de8550e7db6f", "b7c2d5ff3a0f4f9598071a0f7e4c970d", "712d52342aa0471fb241102b0bb635e7", "5ec518ce010e49dc9cf7f8b26f0611a2", "60277ce25e174f608c270942ea3fa34c", "d2122eafd719474ebdc5b013cffa4a5c", "b46054a7f51e42ff9a2a3d61b076d9ad", "5ceb633428524b20920edf79e890096c", "4ab84f22014449eb8d69b2a0e2f5107d", "4b9b952c5866473faab3c9198b3e42a1", "d19f4d0097f74161b139ecdd0bae1138", "162b6a757ec14805a41dd027af5c139e", "790b9dd0c1e44fbba1ccfc2a87831cc3", "54780667be98486bbe1e25f60bc88233", "35218713ae364046ab6483c26177e2ca", "3e29f5f7cecb4963a4e708b6257c6bd6", "e0c7815c4348433db5a17c660f6281ac", "ac3dc101e5794a17a25fe2b81cce26bb", "48ffb9091bb84c898fee9709aa2f9178", "a4e4e2e190e34a70b15ee008ca59c6f6", "a461b8ea5396418b918660fced348307", "9263200f7ac74de184b030273920181e", "66fb889776b4494c84b83ec3079059a5", "3283492e4f4040e48105442cde02dc49", "3d9929a4c1704e13bf9553d033470c7a", "39e147f66a184d9f89156436dd408686", "246ed557bc2a42d3b4bc26d6d2b5ba39", "5a87aab610bb4760a1214920fc0000dc", "793a3a57973544b98372aff38adc9daf", "46f93692344d4c888ab776a61547c919"]}, "outputId": "d8d64332-23ba-4396-b8bb-c59fceec38a7", "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:09:18.405019Z", "iopub.execute_input": "2025-10-24T14:09:18.405273Z", "iopub.status.idle": "2025-10-24T14:09:54.652241Z", "shell.execute_reply.started": "2025-10-24T14:09:18.405248Z", "shell.execute_reply": "2025-10-24T14:09:54.651627Z"}}, "outputs": [{"output_type": "display_data", "data": {"text/plain": "Downloading readme: 0.00B [00:00, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "baf52e6b8a9841bcb9f39155c1f4cb08"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c25d7d1ed8c9485cada6936fb1fdadba"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading data:   0%|          | 0.00/733k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c25fac95711441fcb7628275b52f857d"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading data:   0%|          | 0.00/6.36M [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "137a83da36d34accaf9f0a273e7c7520"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Downloading data:   0%|          | 0.00/657k [00:00<?, ?B/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "38c9aeaa3ae643068cd38a118ba88c7c"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f7ce161d20094be081a5454b9f1083aa"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0c964b3e0ee242f08861f0d5f48ee471"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8f4deb35c503429ebc288d75e39b176b"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "019b456d53184eadbf77f0e098ebbefc"}}, "metadata": {}}], "execution_count": 20}, {"cell_type": "markdown", "source": "**Model offloading**\n\nWe want to evaluate the model's performance on a large dataset. The model barely fits on the *GPU*, and we'll have to infer in on long text sequences. We don't have enought *VRAM* to do that.\n\nInstead, we'll keep most of the model in *RAM*, only transferring the layers to *GPU* as we go through them one by one, and putting them back when we're done.\n\n**Obtaining the first layer inputs**\n\nTo start iterating over the layers, we'll first have to obtain the fist layer inputs. We use the function below to do it.", "metadata": {"id": "A77M_WBTuOLh"}}, {"cell_type": "code", "source": "!wget -nc https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/utils.py --no-check-certificate", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "QbxZfwj8ipPF", "outputId": "fb2d11f5-be7f-4639-b072-dd6a8f038fa7", "executionInfo": {"status": "ok", "timestamp": 1761053418917, "user_tz": -180, "elapsed": 417, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:09:54.653050Z", "iopub.execute_input": "2025-10-24T14:09:54.653658Z", "iopub.status.idle": "2025-10-24T14:09:55.237033Z", "shell.execute_reply.started": "2025-10-24T14:09:54.653635Z", "shell.execute_reply": "2025-10-24T14:09:55.236034Z"}}, "outputs": [{"name": "stdout", "text": "--2025-10-24 14:09:54--  https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/utils.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1334 (1.3K) [text/plain]\nSaving to: \u2018utils.py\u2019\n\nutils.py            100%[===================>]   1.30K  --.-KB/s    in 0s      \n\n2025-10-24 14:09:55 (55.3 MB/s) - \u2018utils.py\u2019 saved [1334/1334]\n\n", "output_type": "stream"}], "execution_count": 21}, {"cell_type": "code", "source": "from utils import get_first_layer_inputs", "metadata": {"id": "7W50rdH0i62Y", "executionInfo": {"status": "ok", "timestamp": 1761053418918, "user_tz": -180, "elapsed": 4, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:09:55.238417Z", "iopub.execute_input": "2025-10-24T14:09:55.238820Z", "iopub.status.idle": "2025-10-24T14:09:55.244730Z", "shell.execute_reply.started": "2025-10-24T14:09:55.238796Z", "shell.execute_reply": "2025-10-24T14:09:55.243827Z"}}, "outputs": [], "execution_count": 22}, {"cell_type": "code", "source": "class TestModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.ModuleDict({\"layers\": nn.ModuleList([])})\n\n    def forward(self, inp):\n        self.model.layers[0](2 * inp, attention_mask=\"Some Mask\", position_ids=\"Some Ids\")\n\n\ntest_model = TestModule()\nllama_rtn(\"TEST\", test_model, 4)\ntest_inputs = torch.arange(16 * 32).reshape(16, 32).float()\nhidden_states, attention_mask, position_ids = get_first_layer_inputs(test_model, test_inputs)\nassert torch.all(2 * test_inputs == hidden_states)\nassert attention_mask == \"Some Mask\"\nassert position_ids == \"Some Ids\"\nassert len(test_model.model.layers) == 32, \"The function doesn't put back the original layers\"\nprint(\"All tests passed!\")", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 66, "referenced_widgets": ["639678e0aa504b8099b901207fc51abb", "4b9bf563b5a44b238d9c9ed115232af2", "3d9642d6755f4a178c00dfba21fba14b", "9c641a4827d34274a966d2bd9f32c0db", "171417d743914e88823a8adb097f5f5d", "506a4affa57e41a582f32cf542aafc12", "404199ba93634a73b198ff84519cd75e", "4636a7ff23ea4f2cb398b4395de5807f", "61e1690993884d06b1e73dfc576fb314", "7b31c8981fe348e4a87524bb54516340", "47a46bedcbbb43c6b2646de3ed0227f9"]}, "id": "hN2TrGEBi62Y", "outputId": "b4d62095-4924-4ae1-e3cd-c6fac4c919aa", "executionInfo": {"status": "ok", "timestamp": 1761053418923, "user_tz": -180, "elapsed": 7, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:09:55.245767Z", "iopub.execute_input": "2025-10-24T14:09:55.246076Z", "iopub.status.idle": "2025-10-24T14:09:55.303032Z", "shell.execute_reply.started": "2025-10-24T14:09:55.246049Z", "shell.execute_reply": "2025-10-24T14:09:55.302486Z"}}, "outputs": [{"output_type": "display_data", "data": {"text/plain": "  0%|          | 0/32 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "002a5c6302dc4171b74ad76ee9f3d6e2"}}, "metadata": {}}, {"name": "stdout", "text": "All tests passed!\n", "output_type": "stream"}], "execution_count": 23}, {"cell_type": "markdown", "source": "**Forward passing layer-by-layer**", "metadata": {"id": "HFI_nTCXi62Y"}}, {"cell_type": "code", "source": "def forward_pass_layer(layer: nn.Module, inps: torch.Tensor, outs: torch.Tensor, attention_mask: Tensor, position_ids: Tensor):\n    \"\"\"Forward pass inps through the layer ONE INP AT A TIME saving the outputs into the corresponding elements of outs\"\"\"\n    for j in range(inps.shape[0]):\n        outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n\n\ndef get_batch_nll(model: nn.Module, batch: Tensor):\n    # Collect the first layer inputs, put them on .cuda()\n    inps, attention_mask, position_ids = get_first_layer_inputs(model, batch)\n    inps = inps.cuda()\n    attention_mask = attention_mask.cuda()\n    position_ids = position_ids.cuda()\n\n    # Create a buffer for layer outputs\n    outs = torch.zeros_like(inps)\n\n    # Forward pass through the layers\n    layers = model.model.layers\n    assert len(layers) == 32\n    for i in trange(32, leave=False):\n        layer = layers[i].cuda() # Take a layer and put in on .cuda()\n\n        forward_pass_layer(layer, inps, outs, attention_mask, position_ids) # Forward pass a layer\n        inps, outs = outs, inps # Prepare the inputs and the output buffer for the next layer. Reuse the existing buffers\n\n        layers[i] = layer.cpu() # Put layer back on .cpu()\n        del layer\n        torch.cuda.empty_cache()\n\n\n    # Calculate NLL\n    nll = 0\n    model.model.norm = model.model.norm.cuda()\n    model.lm_head = model.lm_head.cuda()\n    for i in range(inps.shape[0]):\n        lm_logits = model.lm_head(model.model.norm(inps[i].unsqueeze(0)))\n        labels = batch[i]\n        # Calculate the language modeling Negative Log Likelyhood\n        shift_logits = lm_logits[:, :-1, :]\n        shift_labels = labels[1:]\n        loss_fct = nn.CrossEntropyLoss()\n        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).cuda())\n        nll += float(loss) * model.seqlen\n    model.model.norm = model.model.norm.cpu()\n    model.lm_head = model.lm_head.cpu()\n    return nll\n\n\n@torch.no_grad()\ndef llama_eval(model, test_input_ids):\n    print('Evaluating ...')\n\n    total_nll = 0\n    for batch in tqdm(torch.tensor_split(test_input_ids, 4)):\n        total_nll += get_batch_nll(model, batch)\n\n    # Calculate PPL\n    ppl = math.exp(total_nll / test_input_ids.numel())\n    print(f\"PPL: {ppl}\")\n    return ppl\n", "metadata": {"id": "zzEPMSsEi62Y", "executionInfo": {"status": "ok", "timestamp": 1761053418939, "user_tz": -180, "elapsed": 14, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:09:55.303692Z", "iopub.execute_input": "2025-10-24T14:09:55.303914Z", "iopub.status.idle": "2025-10-24T14:09:55.312913Z", "shell.execute_reply.started": "2025-10-24T14:09:55.303892Z", "shell.execute_reply": "2025-10-24T14:09:55.312132Z"}}, "outputs": [], "execution_count": 24}, {"cell_type": "markdown", "source": "**Calculating PPL**\n\nWe've already loaded and quantized the model. All that's left is to evaluate it.", "metadata": {"id": "pHq3Z8RlLpuc"}}, {"cell_type": "code", "source": "rtn_ppl = llama_eval(model, test_input_ids)", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 84, "referenced_widgets": ["bd93d973f7294a8784e96d15ab2da38a", "4c9b7da6fdec48938c22ca6bbd86d1aa", "70b0428b455c4c82b8f3527676744e99", "90d261fc86bb44e49c0e3d743d831c9f", "ee62d19ba2c24c12bf4484b1faf5f990", "c842e53af6a342d7873e80a04b05a749", "196e2572a1dd494382fb5de666764680", "de69f8114d904d1fb651614886df4c56", "62956662b32c4e34a9e793ff7454a5e5", "673106c8fccb4c33a132a9f13c4b7aaf", "a96509bdcaa64a91be5c062f26b538cb", "417044778a9e4714b1a4295835fef2bb", "abf6dd237ea64445803b29adbd0c7e82", "4e5090d959f24dad897820d18cd35786", "bd6da6adcaec4c809799df914c03bc1f", "90bb9b3e13854dbf8d76893c68747fef", "b057f584594142d29f77402c628d486f", "90a5597b545143d7b2a7ccb72447d79c", "21f953debe614357b5fe342cd5ea6cec", "e442d51b40bc4811b4749022ff730323", "bc1e281a639e4d12b21e828146482d12", "e024495ea0294923af1c0602aaf9590a", "878093d3f5e6494c91f4a08198ec7820", "0213decb198d4c72a52f4a85b591fb5e", "2d8ea2bba4c94a04a1a836580a360826", "3f1a5fcd06bd4f4296f65da0a46a87c8", "694c0c20d259464c98939a91d16aa264", "0c0d85d9f2a143938e2c23936bb0c40b", "e6f1630882fa4a8eab392d25abd912fa", "45358bd286284fde816b8c69f7e58018", "7d413237160145f2a188eff43f7b541d", "65da09a43611460ea96155845395a7f0", "755bfbdfc5944dfa87fb2e3bcb60cb86", "9b269172c14e475293db9b4c68cb42af", "330a039c402844d3844212ed94110ff7", "81f00f9b00f34199a15f3179d9bfe0d9", "f77b53487dad4d94af8f3b5efb82dedf", "4413bc2889804c909e8499ce4366d675", "2d2bb258a7044f5993a42153ccacbb31", "9e466982e8044fd083021420bf4271d4", "e43e3ec2a51b4280ac21a67fef71b71b", "a60ccf654d0747f9911132cbf6f73b87", "6125c12cbeea4011a5d075658109758c", "12b2a8366cd6428ba7c3b069170e3499", "2245098f8a3a4f4fb64ea6d5068a7e07", "58a85bdc1bf6457090f588161bbf54c6", "fa18fad03b9b461d85c7a871ab4c6c9e", "6f119e2e40b2454f9b7f5b8b96835d88", "f406daa2acbd4876a862c274784d7cd1", "dd83378c552c40f2a4a86fde90293cc7", "e214cc15de5f42848670f40e0dbbac28", "4b20cf990894426ca3e9fcb51339193f", "70abb7a40a6c42d2bdd227b4d43f1072", "009fe53deb204885a186395e211ea3c5", "1882a6e580f84b30833daea45ae89a04"]}, "id": "ybS4iAjsq67Z", "outputId": "9f72ba2a-dffe-48a3-8dd9-bf455ea7ede3", "executionInfo": {"status": "ok", "timestamp": 1761053845636, "user_tz": -180, "elapsed": 426694, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:09:55.313813Z", "iopub.execute_input": "2025-10-24T14:09:55.314159Z", "iopub.status.idle": "2025-10-24T14:18:02.575368Z", "shell.execute_reply.started": "2025-10-24T14:09:55.314115Z", "shell.execute_reply": "2025-10-24T14:18:02.574358Z"}}, "outputs": [{"name": "stdout", "text": "Evaluating ...\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5ac7ca6a0dd84248a3276cd3fce594e3"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "  0%|          | 0/32 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "  0%|          | 0/32 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "  0%|          | 0/32 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "  0%|          | 0/32 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"name": "stdout", "text": "PPL: 6.429625677428965\n", "output_type": "stream"}], "execution_count": 25}, {"cell_type": "code", "source": "# Testing your code\n\nassert rtn_ppl > 6.3 and rtn_ppl < 6.6\nprint(\"All tests passed!\")", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "722oBDnwq_1x", "outputId": "d292ef43-518e-4229-d7d7-d6a864ba1015", "executionInfo": {"status": "ok", "timestamp": 1761053845651, "user_tz": -180, "elapsed": 12, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:18:02.576531Z", "iopub.execute_input": "2025-10-24T14:18:02.576840Z", "iopub.status.idle": "2025-10-24T14:18:02.581654Z", "shell.execute_reply.started": "2025-10-24T14:18:02.576814Z", "shell.execute_reply": "2025-10-24T14:18:02.580971Z"}}, "outputs": [{"name": "stdout", "text": "All tests passed!\n", "output_type": "stream"}], "execution_count": 26}, {"cell_type": "code", "source": "del model\ntorch.cuda.empty_cache()", "metadata": {"id": "urb_7OhSq_zA", "executionInfo": {"status": "ok", "timestamp": 1761053845837, "user_tz": -180, "elapsed": 184, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:18:02.582319Z", "iopub.execute_input": "2025-10-24T14:18:02.582508Z", "iopub.status.idle": "2025-10-24T14:18:02.944102Z", "shell.execute_reply.started": "2025-10-24T14:18:02.582493Z", "shell.execute_reply": "2025-10-24T14:18:02.943413Z"}}, "outputs": [], "execution_count": 27}, {"cell_type": "markdown", "source": "### GPTQ", "metadata": {"id": "-CQIk33arN6A"}}, {"cell_type": "markdown", "source": "GPTQ is the State Of The Art quantization algorithm for post-training DL model quantization. It works by sequentially quantizing the model's linear layer weights.\n\nAlthough in outputs results similar to what one would get with Round To Nearest quantization, it makes a key observations to boost it's end performance:\n * It is layer input aware (also referred to as \"1-Shot\" method), meaning int optimizes the quantized matrix to show best performance on inputs typically encountered in that layer.\nMore formally, the problem can be formulated as:\n$$\nW_q = argmin_{\\widehat{W}}\\|XW^T - X\\widehat{W}^T\\|_2^2\n$$\n, where\n * $X$ is the input matrix of shape `(..., IN)`.\n * $XW^T$ is the unquantized output of shape `(..., OUT)`. We think of the norm above as taking a sum over those (...) dimensions.\n * $W$ is the unquantized weight of shape `(OUT, IN)`.\n * $\\widehat{W}$ is the quantized weight taken from some quantization grid.\n\nOne can notice that the expression above is independent with regard to the rows of $W$ and $\\widehat{W}$, meaning we can solve it for each row in parallel. This is the reason why we're working with row-wise quantization in the first place. Notice that the quantization grid only depends on min/max values withing the row and not the quantization process, so we can think of it as fixed.\n\nand the dimension of the optimization problem is `IN`, which is too much to solve exactly. The algorithm proposes to solve it iteratively.\n\nLess us consider a vector of full precision weights $F$ and corresponding sent of inputs $X_F$. The corresponding objective is quadratic with Hessian\n$$\nH_F = 2X_F^TX_F^.\n$$\nThe algorithm can be described like this:\n * Do the following steps until $F$ is fully quantized:\n    1. Given the next index to quantize $i$, and corresponding unquantized element $F_i$.\n    2. Quantize the coordinate by prjecting in onto the quantization grid $Q_i = quant(F_i)$.\n    3. Update all of the remaining weights $F_: = F_: - \\frac{F_i - quant(F_i)}{\\left[H_F^{-1}\\right]_{ii}}\\cdot\\left[H_F^{-1}\\right]_{i,:}$.\n    4. Exclude $i$ from $F$.\n\nIt uses the inverse Hessian to slightly tune the remaining unquantized weights to mitigate the quantization error.\n\nAs for how $i$ is chosen, an observation was made that iterating over indices in order of **decreasing diagonal Hessian elements** provides the best performance.\n\nThere are a few more ideas that make this algorithm much faster:\n 1. We can represent the order of quantization (selection of $i$) by permuting the row in advance, and then iterating over the row element in order.\n $$\n   F_{i:} = F_{i:} - \\frac{F_{i} - quant(F_{i})}{\\left[H_F^{-1}\\right]_{ii}}\\cdot\\left[H_F^{-1}\\right]_{i,i:}\n $$\n 2. The problem is row-wise independent, meaning that we use the same permutation each row and perform those operations in a vector fashion for all the rows at the same time.\n $$\n   F_{:,i:} = F_{:,i:} - \\frac{F_{:,i} - quant(F_{:,i})}{\\left[H_F^{-1}\\right]_{ii}}\\odot\\left[H_F^{-1}\\right]_{i,i:}\\leftarrow\\text{\\textbf{ you'll have to code this}}\n $$\n 3. We don't actually need to recompute the inverse Hessian. At $i$-th step we only need its $t$-th row, and we can use fancy math to precompute the matrix containing all of those rows in advance.\n $$\n  H^{-1} = Cholesky(H^{-1})^T    \n $$\n\n 4. We don't need to tune all the remaining unquantized values right away. We can only apply the updates for the closest elements right away and accumulate all the other updates to apply them only once in a while.\n\n    We'll do this in block of fixed size, applying the updates inside of those blocks and updating the weights outside only when we're done with the block. To accumulate those updates, we'll collect the scaled quantization error\n    $$\n      Err_{:,i} =\\frac{F_{:,i} - quant(F_{:,i})}{\\left[H_F^{-1}\\right]_{ii}}\\text{ for all }i\\text{ in block}.\n    $$", "metadata": {"id": "t16SaWf3rQt7"}}, {"cell_type": "markdown", "source": "**GPTQ within blocks**\n\nImplement GPTQ within the block. Iterate over the columns in ordered vector fashion, quantizing them one by one and updating all the remaining colums within the block.\n\nReturn the quantized weight as well as the matrix of quantization errors that we'll need to tune the unquantized weights outside of the block.\n\n**Task (2pt):** Implement GPTQ block:", "metadata": {"id": "ogTj9Op5raaD"}}, {"cell_type": "code", "source": "@torch.no_grad()\ndef gptq_block(block_weight: Tensor, block_hessian_inverse: Tensor, scale: Tensor, zero: Tensor, bits: int) -> tuple[Tensor, Tensor]:\n    \"\"\"Perform GPTQ within block\n    Args:\n        block_weight (Tensor): weight to quantize of shape (OUT, BLOCK_SIZE)\n        block_hessian_inverse (Tensor): Cholesky inverse Hessian. Upper triangular of shape (BLOCK_SIZE, BLOCK_SIZE)\n        scale (Tensor): row-wise quantization constats of shape (OUT, 1)\n        zero (Tensor): row-wise quantization constats of shape (OUT, 1)\n        bits (int): number of bits to quantize() to\n\n    Returns:\n        tuple[Tensor, Tensor]: quantized weight and scaled quantization error\n    \"\"\"\n    block_weight = block_weight.clone()\n    quantized_block_weight = torch.zeros(block_weight.shape, dtype=torch.uint8, device=block_weight.device)\n    scaled_block_error = torch.zeros_like(block_weight)\n\n    # Interate over the block's columns\n    for i in range(block_weight.shape[1]):\n        # Get the column and the corresponding inverse Hessian\n        # YOUR CODE HERE>>>>>>>>>\n        w = block_weight[:, i]\n        h = block_hessian_inverse[i, i]\n        q = quantize(w, scale[:, 0], zero[:, 0], bits).flatten()                                          # quantize column\n        quantized_block_weight[:, i] = q\n        err1 = (w - dequantize(q, scale[:, 0], zero[:, 0])) / h                                           # quantization error\n        block_weight[:, i:] -= err1.unsqueeze(1).matmul(block_hessian_inverse[i, i:].unsqueeze(0))        # update weights\n        scaled_block_error[:, i] = err1\n        # <<<<<<<<<<<<<<<<<<<<<<<\n\n    return quantized_block_weight, scaled_block_error\n", "metadata": {"id": "t8GwQU5CrNv_", "executionInfo": {"status": "ok", "timestamp": 1761055621940, "user_tz": -180, "elapsed": 3, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:18:02.944863Z", "iopub.execute_input": "2025-10-24T14:18:02.945123Z", "iopub.status.idle": "2025-10-24T14:18:02.951752Z", "shell.execute_reply.started": "2025-10-24T14:18:02.945096Z", "shell.execute_reply": "2025-10-24T14:18:02.951072Z"}}, "outputs": [], "execution_count": 28}, {"cell_type": "code", "source": "# Testing your code\n!wget -O gptq_block_weight_reference.pt https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_block_weight_reference.pt --no-check-certificate\n!wget -O gptq_block_error_reference.pt https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_block_error_reference.pt --no-check-certificate\n\ngenerator = torch.Generator()\ngenerator.manual_seed(0)\n\nweight = torch.rand((128, 128), generator=generator).cuda()\nscale, zero = get_scale_and_zero(weight, 15)\n\nblock_weight = weight[:,:16]\n\nblock_hessian_inverse = (torch.triu(torch.rand((16, 16), generator=generator), diagonal=1) + torch.diag(torch.rand(16, generator=generator) + 1)).cuda()\nquantized_block_weight, scaled_block_error = gptq_block(block_weight, block_hessian_inverse, scale, zero, 4)\n\nassert torch.all(quantized_block_weight == torch.load(\"gptq_block_weight_reference.pt\"))\nassert torch.allclose(scaled_block_error, torch.load(\"gptq_block_error_reference.pt\"), rtol=1e-5, atol=1e-06)\n\nprint(\"All tests passed!\")", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "LgRy6B_didyY", "outputId": "17ac2812-6886-453e-c8c5-613c35eb8d63", "executionInfo": {"status": "ok", "timestamp": 1761055622758, "user_tz": -180, "elapsed": 310, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:18:02.952536Z", "iopub.execute_input": "2025-10-24T14:18:02.952787Z", "iopub.status.idle": "2025-10-24T14:18:04.079247Z", "shell.execute_reply.started": "2025-10-24T14:18:02.952767Z", "shell.execute_reply": "2025-10-24T14:18:04.078314Z"}}, "outputs": [{"name": "stdout", "text": "--2025-10-24 14:18:03--  https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_block_weight_reference.pt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3392 (3.3K) [application/octet-stream]\nSaving to: \u2018gptq_block_weight_reference.pt\u2019\n\ngptq_block_weight_r 100%[===================>]   3.31K  --.-KB/s    in 0s      \n\n2025-10-24 14:18:03 (36.6 MB/s) - \u2018gptq_block_weight_reference.pt\u2019 saved [3392/3392]\n\n--2025-10-24 14:18:03--  https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_block_error_reference.pt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 9531 (9.3K) [application/octet-stream]\nSaving to: \u2018gptq_block_error_reference.pt\u2019\n\ngptq_block_error_re 100%[===================>]   9.31K  --.-KB/s    in 0.001s  \n\n2025-10-24 14:18:03 (14.3 MB/s) - \u2018gptq_block_error_reference.pt\u2019 saved [9531/9531]\n\nAll tests passed!\n", "output_type": "stream"}], "execution_count": 29}, {"cell_type": "markdown", "source": "**Now we can implement the full algorithm:**\n * Get row-wise quantization constants.\n * Sort the columns by decreasing Hessian diagonal values. Think about how you'd have to permute the Hessian as well.\n * Process the Hessian to obtain the precomputed inverse Hessian.\n * Iterate over the columns in blocks:\n    * Get the next block and quantize it.\n    * Tune all the following blocks to mitigate the quantization error.\n      $$\n         F_{:,block\\_end:} = F_{:,block\\_end:} - Err_{:,block\\_start:block\\_end}\\odot\\left[H_F^{-1}\\right]_{block\\_start:block\\_end,block\\_end:}\n      $$\n * Restore the original order for quantized columns.\n\n**Task (2pt):** implement the full algorithms:", "metadata": {"id": "RpXmzbg8y18R"}}, {"cell_type": "code", "source": "def prepare_inverse_hessian(hessian: Tensor, percdamp: float) -> Tensor:\n    \"\"\"Precomputes inverse Hessian\n    Args:\n        hessian (Tensor): problem hessian\n        percdamp (float): diagonal damping constant for numerical stability\n    Returns:\n        Tensor: precomputed inverse Hessian\n    \"\"\"\n    damp = percdamp * torch.mean(torch.diag(hessian))\n    diag = torch.arange(hessian.shape[0], device=weight.device)\n    hessian[diag, diag] += damp\n    hessian = torch.linalg.cholesky(hessian)\n    hessian = torch.cholesky_inverse(hessian)\n    hessian = torch.linalg.cholesky(hessian, upper=True)\n    return hessian\n\n\n@torch.no_grad()\ndef gptq(weight: torch.Tensor, bits: int, hessian: torch.Tensor, blocksize:int=128, percdamp:float=.01) -> tuple[Tensor, Tensor, Tensor]:\n    \"\"\"Quantizes weight with GPTQ\n    Args:\n        weight (torch.Tensor): weight to quantize\n        bits (int): number of bits to quantize to\n        hessian (torch.Tensor): problem Hessian\n        blocksize (int, optional): Defaults to 128.\n        percdamp (float, optional): Hessian damping constant for numerical stability. Defaults to .01.\n\n    Returns:\n        tuple[Tensor, Tensor, Tensor]: quantized_weight, row-wise quantization scales, row-wise quantization zeroes\n    \"\"\"\n    dtype = weight.dtype\n    weight = weight.clone().detach()\n    weight = weight.float()\n    num_columns = weight.shape[1]\n    hessian = hessian.float()\n\n    # Identify and patch always-zero input coordinates\n    dead = torch.diag(hessian) == 0\n    hessian[dead, dead] = 1\n    weight[:, dead] = 0\n\n    # Get row-wise quantization constants\n    scale, zero = get_scale_and_zero(weight, 2 ** bits - 1)\n\n    # Sort the columns by decreasing Hessian diagonal values.\n    # Transform the hessian accordingly.\n    # YOUR CODE HERE>>>>>>>>>\n    perm = torch.argsort(torch.diag(hessian), descending=True)\n    weight = weight[:, perm]\n    hessian = hessian[perm][:, perm]\n    invperm = torch.argsort(perm)\n    # <<<<<<<<<<<<<<<<<<<<<<<\n\n    # Process the Hessian to obtain the precomputed inverse Hessian\n    hessian_inverse = prepare_inverse_hessian(hessian, percdamp)\n\n    # Iterate over the columns in blocks\n    quantized_weight = torch.zeros(weight.shape, dtype=torch.uint8, device=weight.device)\n    for block_start in range(0, num_columns, blocksize):\n\n        block_end = min(block_start + blocksize, num_columns)\n\n        # Get the next block and quantize it\n        # YOUR CODE HERE>>>>>>>>>\n        next_block = weight[:, block_start:block_end]\n        next_hessian_inv = hessian_inverse[block_start:block_end, block_start:block_end]\n        quantized_weight[:, block_start:block_end], error = gptq_block(next_block, next_hessian_inv, scale, zero, bits)\n        weight[:, block_end:] -= error.matmul(hessian_inverse[block_start:block_end, block_end:])\n\n        # Tune all the following blocks to mitigate the quantization error\n\n        # <<<<<<<<<<<<<<<<<<<<<<<\n\n    # Reverse the permutation of the quantized weight\n    quantized_weight = quantized_weight[:, invperm]\n\n    return quantized_weight, scale.to(dtype), zero.to(dtype)\n", "metadata": {"id": "iqI4FamprFPE", "executionInfo": {"status": "ok", "timestamp": 1761057295204, "user_tz": -180, "elapsed": 5, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:18:04.080458Z", "iopub.execute_input": "2025-10-24T14:18:04.080736Z", "iopub.status.idle": "2025-10-24T14:18:04.091482Z", "shell.execute_reply.started": "2025-10-24T14:18:04.080712Z", "shell.execute_reply": "2025-10-24T14:18:04.090836Z"}}, "outputs": [], "execution_count": 30}, {"cell_type": "code", "source": "# Testing your code\n!wget -O gptq_weight_reference.pt https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_weight_reference.pt --no-check-certificate\n\ngenerator = torch.Generator()\ngenerator.manual_seed(0)\n\nhessian = torch.triu(torch.rand((128, 128), generator=generator) + 4 * torch.eye(128)).cuda().half()\nhessian += hessian.clone().T\nquantized_weight, _, _ = gptq(weight, 4, hessian, 32)\n\nassert torch.all(quantized_weight == torch.load(\"gptq_weight_reference.pt\"))\n\nprint(\"All tests passed!\")", "metadata": {"id": "KI1R_stdrFMk", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1761057295925, "user_tz": -180, "elapsed": 310, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "outputId": "18858efe-f79f-4ae0-bb0b-d849095ff701", "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:18:04.092268Z", "iopub.execute_input": "2025-10-24T14:18:04.092525Z", "iopub.status.idle": "2025-10-24T14:18:04.877911Z", "shell.execute_reply.started": "2025-10-24T14:18:04.092501Z", "shell.execute_reply": "2025-10-24T14:18:04.877101Z"}}, "outputs": [{"name": "stdout", "text": "--2025-10-24 14:18:04--  https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/gptq_weight_reference.pt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 17634 (17K) [application/octet-stream]\nSaving to: \u2018gptq_weight_reference.pt\u2019\n\ngptq_weight_referen 100%[===================>]  17.22K  --.-KB/s    in 0.001s  \n\n2025-10-24 14:18:04 (11.7 MB/s) - \u2018gptq_weight_reference.pt\u2019 saved [17634/17634]\n\nAll tests passed!\n", "output_type": "stream"}], "execution_count": 31}, {"cell_type": "markdown", "source": "**Sequential Model Quantization**\n\nThe GPT quantization approach implemented here requires an ordered approach due to its input-dependent nature. For each `Linear` submodule within the GPT model, we need to ensure that the input data is representative of the actual operating conditions post-quantization. This involves propagating a batch of input samples through the model sequentially, with each layer's input being the output of the preceding **quantized** layers.\n\nThe quantization process must follow a strict sequence both across and within layers. Within each layer, there is a predetermined order in which the submodules must be quantized, which is dictated by the dependencies between them. This order is defined by the \"sequential groups\".\n\nThe steps of the algorithm are as follows:\n- Retrieve and prepare inputs for the first layer.\n- Iterate through each layer in the model:\n  - Load the current layer for processing.\n  - Within each layer, process the sequential groups of submodules:\n    - Attach forward hooks to collect input data to each submodule.\n    - Execute a forward pass through the layer to accumulate the necessary input data for quantization.\n    - Remove the hooks after data collection.\n    - Apply GPTQ to quantize the submodule weights using the accumulated input data.\n  - Perform another forward pass through the quantized layer to generate the inputs for the next layer.", "metadata": {"id": "cHAnRizBy8HC"}}, {"cell_type": "code", "source": "def get_accumulate_input_fn(name: str, hessians: Mapping[str, Tensor], num_samples: Mapping[str, int]):\n    \"\"\"Generate a callback that updates the corresponding hessians and counts when given input\n    Args:\n        name (str): module name\n        hessians (Mapping[str, Tensor]): a dict of modules' hessians, accessible by module name\n        num_samples (Mapping[str, int]): a dict of callback call counters\n    \"\"\"\n    def tmp(_, inp, out):\n        inp = inp[0].data # ... x hidden_size\n        inp = inp.reshape((-1, inp.shape[-1])) # inputs x hidden_size\n        inp = inp.t().float() # hidden_size x inputs\n        num_samples[name] += 1\n        if hessians[name] is None:\n            hessians[name] = inp.matmul(inp.t())\n        else:\n            hessians[name] += inp.matmul(inp.t())\n    return tmp\n\n\n@torch.no_grad()\ndef llama_gptq(checkpoint_path: os.PathLike, model: LlamaForCausalLM, batch: Tensor, bits: int):\n    \"\"\"Loads LLaMA layers one by one and quantizes them with GPTQ\n    Args:\n        checkpoint_path (os.PathLike): folder containing LLaMA weights\n        model (LlamaForCausalLM): model to dispatch layers into\n        batch (Tensor): sample model inputs\n        bits (int): number of bits to quantize to\n    \"\"\"\n    # Collect the first layer inputs, put them on .cuda() (the same as in get_batch_nll)\n    inps, attention_mask, position_ids = get_first_layer_inputs(model, batch)\n    inps = inps.cuda()\n    attention_mask = attention_mask.cuda()\n    position_ids = position_ids.cuda()\n\n    # Create a buffer for layer outputs\n    outs = torch.zeros_like(inps)\n\n    # Forward pass through the layers\n    layers = model.model.layers\n    assert len(layers) == 0\n    for i in trange(32):\n        # Load and dispatch the next layer\n        load_and_dispatch_a_layer(i, checkpoint_path, model)\n        layer = layers[i].cuda()\n        linear_layers = find_layers(layer)\n\n        hessians = {name: None for name in linear_layers}\n        num_samples = {name: 0 for name in linear_layers}\n        handles = [\n            linear_layers[name].register_forward_hook(\n                get_accumulate_input_fn(name, hessians, num_samples)\n            ) for name in linear_layers\n        ]\n        forward_pass_layer(layer, inps, outs, attention_mask, position_ids)\n        for h in handles:\n            h.remove()\n\n        for name, linear in linear_layers.items():\n            q, scale, zero = gptq(linear.weight.data, bits, 2 * hessians[name] / num_samples[name])\n            quantized_linear = QuantizedLinear(q, scale, zero, linear.bias)\n            replace_submodule(layer, name, quantized_linear)\n\n        forward_pass_layer(layer, inps, outs, attention_mask, position_ids)\n        inps, outs = outs, inps\n        layers[i] = layer.cpu()\n        del layer\n        torch.cuda.empty_cache()\n", "metadata": {"id": "fMyrGkHqy-xg", "executionInfo": {"status": "ok", "timestamp": 1761057518729, "user_tz": -180, "elapsed": 2, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:18:04.879073Z", "iopub.execute_input": "2025-10-24T14:18:04.879414Z", "iopub.status.idle": "2025-10-24T14:18:04.889616Z", "shell.execute_reply.started": "2025-10-24T14:18:04.879383Z", "shell.execute_reply": "2025-10-24T14:18:04.888969Z"}}, "outputs": [], "execution_count": 32}, {"cell_type": "markdown", "source": "**Evaluating the model with GPTQ**", "metadata": {"id": "p6lN_q-0zFej"}}, {"cell_type": "code", "source": "model = initialize_layerless_llama(MODEL)\nllama_gptq(MODEL, model, train_batch, BITS)", "metadata": {"id": "GKRuiX7Vza96", "colab": {"base_uri": "https://localhost:8080/", "height": 49, "referenced_widgets": ["10dd9a8e07e94928a756bbeda5172ace", "f003cc2303f34331bf3c00dd28be04ec", "e764017ca6fc46eb929bb01f0aa78639", "92c426e589e54be6a588b974b07f95e0", "e99a2563d64f41acac75f6c1dee0496e", "20a0941efcf2478099dee63f318fd1cb", "f29aac6d8057474da63b59c145e1b9ce", "0f841993abab4b06b8bb847d38941960", "bdf4593c8a0f47df9684250129de5ea1", "4974e36cc8884cab8ff7c4f6991ac82b", "c3fdbf91decc4f58a8070eabcbd0bff5"]}, "executionInfo": {"status": "ok", "timestamp": 1761059344962, "user_tz": -180, "elapsed": 1816311, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "outputId": "78724846-2902-4309-f442-d4be0d65370a", "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:18:04.890308Z", "iopub.execute_input": "2025-10-24T14:18:04.890500Z", "iopub.status.idle": "2025-10-24T14:49:21.698499Z", "shell.execute_reply.started": "2025-10-24T14:18:04.890485Z", "shell.execute_reply": "2025-10-24T14:49:21.697703Z"}}, "outputs": [{"output_type": "display_data", "data": {"text/plain": "  0%|          | 0/32 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f98040950f1849a9a2ec199bb09cf1a1"}}, "metadata": {}}], "execution_count": 33}, {"cell_type": "code", "source": "gptq_ppl = llama_eval(model, test_input_ids)", "metadata": {"id": "53ARPLErza7N", "colab": {"base_uri": "https://localhost:8080/", "height": 84, "referenced_widgets": ["efe88af433ca48c883ee525ab81c67cc", "414ba0ebb50e47ffbe747727b4d40afe", "a9b6d5e8a4b742a7a0228e78155201f6", "df66bb402305483ea080900f80c24ef7", "c989cb7fe5f04b0da1db36c00e895db8", "a21775dd62b24457a6e14ee6d3e69caa", "eb6827e1b65f4bca93889f2093deaf0f", "26729b849f49499d90a5622d24ed10bc", "19b9a1f840744f97a079cbaebe5b878c", "8a39f860a8f14ae29c29260183c5747d", "9c002a3bdd634b5387f0fe15cbd33507", "f4ef8a3e23d24a949c1c4daa34acdf34", "ee0c5b9930ad41c4a48d669078a75b35", "101251ebb0cd4be3ba577c7c14d3b33f", "4715bb3684c64e3783c8cb79905183f4", "596fefc2a43b490da8ce5fc56f97a7ef", "3641b905636b40c983b00789ceea10b6", "36cc510fc7024fcfb1d7eab74d51ab48", "15750efa4ba74ac2a5f1021024fe7046", "429dd4d147024b569d50a7208299f16a", "d630318d771241f2a227cbd0be5b81dc", "e7f0f14dd9ec40ab97905fec342ab936", "f70069cbef3c48deac7d5443e3c51104", "c889809deaa14a6a852a1ec7d0226ab4", "dc56790ebf144298bf8b450eaa147a3f", "5f8d271540de4c40a8e239e7ef7fb1e2", "dfb7027b66a44c63bd92ff533ab7f000", "8a6c9d16bedd4b4499d61d7130395dbf", "f8fe09479c8f4619898337326683dfa2", "44ee502dedb24b7691dc1560b927fd38", "6f79331e12444d739625aa4c0d8d8cb1", "413bb949958d4b66980348ce3c58b150", "b8309e7bf2b441559f723ed39ac309ca", "feca66a8466740f080121e6cfee25602", "46766ba0ac02474f8467bf392ddfe92a", "bcfc0402952c44bfaf346cbae6e5d831", "ded89965295c4bd296d2bb5396b7d388", "b009f00da2364218a978d37a23567160", "9b244a5d52a34ec18d030a1282043e6d", "7d9fff6c98b84455b840b1b6ec963bec", "4a29aafb23534998b70fc434b13cfdb4", "230be2db5755412d8a690e97c23c1a39", "08c6ed58a4b84808a72791fc50693f07", "1e705fb0343e4527a62fcbfb9f44e112", "b5b3c950b2d54e63ac9197821fffc8bf", "9a55593fa45841f185b2905d4964a688", "60ccd9d5d7754af8ace4cc5b12c62d5e", "74ab282443bc4eb895dd8b4b97abf082", "184b4d673a584a258713e948214d2e42", "ed2abd8ed9f64df7bcaeb9abcd0eb676", "6a7d3bc07a044615a70587ff2998c657", "9c3a2eca599a4aac90ea98e153c0669b", "547c114ddb5749438ee5928ecee1bd88", "84345e6a235840219ec449cb27252654", "0c7111eb7f9c4da491a8b1f39f4bcd3b"]}, "executionInfo": {"status": "ok", "timestamp": 1761059757742, "user_tz": -180, "elapsed": 412776, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "outputId": "ca7cd5ed-e937-42b4-82da-c7a8df88cd21", "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:49:21.699541Z", "iopub.execute_input": "2025-10-24T14:49:21.699877Z", "iopub.status.idle": "2025-10-24T14:57:22.800912Z", "shell.execute_reply.started": "2025-10-24T14:49:21.699848Z", "shell.execute_reply": "2025-10-24T14:57:22.799986Z"}}, "outputs": [{"name": "stdout", "text": "Evaluating ...\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "cf64649f2a5545dfa7b40d2c7d8a1ea1"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "  0%|          | 0/32 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "  0%|          | 0/32 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "  0%|          | 0/32 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "  0%|          | 0/32 [00:00<?, ?it/s]", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"name": "stdout", "text": "PPL: 5.958793260910872\n", "output_type": "stream"}], "execution_count": 34}, {"cell_type": "code", "source": "# Testing your code\n\nassert gptq_ppl < 6.2\nprint(\"All tests passed!\")", "metadata": {"id": "9QwMLEqUza43", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1761059757755, "user_tz": -180, "elapsed": 10, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "outputId": "1d47d823-a91b-4feb-fa0b-9375a02894c5", "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:57:22.801778Z", "iopub.execute_input": "2025-10-24T14:57:22.802036Z", "iopub.status.idle": "2025-10-24T14:57:22.806575Z", "shell.execute_reply.started": "2025-10-24T14:57:22.802013Z", "shell.execute_reply": "2025-10-24T14:57:22.805773Z"}}, "outputs": [{"name": "stdout", "text": "All tests passed!\n", "output_type": "stream"}], "execution_count": 35}, {"cell_type": "code", "source": "del model\ntorch.cuda.empty_cache()", "metadata": {"id": "18BHd4Nnza2T", "executionInfo": {"status": "ok", "timestamp": 1761059758041, "user_tz": -180, "elapsed": 285, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-24T14:57:22.807295Z", "iopub.execute_input": "2025-10-24T14:57:22.807525Z", "iopub.status.idle": "2025-10-24T14:57:23.234677Z", "shell.execute_reply.started": "2025-10-24T14:57:22.807502Z", "shell.execute_reply": "2025-10-24T14:57:23.233800Z"}}, "outputs": [], "execution_count": 36}, {"cell_type": "markdown", "source": "# Dense Integer Packing\n\nFor the most part, we used `torch.uint8` to represent tensors quantized to *4 bits*. We encoded each *4-bit* value with an *8-bit* value, using twice as much memory as we really needed. We did this because `torch` lacks support for 4-bit tensors. However, we never performed any native operations in *8 bits*; we always converted it to `torch.float16` first. That means we can design a more efficient way to encode *4-bit* tensors and convert to and from it.\n\nYour task is to implement these conversions, converting *8-bit* tensors containing *4-bit* values (the way we used to store quantized weights) to and from smaller *8-bit* tensor, utilizing the whole range of values. Moreover, you'll have to do it in a way such that memory representation of contiguous arrays wouldn't change. That is, adjacent columns get squashed together.\n\n**Task (0.5pt):** Implement dense *int4*:", "metadata": {"id": "KggEz3g37B1g"}}, {"cell_type": "code", "source": "def dense_pack_4_8(x: Tensor) -> Tensor:\n    \"\"\"Constructs a densely packed int tensor\n    Args:\n        x (Tensor): uint8 with uint4 values range\n\n    Returns:\n        Tensor: twice as small uint8 tensor with uint8 values range\n    \"\"\"\n    x = x.clone().detach()\n    # YOUR CODE HERE>>>>>>>>>\n    r = x[:, ::2] * 16 + x[:, 1::2]\n    return r\n    # <<<<<<<<<<<<<<<<<<<<<<<\n\ndef dense_unpack_4_8(x: Tensor) -> Tensor:\n    \"\"\"Deconstructs a densely packed int tensor\n    Args:\n        x (Tensor): uint8 tensor with uint8 values range\n\n    Returns:\n        Tensor: twice as large int8 tensor with uint4 values range\n    \"\"\"\n    x = x.clone().detach()\n    # YOUR CODE HERE>>>>>>>>>\n    r = torch.zeros((x.shape[0], x.shape[1] * 2), dtype=torch.uint8)\n    r[:, ::2] = x // 16\n    r[:, 1::2] = x % 16\n    return r\n    # <<<<<<<<<<<<<<<<<<<<<<<\n", "metadata": {"id": "p-jaEhv57MC9", "executionInfo": {"status": "ok", "timestamp": 1761059900201, "user_tz": -180, "elapsed": 29, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T08:15:32.935604Z", "iopub.execute_input": "2025-10-23T08:15:32.935912Z", "iopub.status.idle": "2025-10-23T08:15:32.941272Z", "shell.execute_reply.started": "2025-10-23T08:15:32.935892Z", "shell.execute_reply": "2025-10-23T08:15:32.940453Z"}}, "outputs": [], "execution_count": 38}, {"cell_type": "code", "source": "# Testing your code\n\nx = torch.arange(512 * 1024).reshape(512, 1024).float()\nquantized_x, scale, zero = measure_and_quantize(x, 4)\ndense_quantized_x = dense_pack_4_8(quantized_x)\nundense_quantized_x = dense_unpack_4_8(dense_quantized_x)\n\nassert dense_quantized_x.shape == (512, 512)\nassert torch.all(undense_quantized_x == quantized_x)\nassert (dense_quantized_x // 16 == dense_quantized_x % 16).float().mean() > 0.95, \"close values are supposed to be packed together\"\nprint(\"All tests passed!\")", "metadata": {"id": "4sM4tAOU7Oez", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1761059900685, "user_tz": -180, "elapsed": 101, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "outputId": "e8f94fba-94d3-4f4d-f796-f488c8f43ad2", "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T08:15:34.368550Z", "iopub.execute_input": "2025-10-23T08:15:34.369337Z", "iopub.status.idle": "2025-10-23T08:15:34.396394Z", "shell.execute_reply.started": "2025-10-23T08:15:34.369310Z", "shell.execute_reply": "2025-10-23T08:15:34.395768Z"}}, "outputs": [{"name": "stdout", "text": "All tests passed!\n", "output_type": "stream"}], "execution_count": 39}, {"cell_type": "markdown", "source": "# Bonus: QUIK\n\n[QUIK](https://arxiv.org/abs/2310.09259) is an extension of GPTQ. It's main feature is that, in addition to model quantization, it also quantizes activations. That way, multiplication can be performed in `int`, leading to 2x-3x improvement in inference speed over GPTQ, which dequantizes the weights and performs multiplication in `float`.", "metadata": {"id": "E14awThL68JM"}}, {"cell_type": "markdown", "source": "### Different Range, Different Scales and Zeros\n\nAt basic quantization level, QUIK already has a number of differences compared to GPTQ:\n 1. QUIK quantizes the weights to **signed** integer values for better stability of matrix multiplication.\n 2. QUIK enforces `zero` to be integer, to be able to perform `int` multiplication with it as well.\n 3. QUIK changes the sign of `zero` (compare `quik_dequantize` with `dequantize`) for simplicity.", "metadata": {"id": "JipC2pi-idyg"}}, {"cell_type": "code", "source": "def quik_get_scale_and_zero(x: Tensor, max_abs: float) -> tuple[Tensor, Tensor]:\n    \"\"\" Given a tensor x of shape (m, k) and max_abs > 0 produce tensors scale and zero of shape (m, 1)\n        such that 0 < x / scale + zero < max_abs\"\"\"\n    if x.shape[-1] == 0:\n        return torch.ones(x.shape[:-1], dtype=x.dtype, device=x.device), torch.zeros(x.shape[:-1], dtype=torch.int, device=x.device)\n    xmin = x.min(-1)[0]\n    xmax = x.max(-1)[0]\n\n    scale = (xmax - xmin) / max_abs / 2\n    scale[scale == 0] = 1\n    zero = torch.round((xmax + xmin) / 2 / scale) # zero is int, since we want to use it in int operations\n\n    return scale.unsqueeze(-1), zero.unsqueeze(-1).to(torch.int)\n\n\ndef quik_quantize(x: Tensor, scale: Tensor, zero: Tensor, bits: int) -> Tensor:\n    \"\"\"Given a tensor x quantize it, producing tensors quantized_x of torch.int8 dtype\n    Args:\n        x (Tensor): tensor to quantize\n        scale (Tensor): values interval mapping scale\n        zero (Tensor): values interval mapping zero\n        bits (int): number of bits to quantize to\n\n    Returns:\n        Tensor: quantized tensor in int8\n    \"\"\"\n    if x.shape[-1] == 0:\n        return torch.zeros(x.shape, dtype=torch.int8, device=x.device)\n    max_abs = 2 ** (bits - 1) - 1\n    quantized_x = torch.round(x / scale) - zero\n    quantized_x = torch.clamp(quantized_x, -max_abs, max_abs) # what are the allowed values for int8?\n    return quantized_x.to(torch.int8)\n\n\ndef quik_dequantize(x: Tensor, scale: Tensor, zero: Tensor) -> Tensor:\n    \"\"\"Dequantize a tensor\n    Args:\n        quantized_x (Tensor): quantized tensor in int8\n        scale (Tensor): values interval mapping scale\n        zero (Tensor): values interval mapping zero\n\n    Returns:\n        Tensor: dequantized tensor\n    \"\"\"\n    return scale * x + scale * zero\n\n\ndef quik_measure_and_quantize(x: Tensor, bits: float) -> tuple[Tensor, Tensor, Tensor]:\n    \"\"\"Determine the values interval mapping parameters and quantize a tensor\n    Args:\n        x (Tensor): tensor to quantize\n        bits (float): number of bits to quantize to\n\n    Returns:\n        tuple[Tensor, Tensor, Tensor]: quantized tensor, scale, zero\n    \"\"\"\n    max_abs = 2 ** (bits - 1) - 1\n    scale, zero = quik_get_scale_and_zero(x, max_abs)\n    x_quantized = quik_quantize(x, scale, zero, bits)\n    return x_quantized, scale, zero\n", "metadata": {"id": "XEtnoIS5R8Z8", "executionInfo": {"status": "ok", "timestamp": 1761059932843, "user_tz": -180, "elapsed": 101, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T08:15:39.651817Z", "iopub.execute_input": "2025-10-23T08:15:39.652124Z", "iopub.status.idle": "2025-10-23T08:15:39.660457Z", "shell.execute_reply.started": "2025-10-23T08:15:39.652104Z", "shell.execute_reply": "2025-10-23T08:15:39.659713Z"}}, "outputs": [], "execution_count": 40}, {"cell_type": "code", "source": "# Testing my code\n\nx = torch.arange(512 * 1024).reshape(512, 1024).float()\nscale, zero = quik_get_scale_and_zero(x, 15)\nquantized_x, scale, zero = quik_measure_and_quantize(x, 4)\n\nassert scale.shape == (512, 1), \"Shape of scale is incorrect\"\nassert zero.shape == (512, 1), \"Shape of zero is incorrect\"\nassert torch.allclose(x, quik_dequantize(quantized_x, scale, zero), atol=50), \"Dequantized values are too far from the original values\"\nprint(\"All tests passed!\")", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "-4zWRo8tidyh", "outputId": "17275407-cc47-4c22-b624-9e0e854c0214", "executionInfo": {"status": "ok", "timestamp": 1761059933284, "user_tz": -180, "elapsed": 35, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T08:15:40.117843Z", "iopub.execute_input": "2025-10-23T08:15:40.118447Z", "iopub.status.idle": "2025-10-23T08:15:40.135853Z", "shell.execute_reply.started": "2025-10-23T08:15:40.118423Z", "shell.execute_reply": "2025-10-23T08:15:40.135174Z"}}, "outputs": [{"name": "stdout", "text": "All tests passed!\n", "output_type": "stream"}], "execution_count": 41}, {"cell_type": "markdown", "source": "### Weight Quantization", "metadata": {"id": "nSqZunEhR-_Y"}}, {"cell_type": "markdown", "source": "**Block quantization**\n\nOn block level, QUIK is identical to GPTQ. Paste your `gptq_block` solution here and simply replace `quantize` with `quik_quantize` and `dequantize` with `quik_dequantize`.\n\n**Task (1pt):** Paste GPTQ code into QUIK:", "metadata": {"id": "xa5TnPD1idyi"}}, {"cell_type": "code", "source": "@torch.no_grad()\ndef quik_block(block_weight: Tensor, block_hessian_inverse: Tensor, scale: Tensor, zero: Tensor, bits: int) -> tuple[Tensor, Tensor]:\n    \"\"\"NOTE: This function is allowed to alter the block_weight as we won't need those weights anymore\n\n    Args:\n        block_weight (Tensor): weight to quantize of shape (OUT, BLOCK_SIZE)\n        block_hessian_inverse (Tensor): Cholesky inverse Hessian. Upper triangular of shape (BLOCK_SIZE, BLOCK_SIZE)\n        scale (Tensor): row-wise quantization constants of shape (OUT, 1)\n        zero (Tensor): row-wise quantization constants of shape (OUT, 1)\n        bits (int): number of bits to quantize() to\n\n    Returns:\n        tuple[Tensor, Tensor]: quantized weight and scaled quantization error\n    \"\"\"\n    block_weight = block_weight.clone()\n    quantized_block_weight = torch.zeros(block_weight.shape, dtype=torch.int8, device=block_weight.device)\n    scaled_block_error = torch.zeros_like(block_weight)\n\n    # Iterate over the block's columns\n    for i in range(block_weight.shape[1]):\n        # Get the column and the corresponding inverse Hessian\n        column_weight = block_weight[:, [i]]\n        # YOUR CODE HERE>>>>>>>>>\n        w = block_weight[:, i]\n        h = block_hessian_inverse[i, i]\n        q = quik_quantize(w, scale[:, 0], zero[:, 0], bits).flatten()                                          # quantize column\n        quantized_block_weight[:, i] = q\n        err1 = (w - quik_dequantize(q, scale[:, 0], zero[:, 0])) / h                                           # quantization error\n        block_weight[:, i:] -= err1.unsqueeze(1).matmul(block_hessian_inverse[i, i:].unsqueeze(0))        # update weights\n        scaled_block_error[:, i] = err1\n        # <<<<<<<<<<<<<<<<<<<<<<<\n\n    return quantized_block_weight, scaled_block_error\n", "metadata": {"id": "VaVrG9jDSFZj", "executionInfo": {"status": "ok", "timestamp": 1761060042451, "user_tz": -180, "elapsed": 8, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T08:15:43.857699Z", "iopub.execute_input": "2025-10-23T08:15:43.858008Z", "iopub.status.idle": "2025-10-23T08:15:43.864707Z", "shell.execute_reply.started": "2025-10-23T08:15:43.857957Z", "shell.execute_reply": "2025-10-23T08:15:43.863879Z"}}, "outputs": [], "execution_count": 42}, {"cell_type": "markdown", "source": "**Full quantization**\n\nTo determine the outliers, QUIK needs additional information about layer inputs. Namely, `l_inf_norms` - max module values for each input coordinate in a minibatch. We can see how it's used to extract outliers.\n\nThe rest of the function, again, is copy-paste from `gptq(...)`. Feel free to reuse your code, but don't forget to replace `gptq_block` with `quik_block`.\n\n**Task (0pt):** Paste GPTQ code into QUIK:", "metadata": {"id": "A-Vrjo7qidyj"}}, {"cell_type": "code", "source": "def quik(weight: torch.Tensor, bits: int, hessian: torch.Tensor, l_inf_norms: torch.Tensor, blocksize:int=128, percdamp:float=.01, n_outliers=128):\n    dtype = weight.dtype\n    weight = weight.clone().detach()\n    weight = weight.float()\n\n    # Identify and patch always-zero input coordinates\n    dead = torch.diag(hessian) == 0\n    hessian[dead, dead] = 1\n    weight[:, dead] = 0\n\n    # Identify outliers by decreasing l_inf_norms. Sort the remained by decrasing hessian values\n    perm = torch.argsort(l_inf_norms, descending=True)\n    perm[n_outliers:] = perm[n_outliers:][torch.argsort(torch.diag(hessian)[perm][n_outliers:], descending=True)]\n    weight = weight[:, perm]\n    hessian = hessian[perm, :][:, perm]\n\n    # Process outliers\n    outlier_weight = weight[:,:n_outliers]\n    weight = weight[:,n_outliers:]\n    num_columns = weight.shape[1]\n    hessian = hessian[n_outliers:,:][:,n_outliers:]\n\n    max_abs = 2 ** (bits - 1) - 1\n    scale, zero = quik_get_scale_and_zero(weight, max_abs)\n\n    # Process the Hessian to obtain the precomputed inverse Hessian\n    damp = percdamp * torch.mean(torch.diag(hessian))\n    diag = torch.arange(num_columns, device=weight.device)\n    hessian[diag, diag] += damp\n    hessian = torch.linalg.cholesky(hessian)\n    hessian = torch.cholesky_inverse(hessian)\n    hessian = torch.linalg.cholesky(hessian, upper=True)\n    hessian_inverse = hessian\n\n    # Iterate over the columns in blocks\n    quantized_weight = torch.zeros(weight.shape, dtype=torch.int8, device=weight.device)\n    for block_start  in range(0, num_columns, blocksize):\n        block_end = min(block_start + blocksize, num_columns)\n\n        # YOUR CODE HERE>>>>>>>>>\n        # Get the next block and quantize it\n        next_block = weight[:, block_start:block_end]\n        next_hessian_inv = hessian_inverse[block_start:block_end, block_start:block_end]\n        quantized_weight[:, block_start:block_end], error = quik_block(next_block, next_hessian_inv, scale, zero, bits)\n        weight[:, block_end:] -= error.matmul(hessian_inverse[block_start:block_end, block_end:])\n        # Tune all the following blocks to mitigate the quantization error\n\n        # <<<<<<<<<<<<<<<<<<<<<<<\n\n    return quantized_weight, scale.to(dtype), zero, outlier_weight.to(dtype), perm", "metadata": {"id": "e7TGlfLgidyj", "executionInfo": {"status": "ok", "timestamp": 1761060173408, "user_tz": -180, "elapsed": 31, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T08:15:48.192374Z", "iopub.execute_input": "2025-10-23T08:15:48.192680Z", "iopub.status.idle": "2025-10-23T08:15:48.201718Z", "shell.execute_reply.started": "2025-10-23T08:15:48.192659Z", "shell.execute_reply": "2025-10-23T08:15:48.200933Z"}}, "outputs": [], "execution_count": 43}, {"cell_type": "markdown", "source": "### QUIK Linear Layer", "metadata": {"id": "1GAoLdmOSF6-"}}, {"cell_type": "markdown", "source": "**Install CUDA 12.3 and cutlass**", "metadata": {"id": "JbaTV0rpidyk"}}, {"cell_type": "code", "source": "%%capture\n!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n!sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n!wget https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda-repo-ubuntu2204-12-3-local_12.3.0-545.23.06-1_amd64.deb\n!sudo dpkg -i cuda-repo-ubuntu2204-12-3-local_12.3.0-545.23.06-1_amd64.deb\n!sudo cp /var/cuda-repo-ubuntu2204-12-3-local/cuda-*-keyring.gpg /usr/share/keyrings/\n!sudo apt-get update\n!sudo apt-get -y install cuda-toolkit-12-3\n!pip install --upgrade pip\n\n!git clone https://github.com/NVIDIA/cutlass.git\n!export CUDACXX=/usr/local/cuda/bin/nvcc\n!cd cutlass && mkdir build && cd build && cmake .. -DCUTLASS_NVCC_ARCHS=75", "metadata": {"id": "LceebQnjidyk", "executionInfo": {"status": "ok", "timestamp": 1761060819852, "user_tz": -180, "elapsed": 628458, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T08:15:52.902474Z", "iopub.execute_input": "2025-10-23T08:15:52.902772Z", "iopub.status.idle": "2025-10-23T08:32:55.264046Z", "shell.execute_reply.started": "2025-10-23T08:15:52.902749Z", "shell.execute_reply": "2025-10-23T08:32:55.262882Z"}}, "outputs": [], "execution_count": 44}, {"cell_type": "markdown", "source": "**Loading custom CUDA kernels**\n\nAlong with this jupyter notebook, two `CUDA` files are provided, implementing efficient matrix multiplication for `int4` and `int8`.", "metadata": {"id": "B3l9KCjuidyl"}}, {"cell_type": "code", "source": "!wget -nc https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/kernel.cpp --no-check-certificate\n!wget -nc https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/kernel.cu --no-check-certificate", "metadata": {"id": "qF1ZAk6Didyl", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1761060820508, "user_tz": -180, "elapsed": 653, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "outputId": "3730c4d8-3e0b-41f7-9291-4fe59c4ee32d", "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T08:32:55.266141Z", "iopub.execute_input": "2025-10-23T08:32:55.266453Z", "iopub.status.idle": "2025-10-23T08:32:56.032645Z", "shell.execute_reply.started": "2025-10-23T08:32:55.266430Z", "shell.execute_reply": "2025-10-23T08:32:56.031866Z"}}, "outputs": [{"name": "stdout", "text": "--2025-10-23 08:32:55--  https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/kernel.cpp\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 851 [text/plain]\nSaving to: \u2018kernel.cpp\u2019\n\nkernel.cpp          100%[===================>]     851  --.-KB/s    in 0s      \n\n2025-10-23 08:32:55 (47.8 MB/s) - \u2018kernel.cpp\u2019 saved [851/851]\n\n--2025-10-23 08:32:55--  https://raw.githubusercontent.com/yandexdataschool/nlp_course/2023/week10_efficiency/kernel.cu\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3003 (2.9K) [text/plain]\nSaving to: \u2018kernel.cu\u2019\n\nkernel.cu           100%[===================>]   2.93K  --.-KB/s    in 0s      \n\n2025-10-23 08:32:55 (37.5 MB/s) - \u2018kernel.cu\u2019 saved [3003/3003]\n\n", "output_type": "stream"}], "execution_count": 45}, {"cell_type": "code", "source": "from torch.utils.cpp_extension import load\n\ncustom_kernel = load(name='custom_kernel', sources=['kernel.cpp', 'kernel.cu'], extra_include_paths=[r\"cutlass/include\"])", "metadata": {"id": "YXqhFBw1idym", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1761060923997, "user_tz": -180, "elapsed": 103487, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "outputId": "e0c3296a-9e8a-42bb-b1dc-f2b38afab810", "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T08:32:56.033621Z", "iopub.execute_input": "2025-10-23T08:32:56.033889Z", "iopub.status.idle": "2025-10-23T08:34:04.150782Z", "shell.execute_reply.started": "2025-10-23T08:32:56.033866Z", "shell.execute_reply": "2025-10-23T08:34:04.150128Z"}}, "outputs": [{"name": "stderr", "text": "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\n", "output_type": "stream"}], "execution_count": 46}, {"cell_type": "markdown", "source": "Usage: `output = custom_kernel.int8_matmul(X, Y)` and `output = custom_kernel.int4_matmul(X, Y)` compute $XY^T$ (same as `nn.functional.linear`).\n\nNotice that `int8_matmul` takes the normal `torch.int8` tensors, but int4_matmul expects `int4` values densely packed into `torch.uint8` tensors. This is exactly what we wrote **Dense Integer Packing** for. They have a <font color='red'>severe limitation</font>: the dimension of multiplication must be divisible by 16.\n\nNow you have to implement the quantized forward pass:\n\n $$\n \\begin{align}\n    XW^T &= (Q_x \\cdot scale_x + zero_x \\cdot scale_x)(Q_w \\cdot scale_w + zero_w \\cdot scale_w)^T =\\\\\n    &= (Q_x  + zero_x)(Q_w + zero_w)^T \\cdot (scale_x \\odot scale_w^T) =\\\\\n    &= (Q_xQ_w^T + Q_x zero_w^T + zero_x Q_w^T + zero_x zero_w^T) \\cdot (scale_x \\odot scale_w^T)\n \\end{align}\n $$\n\nbecause of the kernel limitations mentioned above, only the largest integer multiplication ($Q_xQ_w^T$) can be done in `int`. Perform the other ones in `float`.\n\n**Task (3pt)**: implement QUIK forward pass and measure it's performance:", "metadata": {"id": "6RIy26cHidym"}}, {"cell_type": "code", "source": "class QuikLinear(nn.Module):\n    def __init__(self, quantized_weight, weight_scale, weight_zero, outlier_weight, bias, bits: int, perm):\n        super().__init__()\n        self.bits = bits\n        self.perm = perm\n        self.n_outliers = outlier_weight.shape[1]\n\n        self.quantized_weight = nn.Parameter(quantized_weight, requires_grad=False)\n        self.weight_scale = nn.Parameter(weight_scale, requires_grad=False)\n        self.weight_zero = nn.Parameter(weight_zero, requires_grad=False)\n\n        self.outlier_weight = nn.Parameter(outlier_weight, requires_grad=False)\n        self.weights_reduced = self.quantized_weight.to(torch.int32).sum(dim=1).float()\n        print(self.weights_reduced.shape)\n\n        if bias is not None:\n            self.bias = nn.Parameter(bias.data.clone().detach())\n        else:\n            self.bias = None\n\n    def forward(self, input):\n        out_size, in_size = self.quantized_weight.shape\n        input = input[...,self.perm]\n        input_quantized, input_scale, input_zero = quik_measure_and_quantize(input[...,self.n_outliers:], self.bits)\n\n        outliers_result = F.linear(input[...,:self.n_outliers], self.outlier_weight, self.bias).float()\n        if input_quantized.shape[-1] == 0:\n            return outliers_result\n\n        # Convert necessary components to float\n        input_reduced = input_quantized.to(torch.int32).sum(dim=-1).float()\n        input_zero = input_zero.float()\n        weight_zero = self.weight_zero.data.float()\n\n        # Fully int operations\n        # YOUR CODE HERE>>>>>>>>>\n        x = input_quantized.detach().clone()\n        w = self.quantized_weight.detach().clone()\n        scale = input_scale.float() @ self.weight_scale.T.float()\n        quantized_result = custom_kernel.int8_matmul(x, w) * scale\n        # print(quantized_result.shape)\n        quantized_result = quantized_result.float()\n        # <<<<<<<<<<<<<<<<<<<<<<<\n\n        # I wish those were int, but float operations\n        # YOUR CODE HERE>>>>>>>>>\n        # x = quantized_result * (input_scale.float() @ self.weight_scale.T.float())\n        # weights = input_scale * input_quantized.float() + input_scale * input_zero\n        # half_range = (weights.max(dim=1)[0] - weights.min(dim=1)[0]) / 2\n        # weights = weights.cpu()\n        # del weights\n        # shift = input_zero + half_range * input_scale\n        # shift *= self.weights_reduced\n        # quantized_result += shift\n        # shift = shift.cpu()\n        \n        quantized_result += (input_scale * input_quantized.float()) @ (self.weight_scale * weight_zero)\n        quantized_result += (input_scale * input_zero.repeat(1, self.quantized_weight.shape[1])) @ (self.weight_scale * self.quantized_weight.float()).T\n        quantized_result += (input_scale * input_zero) @ (self.weight_scale * weight_zero).T\n        # print(quantized_result.shape)\n        # quantized_result += input_zero @ self.weights_reduced.unsqueeze(0)\n        # # print(quantized_result.shape)\n        # quantized_result += input_zero @ weight_zero.T\n        # print(quantized_result.shape, input_scale.shape, self.weight_scale.shape)\n        # quantized_result = quantized_result * (input_scale.float() @ self.weight_scale.T.float())\n        # print(quantized_result.shape)\n        # <<<<<<<<<<<<<<<<<<<<<<<\n\n        # print(outliers_result.shape)\n        results = outliers_result + quantized_result\n        # results[:, self.n_outliers:] = quantized_result\n        # results = (input_quantized * input_scale + input_zero * input_scale) @ (self.quantized_weight * self.weight_scale + self.weight_zero * self.weight_scale).T.float()\n        # results = (input_quantized + input_zero) @ (self.quantized_weight + self.weight_zero.float()).T.float() * (input_scale @ self.weight_scale.T)\n        # results += outliers_result\n        results = results.to(torch.float16)\n        \n        quantized_result = quantized_result.cpu()\n        outliers_result = outliers_result.cpu()\n        del outliers_result\n        del quantized_result\n        results = results.cpu()\n        torch.cuda.empty_cache()\n        return results", "metadata": {"id": "WG8Y2aP4wDMK", "executionInfo": {"status": "ok", "timestamp": 1761063922593, "user_tz": -180, "elapsed": 5, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T11:08:54.087529Z", "iopub.execute_input": "2025-10-23T11:08:54.088160Z", "iopub.status.idle": "2025-10-23T11:08:54.098684Z", "shell.execute_reply.started": "2025-10-23T11:08:54.088136Z", "shell.execute_reply": "2025-10-23T11:08:54.098208Z"}}, "outputs": [], "execution_count": 206}, {"cell_type": "markdown", "source": "### Loading the First LLaMA Layer Attention Q Projection", "metadata": {"id": "6JIYoW6jJxuo"}}, {"cell_type": "code", "source": "model = initialize_layerless_llama(MODEL)\ninps, _, _ = get_first_layer_inputs(model, train_batch)\ninps = inps.cuda()\nmodel.model.layers = nn.ModuleList()\nload_and_dispatch_a_layer(0, \"./model\", model)\nlayer = model.model.layers[0].self_attn.q_proj.cuda()\ndel model\ntorch.cuda.empty_cache()", "metadata": {"id": "_CU8tqdd0z0h", "executionInfo": {"status": "ok", "timestamp": 1761062113725, "user_tz": -180, "elapsed": 13771, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T08:34:04.163565Z", "iopub.execute_input": "2025-10-23T08:34:04.164129Z", "iopub.status.idle": "2025-10-23T08:34:14.439923Z", "shell.execute_reply.started": "2025-10-23T08:34:04.164106Z", "shell.execute_reply": "2025-10-23T08:34:14.439320Z"}}, "outputs": [], "execution_count": 48}, {"cell_type": "markdown", "source": "### Gathering the Layer Inputs", "metadata": {"id": "SrJinm85Jxup"}}, {"cell_type": "code", "source": "class AccumulatedInput:\n    hessians = None\n    num_samples = 0\n    actnorms = None\n\n@torch.no_grad()\ndef accumulate_layer_input(_, inp, out):\n    inp = inp.reshape((-1, inp.shape[-1])) # inputs x hidden_size\n    inp = inp.t().float() # hidden_size x inputs\n    AccumulatedInput.num_samples += 1\n    if AccumulatedInput.hessians is None:\n        AccumulatedInput.hessians = inp.matmul(inp.t())\n        AccumulatedInput.actnorms = inp.abs().amax(dim=1)\n    else:\n        AccumulatedInput.hessians += inp.matmul(inp.t())\n        AccumulatedInput.actnorms = torch.maximum(AccumulatedInput.actnorms, inp.abs().amax(dim=1))\n\n\nfor inp in inps:\n    accumulate_layer_input(None, inp, None)", "metadata": {"id": "7hsx8WPe1Kek", "executionInfo": {"status": "ok", "timestamp": 1761062130880, "user_tz": -180, "elapsed": 199, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T08:34:14.440596Z", "iopub.execute_input": "2025-10-23T08:34:14.440769Z", "iopub.status.idle": "2025-10-23T08:34:14.522768Z", "shell.execute_reply.started": "2025-10-23T08:34:14.440755Z", "shell.execute_reply": "2025-10-23T08:34:14.522021Z"}}, "outputs": [], "execution_count": 49}, {"cell_type": "code", "source": "OUT = layer.weight.shape[0]", "metadata": {"id": "frvUz5b51Of3", "executionInfo": {"status": "ok", "timestamp": 1761062131633, "user_tz": -180, "elapsed": 30, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T08:34:14.523599Z", "iopub.execute_input": "2025-10-23T08:34:14.523803Z", "iopub.status.idle": "2025-10-23T08:34:14.527568Z", "shell.execute_reply.started": "2025-10-23T08:34:14.523787Z", "shell.execute_reply": "2025-10-23T08:34:14.526753Z"}}, "outputs": [], "execution_count": 50}, {"cell_type": "code", "source": "with torch.no_grad():\n    reference = torch.stack(tuple(layer(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-10-23T08:34:14.528420Z", "iopub.execute_input": "2025-10-23T08:34:14.528665Z", "iopub.status.idle": "2025-10-23T08:34:14.835571Z", "shell.execute_reply.started": "2025-10-23T08:34:14.528649Z", "shell.execute_reply": "2025-10-23T08:34:14.834778Z"}}, "outputs": [], "execution_count": 51}, {"cell_type": "markdown", "source": "### Benchmarking MSE", "metadata": {"id": "Pa_7ovEnJxup"}}, {"cell_type": "code", "source": "with torch.no_grad():\n    quantized_weight, scale, zero = measure_and_quantize(layer.weight.data, 8)\n    shit = QuantizedLinear(quantized_weight, scale, zero, layer.bias)\n\n    result = torch.stack(tuple(shit(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)\n\n    rtn_mse = float(torch.pow(result - reference, 2).mean() ** (1/2))\n    print(\"MSE:\", rtn_mse)\n\nassert rtn_mse < 8e-5 and rtn_mse > 7e-5", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "WI6ShN5gZby9", "outputId": "37a29a6f-1555-4bf8-c911-43c92407ad5b", "executionInfo": {"status": "ok", "timestamp": 1761062140110, "user_tz": -180, "elapsed": 620, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T08:34:14.836383Z", "iopub.execute_input": "2025-10-23T08:34:14.836591Z", "iopub.status.idle": "2025-10-23T08:34:17.327999Z", "shell.execute_reply.started": "2025-10-23T08:34:14.836565Z", "shell.execute_reply": "2025-10-23T08:34:17.327285Z"}}, "outputs": [{"name": "stdout", "text": "MSE: 7.630504842381924e-05\n", "output_type": "stream"}], "execution_count": 52}, {"cell_type": "code", "source": "with torch.no_grad():\n    quantized_weight, scale, zero = gptq(layer.weight.data, 8, 2 * AccumulatedInput.hessians / AccumulatedInput.num_samples)\n    shit = QuantizedLinear(quantized_weight, scale, zero, layer.bias)\n\n    result = torch.stack(tuple(shit(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)\n\n    gptq_mse = float(torch.pow(result - reference, 2).mean() ** (1/2))\n    print(\"MSE:\", gptq_mse)\n\nassert gptq_mse < 3e-5 and gptq_mse > 1.5e-5", "metadata": {"id": "AifKxdLHZhii", "colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"status": "ok", "timestamp": 1761062153625, "user_tz": -180, "elapsed": 2263, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "outputId": "ab293fdc-6844-46c8-bf05-aa655fca0d5b", "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T08:34:17.329674Z", "iopub.execute_input": "2025-10-23T08:34:17.329887Z", "iopub.status.idle": "2025-10-23T08:34:19.020130Z", "shell.execute_reply.started": "2025-10-23T08:34:17.329870Z", "shell.execute_reply": "2025-10-23T08:34:19.019450Z"}}, "outputs": [{"name": "stdout", "text": "MSE: 1.8218925106339157e-05\n", "output_type": "stream"}], "execution_count": 53}, {"cell_type": "code", "source": "torch.cuda.empty_cache()", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-10-22T14:11:19.070702Z", "iopub.execute_input": "2025-10-22T14:11:19.071427Z", "iopub.status.idle": "2025-10-22T14:11:19.079026Z", "shell.execute_reply.started": "2025-10-22T14:11:19.071402Z", "shell.execute_reply": "2025-10-22T14:11:19.078248Z"}}, "outputs": [], "execution_count": 229}, {"cell_type": "code", "source": "torch.cuda.empty_cache()\nwith torch.no_grad():\n    quantized_weight, scale, zero, outlier_weight, perm = quik(layer.weight.data, 8, 2 * AccumulatedInput.hessians / AccumulatedInput.num_samples, AccumulatedInput.actnorms, n_outliers=0)\n    shit = QuikLinear(quantized_weight, scale, zero, outlier_weight, layer.bias, 8, perm)\n\n    result = torch.stack(tuple(shit(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)\n\n    quik_0_mse = float(torch.pow(result.cuda() - reference, 2).mean() ** (1/2))\n    print(\"MSE:\", quik_0_mse)\n\nassert quik_0_mse < 6e-5 and quik_0_mse > 5e-5", "metadata": {"id": "WBAhlxPcF0Mv", "colab": {"base_uri": "https://localhost:8080/", "height": 443}, "executionInfo": {"status": "error", "timestamp": 1761063929557, "user_tz": -180, "elapsed": 2734, "user": {"displayName": "\u0421\u0435\u0440\u0433\u0435\u0439 \u0421\u0430\u0432\u0438\u043d\u043e\u0432", "userId": "02828597018762863611"}}, "outputId": "d40efc97-9845-49af-cae8-14c55ecd6e6c", "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T11:09:02.436701Z", "iopub.execute_input": "2025-10-23T11:09:02.437525Z", "iopub.status.idle": "2025-10-23T11:09:15.994790Z", "shell.execute_reply.started": "2025-10-23T11:09:02.437499Z", "shell.execute_reply": "2025-10-23T11:09:15.993892Z"}}, "outputs": [{"name": "stdout", "text": "torch.Size([4096])\nMSE: 0.005766755435615778\n", "output_type": "stream"}, {"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)", "\u001b[0;32m/tmp/ipykernel_37/3649581756.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSE:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquik_0_mse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mquik_0_mse\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m6e-5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mquik_0_mse\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m5e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mAssertionError\u001b[0m: "], "ename": "AssertionError", "evalue": "", "output_type": "error"}], "execution_count": 207}, {"cell_type": "code", "source": "torch.cuda.empty_cache()\nwith torch.no_grad():\n    quantized_weight, scale, zero, outlier_weight, perm = quik(layer.weight.data, 8, 2 * AccumulatedInput.hessians / AccumulatedInput.num_samples, AccumulatedInput.actnorms, n_outliers=256)\n    shit = QuikLinear(quantized_weight, scale, zero, outlier_weight, layer.bias, 8, perm)\n\n    result = torch.stack(tuple(shit(inp).float() for inp in inps)).reshape(-1, OUT).mean(dim=0)\n\n    quik_256_mse = float(torch.pow(result.cuda() - reference, 2).mean() ** (1/2))\n    print(\"MSE:\", quik_256_mse)\n\nassert quik_256_mse < 4.25e-5 and quik_256_mse > 3.25e-5", "metadata": {"id": "euq4H-VwZj9Q", "trusted": true, "execution": {"iopub.status.busy": "2025-10-23T09:20:21.401319Z", "iopub.execute_input": "2025-10-23T09:20:21.402036Z", "iopub.status.idle": "2025-10-23T09:20:34.843324Z", "shell.execute_reply.started": "2025-10-23T09:20:21.402008Z", "shell.execute_reply": "2025-10-23T09:20:34.842563Z"}}, "outputs": [{"name": "stdout", "text": "torch.Size([4096])\nMSE: 3.753065175260417e-05\n", "output_type": "stream"}], "execution_count": 92}, {"cell_type": "code", "source": "quik_256_mse = float(torch.pow(result.cuda() - reference, 2).mean() ** (1/2))\nprint(\"MSE:\", quik_256_mse)", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-10-22T13:04:08.164712Z", "iopub.execute_input": "2025-10-22T13:04:08.165307Z", "iopub.status.idle": "2025-10-22T13:04:08.170327Z", "shell.execute_reply.started": "2025-10-22T13:04:08.165284Z", "shell.execute_reply": "2025-10-22T13:04:08.169662Z"}}, "outputs": [{"name": "stdout", "text": "MSE: 0.055914491415023804\n", "output_type": "stream"}], "execution_count": 135}, {"cell_type": "code", "source": "outlier_weight", "metadata": {"trusted": true, "execution": {"iopub.status.busy": "2025-10-22T13:30:13.052863Z", "iopub.execute_input": "2025-10-22T13:30:13.053452Z", "iopub.status.idle": "2025-10-22T13:30:13.102956Z", "shell.execute_reply.started": "2025-10-22T13:30:13.053432Z", "shell.execute_reply": "2025-10-22T13:30:13.102162Z"}}, "outputs": [{"execution_count": 176, "output_type": "execute_result", "data": {"text/plain": "tensor([[-0.0139,  0.0037, -0.0396,  ...,  0.0013,  0.0098, -0.0066],\n        [ 0.0017, -0.0031,  0.0025,  ...,  0.0084,  0.0039,  0.0006],\n        [ 0.0038,  0.0009,  0.0071,  ...,  0.0080,  0.0025,  0.0054],\n        ...,\n        [ 0.0015, -0.0148,  0.0409,  ..., -0.0149, -0.0083,  0.0391],\n        [ 0.0070, -0.0227,  0.0616,  ...,  0.0046,  0.0084, -0.0028],\n        [-0.0276,  0.0538, -0.0298,  ..., -0.0190, -0.0412,  0.0069]],\n       device='cuda:0', dtype=torch.float16)"}, "metadata": {}}], "execution_count": 176}, {"cell_type": "code", "source": "fruits = ['GPTQ', 'QUIK 256 outliers', 'QUIK 0 outliers', 'RNT']\ncounts = [gptq_mse, quik_256_mse, quik_0_mse, rtn_mse]\nbar_labels = ['green', 'blue', 'blue', 'red']\nbar_colors = ['green', 'blue', 'blue', 'red']\n\nplt.bar(fruits, counts, label=bar_labels, color=bar_colors)\nplt.ylabel(\"Layer MSE\")\nplt.show()", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 445}, "id": "-J0ychH4GDUk", "outputId": "f991312d-b3b3-4f6a-ea15-997f2839f430"}, "outputs": [{"data": {"image/png": "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", "text/plain": ["<Figure size 640x480 with 1 Axes>"]}, "metadata": {}, "output_type": "display_data"}], "execution_count": null}, {"cell_type": "markdown", "source": "As we can see, QUIK allows for errors smaller than RTN despite utilizing quantized activations, and addition of outliesr further decreases the error, bringing it closer to GPTQ. As we have seen in `benchmark.ipynb`, `int` matmul can lead to 2x-3x speedup over `float16`.", "metadata": {"id": "sF78dQBVIASz"}}, {"cell_type": "code", "source": "", "metadata": {"id": "SFp2mztPGYY4"}, "outputs": [], "execution_count": null}]}